{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybTuHh8ZryFt"
      },
      "source": [
        "## 최종 제출물은 약 1000만(9790K Pairs)개의 코드쌍을 학습시킨 결과입니다.\n",
        "### 각 모델 별 세부 전략에 사용된 코드는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpDZg1QsqvmJ"
      },
      "source": [
        "## Improved R-Drop Trainer for GraphCodeRoberta with RBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9cOeOKor9yC"
      },
      "source": [
        "### \"R-Drop: Regularized Dropout for Neural Networks\"(https://arxiv.org/abs/2106.14448)\n",
        "\n",
        "트랜스포머 내부의 Dropout은 forward passing 때마다 랜덤하게 분포합니다. 이때 어텐션 블록들을 각자 독립적으로 통과한 두 개의 Logit을 바탕으로 KL Divergence Loss를 최소화시키면 성능향상이 있습니다. 저희 팀에서는 이때 논문대로 동일한 코드쌍을 단순히 두 번 통과시키지는 않았습니다. 앞뒤 순서를 바꾼 코드쌍: \"Code1[SEP][SEP]Code2\"와 \"Code2[SEP][SEP]Code1\"를 각자 한 번씩 통과시키면서 R-Drop Regularization을 진행했고, 이를 통해 성능을 올렸습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy0Rw8Paqzuv"
      },
      "outputs": [],
      "source": [
        "class RobertaRBERT(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Dropout(config.hidden_dropout_prob),\n",
        "            nn.Linear(config.hidden_size, config.hidden_size),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.classifier = nn.Linear(config.hidden_size * 4, config.num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        return_dict = (\n",
        "            return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        )\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        \n",
        "        hidden_states = outputs[0]\n",
        "        batch_size, _, hidden_size = hidden_states.shape    \n",
        "\n",
        "        # CLS code1 SEP SEP code2 SEP\n",
        "        cls_flag = input_ids == self.config.tokenizer_cls_token_id  # cls token\n",
        "        sep_flag = input_ids == self.config.tokenizer_sep_token_id  # sep token\n",
        "\n",
        "        special_token_states = hidden_states[cls_flag + sep_flag].view(batch_size, -1, hidden_size)  # (batch_size, 4, hidden_size)\n",
        "        special_hidden_states = self.net(special_token_states)  # (batch_size, 4, hidden_size)\n",
        "\n",
        "        pooled_output = special_hidden_states.view(batch_size, -1)  # (batch_size, hidden_size * 4)\n",
        "        logits = self.classifier(pooled_output)  # (batch_size, num_labels)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwI5u1fKqzsF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Any, List, Optional, Union, Tuple\n",
        "from transformers import Trainer\n",
        "from transformers.trainer_pt_utils import nested_detach\n",
        "from transformers.utils import logging\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "class ImprovedRDropTrainer(Trainer):\n",
        "    def get_kl_loss(self, loss_fn, logits_1, logits_2, alpha=1):\n",
        "        loss_kl_1 = loss_fn(F.log_softmax(logits_1, dim=-1), F.softmax(logits_2, dim=-1))\n",
        "        loss_kl_2 = loss_fn(F.log_softmax(logits_2, dim=-1), F.softmax(logits_1, dim=-1))\n",
        "        return alpha * (loss_kl_1 + loss_kl_2) / 2\n",
        "\n",
        "    def prediction_step(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "        prediction_loss_only: bool,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "\n",
        "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        if ignore_keys is None:\n",
        "            if hasattr(self.model, \"config\"):\n",
        "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
        "            else:\n",
        "                ignore_keys = []\n",
        "\n",
        "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
        "        if has_labels:\n",
        "            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
        "            if len(labels) == 1:\n",
        "                labels = labels[0]\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if has_labels:\n",
        "                with self.autocast_smart_context_manager():\n",
        "                    loss, logits = self.compute_eval_loss(model, inputs, return_outputs=True)\n",
        "                loss = loss.mean().detach()\n",
        "\n",
        "            else:\n",
        "                loss = None\n",
        "                with self.autocast_smart_context_manager():\n",
        "                    outputs1 = model(\n",
        "                        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
        "                    )\n",
        "                    outputs2 = model(\n",
        "                        input_ids=inputs[\"input_ids2\"], attention_mask=inputs[\"attention_mask2\"]\n",
        "                    )\n",
        "\n",
        "                    logits = (outputs1.logits + outputs2.logits) / 2\n",
        "\n",
        "        if prediction_loss_only:\n",
        "            return (loss, None, None)\n",
        "\n",
        "        if len(logits) == 1:\n",
        "            logits = logits[0]\n",
        "\n",
        "        return (loss, logits, labels)\n",
        "\n",
        "    def compute_loss(self, model, inputs):\n",
        "        num_labels = 2\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        # cls code1 sep sep code2 sep\n",
        "        outputs1 = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "        logits1 = outputs1.logits\n",
        "\n",
        "        # cls code2 sep sep code1 sep\n",
        "        outputs2 = model(input_ids=inputs[\"input_ids2\"], attention_mask=inputs[\"attention_mask2\"])\n",
        "        logits2 = outputs2.logits\n",
        "\n",
        "        # Crossentropy Loss\n",
        "        loss_fct_1 = nn.CrossEntropyLoss()\n",
        "        loss_nll = (\n",
        "            loss_fct_1(logits1.view(-1, num_labels), labels.view(-1))\n",
        "            + loss_fct_1(logits2.view(-1, num_labels), labels.view(-1))\n",
        "        ) / 2\n",
        "\n",
        "        # KL-Divergence Loss\n",
        "        loss_fct_2 = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        loss_kl = self.get_kl_loss(loss_fct_2, logits1, logits2)\n",
        "        return loss_nll + loss_kl\n",
        "\n",
        "    def compute_eval_loss(self, model, inputs, return_outputs=False):\n",
        "        num_labels = 2\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        outputs1 = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "        outputs2 = model(input_ids=inputs[\"input_ids2\"], attention_mask=inputs[\"attention_mask2\"])\n",
        "\n",
        "        logits = (outputs1.logits + outputs2.logits) / 2\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, logits) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-FhSBsWqrrg"
      },
      "source": [
        "## PLBART-Large Encoder\n",
        "\n",
        "### \"RBERT: Enriching Pre-Training Language Model with Entity Information for Relation Classification\" (https://arxiv.org/pdf/1905.08284.pdf)\n",
        "### \"Unified Pre-training for Program Understanding and Generation\"(https://arxiv.org/abs/2103.06333.pdf)\n",
        "\n",
        "\n",
        "GraphCodeRobera의 경우는 모델은 훌륭하게 기학습되었으나 토크나이저가 비효율적입니다. 데이터셋의 75%만이 512 토큰 안에 들어갑니다. 데이터셋이 지나치게 잘게 쪼개진다는 단점을 저희 팀에서는 어떻게든 극복해보려고 했습니다. 또한 Base 모델 밖에 제공이 안 된다는 단점이 있습니다. \n",
        "\n",
        "이에 반해 PLBart 같은 경우는 Base 모델이 아니라 Large 모델까지 제공하며,  임베딩이 깊이가 1024 차원으로써 더 많은 내용을 담고 있습니다. 무엇보다 92%의 데이터셋이 512 토큰안에 들어갑니다. 그러나, 12 Layer Encoder - 12 Layer Decoder이라는 점에서 모델이 지나치게 크기 때문에 훈련시키기에는 비효율적이라는 단점이 있습니다. 따라서 Encoder - Decoder 모델을 전부 사용하는 것보다는, Encoder 부분만 따와서 사용했으며 이 방법이 Batch size를 늘리기에도 좋고 훈련도 빠르게 할 수 있었습니다. \n",
        "\n",
        "BART는 다른 모델들과는 다르게, CLS 토큰이 없습니다. 보통 BART로 분류작업을 할 때, SEP토큰과 EOS 토큰이 동일하기 때문에 마지막 토큰을 추출하여서 분류를 진행합니다. 이때, PLBART의 논문에서 특이하게 \"python\" 토큰도 집어넣어서 기학습을 진행했다는 걸 확인했습니다. 또한 92%의 데이터가 전부 512토큰 안에 들어간다는 점을 감안하여 Python Token을 포함한 Code1 문장, Code2 문장 단위로 Token들의 hidden state을 평균시켜서 RBERT 방식으로 분류를 진행했습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS8apgMVqtNW"
      },
      "outputs": [],
      "source": [
        "class VHBartEncoderForSequenceClassification(PLBartModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.entity_fc_layer = FCLayer(\n",
        "            self.config.hidden_size, self.config.hidden_size, self.config.dropout_rate\n",
        "        )\n",
        "        self.proj_fc_layer = FCLayer(\n",
        "            self.config.hidden_size * 4, self.config.hidden_size, self.config.dropout_rate\n",
        "        )\n",
        "        self.label_classifier = FCLayer(\n",
        "            self.config.hidden_size * 3,\n",
        "            self.config.num_labels,\n",
        "            self.config.dropout_rate,\n",
        "            use_activation=True,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, attention_mask, code1_mask, code2_mask, last_token_index, labels=None,\n",
        "    ):\n",
        "\n",
        "        outputs = self.encoder(\n",
        "            input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True,\n",
        "        )\n",
        "\n",
        "        # Global token's 4 hidden states concatenation and  projection result\n",
        "        idx_seq = torch.arange(input_ids.size(0)).to(input_ids.device)\n",
        "        # print(idx_seq)\n",
        "        cls_concat = torch.cat(\n",
        "            tuple(\n",
        "                [outputs[\"hidden_states\"][i][idx_seq, last_token_index] for i in [-4, -3, -2, -1]]\n",
        "            ),\n",
        "            dim=-1,\n",
        "        )\n",
        "        cls_output = self.proj_fc_layer(cls_concat)\n",
        "\n",
        "        # Global average on sentences\n",
        "        sequence_output = outputs[\"last_hidden_state\"]\n",
        "\n",
        "        code1_sentence_h = self.entity_average(\n",
        "            sequence_output, code1_mask\n",
        "        )  # token in between code1 entities ->\n",
        "        code1_sentence_h = self.entity_fc_layer(\n",
        "            code1_sentence_h\n",
        "        )  # code1 entity's fully connected layer | yellow on diagram\n",
        "\n",
        "        # Average on code2 sentence\n",
        "        code2_sentence_h = self.entity_average(\n",
        "            sequence_output, code2_mask\n",
        "        )  # token in between code2 entities\n",
        "        code2_sentence_h = self.entity_fc_layer(\n",
        "            code2_sentence_h\n",
        "        )  # code2 entity's fully connected layer | red on diagram\n",
        "\n",
        "        # Concat: global token,code1 average, code2 average\n",
        "        concat = torch.cat(\n",
        "            [\n",
        "                code1_sentence_h,\n",
        "                code2_sentence_h,\n",
        "                cls_output,\n",
        "            ],\n",
        "            dim=-1,\n",
        "        )\n",
        "\n",
        "        # yield logit from label classifier\n",
        "        logits = self.label_classifier(concat)\n",
        "        prob = nn.functional.softmax(logits)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # print(logits)\n",
        "            # print(labels)\n",
        "            labels = labels.squeeze(-1)\n",
        "            loss = loss_fct(logits, labels)\n",
        "            return loss, prob\n",
        "        else:\n",
        "            return prob\n",
        "\n",
        "    def entity_average(self, hidden_output, e_mask):\n",
        "        \"\"\"\n",
        "        Average the entity hidden state vectors (H_i ~ H_j)\n",
        "        :param hidden_output: [batch_size, j-i+1, dim]\n",
        "        :param e_mask: [batch_size, max_seq_len]\n",
        "                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\n",
        "        :return: [batch_size, dim]\n",
        "        \"\"\"\n",
        "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
        "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
        "\n",
        "        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
        "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n",
        "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n",
        "        return avg_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aHBAqn-qtwU"
      },
      "source": [
        "## CodeT5 Encoder\n",
        "\n",
        "### \"An Improved Baseline for Sentence-level Relation Extraction\"(https://arxiv.org/abs/2102.01373)\n",
        "### \"EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks\"(https://arxiv.org/pdf/2110.08426.pdf)\n",
        "\n",
        "마찬가지로 CodeT5도 경우는 토크나이저가 비교적 효율적이라서, 데이터셋의 82%가 512 토큰 안에 들어갑니다. PLBART와 유사하게 Encoder만 따와서 훈련을 시켰습니다.\n",
        "\n",
        "CodeT5를 학습시킬 때 CLS 토큰뿐만 아니라 SEP 토큰의 정보까지 활용해서, 두 문장간의 관계를 추출하도록 했습니다. 논문에서는 추가적인 Punctuation을 제공하지만, 이번 대회는 코드 길이가 중요했기 때문에 CLS, SEP만 추출하는 걸로 살짝 변형해서 사용했습니다. 또한 원래 Decoder에게 정보를 넘겨주는 Layer에서 분류 작업을 하는 것이 불안정할 것을 우려하여 Sentence Average도 추출하였습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it0PS9_SqvN2"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from transformers.models.t5.modeling_t5 import T5EncoderModel, T5Config, T5PreTrainedModel, T5Stack\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n",
        "from utils.heads import (\n",
        "    AdaptivePooler,\n",
        "    MeanPooler,\n",
        "    FCLayer,\n",
        ")\n",
        "\n",
        "\n",
        "class VHT5EncoderForSequenceClassification(T5PreTrainedModel):\n",
        "    \"\"\" \n",
        "    Using [CLS token hidden states, Input Sequence Weighted Average, SEP token hidden states] for classification\n",
        "    \"\"\"\n",
        "\n",
        "    _keys_to_ignore_on_load_missing = [\n",
        "        r\"encoder\\.embed_tokens\\.weight\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, config: T5Config, dropout=0.1, pooler=\"adaptive\"):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
        "\n",
        "        encoder_config = copy.deepcopy(config)\n",
        "        encoder_config.use_cache = False\n",
        "        encoder_config.is_encoder_decoder = False\n",
        "        self.encoder = T5Stack(encoder_config, self.shared)\n",
        "        pooler_class = AdaptivePooler if pooler == \"adaptive\" else MeanPooler\n",
        "        self.pooler = pooler_class(input_size=config.hidden_size)\n",
        "\n",
        "        # FC Layers\n",
        "        self.cls_fc_layer = FCLayer(\n",
        "            self.config.hidden_size * 4,\n",
        "            self.config.hidden_size,\n",
        "            self.config.dropout_rate,\n",
        "            use_activation=True,\n",
        "        )\n",
        "\n",
        "        self.sep_fc_layer = FCLayer(\n",
        "            self.config.hidden_size * 4,\n",
        "            self.config.hidden_size,\n",
        "            self.config.dropout_rate,\n",
        "            use_activation=True,\n",
        "        )\n",
        "\n",
        "        self.label_classifier = FCLayer(\n",
        "            self.config.hidden_size * 3,\n",
        "            self.config.num_labels,\n",
        "            self.config.dropout_rate,\n",
        "            use_activation=False,\n",
        "        )\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "        # Model parallel\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "\n",
        "    def parallelize(self, device_map=None):\n",
        "        self.device_map = (\n",
        "            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n",
        "            if device_map is None\n",
        "            else device_map\n",
        "        )\n",
        "        assert_device_map(self.device_map, len(self.encoder.block))\n",
        "        self.encoder.parallelize(self.device_map)\n",
        "        self.classifier = self.classifier.to(self.encoder.first_device)\n",
        "        self.model_parallel = True\n",
        "\n",
        "    def deparallelize(self):\n",
        "        self.encoder.deparallelize()\n",
        "        self.encoder = self.encoder.to(\"cpu\")\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.shared = new_embeddings\n",
        "        self.encoder.set_input_embeddings(new_embeddings)\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        last_token_index=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # get adaptive pooler for the given sequence\n",
        "        weights, pooled_output = self.pooler(outputs[0], mask=attention_mask)\n",
        "\n",
        "        # Global token's 4 hidden states concatenation and  projection result\n",
        "        idx_seq = torch.arange(input_ids.size(0)).to(input_ids.device)\n",
        "\n",
        "        cls_concat = torch.cat(\n",
        "            tuple([outputs[\"hidden_states\"][i][idx_seq, 0, :] for i in [-4, -3, -2, -1]]), dim=-1\n",
        "        )\n",
        "        cls_output = self.cls_fc_layer(cls_concat)\n",
        "\n",
        "        sep_concat = torch.cat(\n",
        "            tuple(\n",
        "                [outputs[\"hidden_states\"][i][idx_seq, last_token_index] for i in [-4, -3, -2, -1]]\n",
        "            ),\n",
        "            dim=-1,\n",
        "        )\n",
        "        sep_output = self.sep_fc_layer(sep_concat)\n",
        "\n",
        "        concat = torch.cat([cls_output, pooled_output, sep_output], dim=-1)\n",
        "\n",
        "        logits = self.label_classifier(concat)\n",
        "\n",
        "        loss = None\n",
        "        outputs.hidden_states = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (\n",
        "                    labels.dtype == torch.long or labels.dtype == torch.int\n",
        "                ):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej8kd3E0rtej"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "💬 전처리 💬\n",
        "전처리 부분에서 가장 주요했던 것은 import 부분을 아예 제거하는 것이었습니다. Import는 대부분 사용을 하지 않고, 사용한다고 해도 어차피 코드에서 드러나니, import 부분을 전부 제외하자는 상하님의 아이디어는 기발했습니다 👍 \n",
        "\n",
        "그 외에 여러 개의 space, tab 들을 한 개의 space로 제거하는 전처리를 거쳤습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEmfgYU7ruls"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "class FunctionPreprocessor:\n",
        "    def __init__(self,):\n",
        "        pass\n",
        "\n",
        "    def get_function(self, code):\n",
        "        results = []\n",
        "        fn_list = re.findall(\"\\ndef [a-zA-Z0-9_]+\\(\", code)\n",
        "\n",
        "        for fn in fn_list:\n",
        "            results.append(fn[4:-1].strip())\n",
        "        return results\n",
        "\n",
        "    def determine_function(self, code, function_name):\n",
        "        num = len(re.findall(\"[^a-zA-Z]\" + function_name + \"[^a-zA-Z]\", code))\n",
        "        return False if num <= 1 else True\n",
        "\n",
        "    def delete_function(self, code, name):\n",
        "        start_id, _ = re.search(\"def \" + name, code).span()\n",
        "        ptr = start_id\n",
        "\n",
        "        while ptr < len(code) - 1:\n",
        "            if code[ptr] == \"\\n\" and re.search(\"[a-zA-Z]\", code[ptr + 1]) is not None:\n",
        "                break\n",
        "            ptr += 1\n",
        "\n",
        "        if ptr != len(code) - 1:\n",
        "            end_id = ptr\n",
        "            code = code[:start_id] + code[end_id:]\n",
        "\n",
        "        return code\n",
        "\n",
        "    def preprocess(self, code):\n",
        "        code = \"\\n\" + code\n",
        "        fn_list = self.get_function(code)\n",
        "        if len(fn_list) == 0:\n",
        "            return code\n",
        "\n",
        "        for fn in fn_list:\n",
        "            flag = self.determine_function(code, fn)\n",
        "\n",
        "            if flag == False:\n",
        "                code = self.delete_function(code, fn)\n",
        "\n",
        "        return code\n",
        "\n",
        "    def __call__(self, datasets):\n",
        "        code1_list = []\n",
        "        code2_list = []\n",
        "\n",
        "        size = len(datasets[\"code1\"])\n",
        "        for i in range(size):\n",
        "            code1 = self.preprocess(datasets[\"code1\"][i])\n",
        "            code2 = self.preprocess(datasets[\"code2\"][i])\n",
        "\n",
        "            code1_list.append(code1)\n",
        "            code2_list.append(code2)\n",
        "\n",
        "        datasets[\"code1\"] = code1_list\n",
        "        datasets[\"code2\"] = code2_list\n",
        "        return datasets\n",
        "\n",
        "\n",
        "class AnnotationPreprocessor:\n",
        "    def __init__(self,):\n",
        "        pass\n",
        "\n",
        "    def search(self, sen_list, string):\n",
        "        for i, sen in enumerate(sen_list):\n",
        "            if string in sen:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    def delete_annotation_block(self, code, string):\n",
        "        sens = [sen for sen in code.split(\"\\n\")]\n",
        "\n",
        "        start_id = self.search(sens, string)\n",
        "        end_id = self.search(sens[start_id + 1 :], string)\n",
        "        if end_id != -1:\n",
        "            end_id += start_id + 1\n",
        "            code = sens[:start_id] + sens[end_id + 1 :]\n",
        "        else:\n",
        "            code = sens[:start_id] + sens[start_id + 1 :]\n",
        "\n",
        "        code = \"\\n\".join(code)\n",
        "        return code\n",
        "\n",
        "    def delete_block(self, code, string):\n",
        "        while string in code:\n",
        "            code = self.delete_annotation_block(code, string)\n",
        "        return code\n",
        "\n",
        "    def delete_annotation(self, code):\n",
        "        sens = code.split(\"\\n\")\n",
        "\n",
        "        sens_processed = []\n",
        "        for sen in sens:\n",
        "            if \"#\" in sen:\n",
        "                index = sen.index(\"#\")\n",
        "                sen = sen[:index]\n",
        "            sens_processed.append(sen)\n",
        "\n",
        "        return \"\\n\".join(sens_processed)\n",
        "\n",
        "    def delete_import(self, code):\n",
        "        sens = code.split(\"\\n\")\n",
        "\n",
        "        sens_processed = []\n",
        "        for sen in sens:\n",
        "            if \"import\" not in sen:\n",
        "                sens_processed.append(sen)\n",
        "\n",
        "        return \"\\n\".join(sens_processed)\n",
        "\n",
        "    def preprocess(self, code):\n",
        "        code = self.delete_block(code, '\"\"\"')\n",
        "        code = self.delete_block(code, \"'''\")\n",
        "        code = self.delete_annotation(code)\n",
        "        code = self.delete_import(code)\n",
        "        code = re.sub(\"\\s+\", \" \", code).strip()\n",
        "        return code\n",
        "\n",
        "    def __call__(self, datasets):\n",
        "        code1_list = []\n",
        "        code2_list = []\n",
        "\n",
        "        size = len(datasets[\"code1\"])\n",
        "        for i in range(size):\n",
        "            code1 = self.preprocess(datasets[\"code1\"][i])\n",
        "            code2 = self.preprocess(datasets[\"code2\"][i])\n",
        "\n",
        "            code1_list.append(code1)\n",
        "            code2_list.append(code2)\n",
        "\n",
        "        datasets[\"code1\"] = code1_list\n",
        "        datasets[\"code2\"] = code2_list\n",
        "        return datasets\n",
        "\n",
        "\n",
        "class BasePreprocessor:\n",
        "    def __init__(self,):\n",
        "        pass\n",
        "\n",
        "    def search(self, sen_list, string):\n",
        "        for i, sen in enumerate(sen_list):\n",
        "            if string in sen:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    def delete_annotation_block(self, code, string):\n",
        "        sens = [sen for sen in code.split(\"\\n\")]\n",
        "\n",
        "        start_id = self.search(sens, string)\n",
        "        end_id = self.search(sens[start_id + 1 :], string)\n",
        "        if end_id != -1:\n",
        "            end_id += start_id + 1\n",
        "            code = sens[:start_id] + sens[end_id + 1 :]\n",
        "        else:\n",
        "            code = sens[:start_id] + sens[start_id + 1 :]\n",
        "\n",
        "        code = \"\\n\".join(code)\n",
        "        return code\n",
        "\n",
        "    def delete_block(self, code, string):\n",
        "        while string in code:\n",
        "            code = self.delete_annotation_block(code, string)\n",
        "        return code\n",
        "\n",
        "    def delete_annotation(self, code):\n",
        "        sens = code.split(\"\\n\")\n",
        "\n",
        "        sens_processed = []\n",
        "        for sen in sens:\n",
        "            if \"#\" in sen:\n",
        "                index = sen.index(\"#\")\n",
        "                sen = sen[:index]\n",
        "            sens_processed.append(sen)\n",
        "\n",
        "        return \"\\n\".join(sens_processed)\n",
        "\n",
        "    def delete_import(self, code):\n",
        "        sens = code.split(\"\\n\")\n",
        "\n",
        "        sens_processed = []\n",
        "        for sen in sens:\n",
        "            if \"import\" not in sen:\n",
        "                sens_processed.append(sen)\n",
        "\n",
        "        return \"\\n\".join(sens_processed)\n",
        "\n",
        "    def preprocess(self, code):\n",
        "        code = self.delete_block(code, '\"\"\"')\n",
        "        code = self.delete_block(code, \"'''\")\n",
        "        code = self.delete_annotation(code)\n",
        "        code = self.delete_import(code)\n",
        "        # code = re.sub('\\s+', ' ', code).strip()\n",
        "        return code\n",
        "\n",
        "    def __call__(self, datasets):\n",
        "        code1_list = []\n",
        "        code2_list = []\n",
        "        label_list = []\n",
        "\n",
        "        size = len(datasets[\"code1\"])\n",
        "        for i in range(size):\n",
        "            code1 = self.preprocess(datasets[\"code1\"][i])\n",
        "            code2 = self.preprocess(datasets[\"code2\"][i])\n",
        "            if \"similar\" in datasets:\n",
        "                label_list = [int(x) for x in datasets[\"similar\"]]\n",
        "\n",
        "            code1_list.append(code1)\n",
        "            code2_list.append(code2)\n",
        "\n",
        "        datasets[\"code1\"] = code1_list\n",
        "        datasets[\"code2\"] = code2_list\n",
        "        if \"similar\" in datasets:\n",
        "            datasets[\"similar\"] = label_list\n",
        "\n",
        "        return datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBTmSodGsVh3"
      },
      "source": [
        "### 데이터 제작 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMVLoR18sWMG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "\n",
        "# read .py documents\n",
        "df_train = pd.DataFrame({}, columns=[\"file_name\", \"question_label\", \"code\"])\n",
        "TRAIN_PATH = \"/home/ubuntu/plclassification/code\"\n",
        "list_questions = glob.glob(os.path.join(TRAIN_PATH, \"*\"))\n",
        "\n",
        "list_index = []\n",
        "list_problem_names = []\n",
        "list_code_text = []\n",
        "\n",
        "for index, question in enumerate(list_questions):\n",
        "    problem_name = os.path.basename(question)\n",
        "    # extract numerics only from problem_name\n",
        "    problem_name = \"\".join(i for i in problem_name if i.isdigit())\n",
        "\n",
        "    for file in glob.glob(os.path.join(question, \"*.py\")):\n",
        "        file_base_name = os.path.basename(file)\n",
        "        # read text from .py file\n",
        "        with open(file, \"r\") as f:\n",
        "            code_text = f.read()\n",
        "            # append row with index, problem_name and code text\n",
        "            list_index.append(file_base_name)\n",
        "            list_problem_names.append(problem_name)\n",
        "            list_code_text.append(code_text)\n",
        "\n",
        "df_train[\"file_name\"] = list_index\n",
        "df_train[\"question_label\"] = list_problem_names\n",
        "# remove 0 in front of the number in question_label\n",
        "df_train[\"question_label\"] = df_train[\"question_label\"].str.replace(\"^0+\", \"\").astype(int)\n",
        "df_train[\"code\"] = list_code_text\n",
        "\n",
        "# create similar pairs\n",
        "df_similar = pd.DataFrame(\n",
        "    {}, columns=[\"code1\", \"code2\", \"similar\", \"code1_group\", \"code2_group\", \"pair_id\"]\n",
        ")\n",
        "\n",
        "for i in tqdm(range(1, 301)):\n",
        "    list_code1 = []\n",
        "    list_code2 = []\n",
        "    list_question_pair_id = []\n",
        "    list_group_id = []\n",
        "    # get corresponding dataframe where question_label == i\n",
        "    group_index = df_train[df_train[\"question_label\"] == i].index\n",
        "    df_group = df_train.iloc[group_index]\n",
        "\n",
        "    # make unique pair combination from group_index list\n",
        "    combinated_indices = list(combinations(group_index, 2))\n",
        "    for index in combinated_indices:\n",
        "        code1 = df_train.iloc[index[0]][\"code\"]\n",
        "        code2 = df_train.iloc[index[1]][\"code\"]\n",
        "        list_code1.append(code1)\n",
        "        list_code2.append(code2)\n",
        "        question_pair_id = index[0] ** 3 + index[1] ** 3 + index[0] + index[1]\n",
        "        list_question_pair_id.append(question_pair_id)\n",
        "\n",
        "    df_similar = df_similar.append(\n",
        "        pd.DataFrame(\n",
        "            {\n",
        "                \"code1\": list_code1,\n",
        "                \"code2\": list_code2,\n",
        "                \"similar\": 1,\n",
        "                \"code1_group\": i,\n",
        "                \"code2_group\": i,\n",
        "                \"pair_id\": list_question_pair_id,\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "df_similar = df_similar.reset_index(drop=True)\n",
        "df_similar[\"code1_group\"] = df_similar[\"code1_group\"].astype(int)\n",
        "df_similar[\"code2_group\"] = df_similar[\"code2_group\"].astype(int)\n",
        "print(df_similar.shape)\n",
        "\n",
        "# Create different pairs\n",
        "NUM = 120000\n",
        "df_different = pd.DataFrame({}, columns=[\"code1\", \"code2\", \"similar\"])\n",
        "\n",
        "for i in tqdm(range(1, 301)):\n",
        "    # get corresponding dataframe where question_label == i\n",
        "    group_index = df_train[df_train[\"question_label\"] == i].index\n",
        "    other_index = df_train[df_train[\"question_label\"] != i].index\n",
        "    sample_index = np.random.choice(group_index, NUM, replace=True)\n",
        "    other_sample_index = np.random.choice(other_index, NUM, replace=True)\n",
        "\n",
        "    # make unique pair id: multiplication - sum\n",
        "    question_pair_id = (\n",
        "        sample_index ** 3 + other_sample_index ** 3 + (sample_index + other_sample_index)\n",
        "    )\n",
        "    # make group pair id\n",
        "    other_group_index = df_train.iloc[other_sample_index][\"question_label\"].values\n",
        "\n",
        "    # append df_train.iloc[sample_index][\"code\"] to df_different[\"code1\"]\n",
        "    code1 = df_train.iloc[sample_index][\"code\"].values\n",
        "    code2 = df_train.iloc[other_sample_index][\"code\"].values\n",
        "    df_different = df_different.append(\n",
        "        pd.DataFrame(\n",
        "            {\n",
        "                \"code1\": code1,\n",
        "                \"code2\": code2,\n",
        "                \"similar\": 0,\n",
        "                \"code1_group\": i,\n",
        "                \"code2_group\": other_group_index.astype(int),\n",
        "                \"question_pair_id\": question_pair_id,\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "df_different = df_different.drop_duplicates(subset=[\"question_pair_id\"])\n",
        "df_different[\"code1_group\"] = df_different[\"code1_group\"].astype(int)\n",
        "df_different[\"code2_group\"] = df_different[\"code2_group\"].astype(int)\n",
        "print(df_different.shape)\n",
        "\n",
        "\n",
        "# import KFold from sklearn\n",
        "list_groups = list([i for i in range(1, 301)])\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "folds_10 = kf.get_n_splits(list_groups)\n",
        "\n",
        "for fold_index, (train_index, test_index) in enumerate(kf.split(list_groups)):\n",
        "\n",
        "    df_similar_train_fold = df_similar[df_similar[\"code1_group\"].isin(train_index)]\n",
        "    df_similar_val_fold = df_similar[df_similar[\"code1_group\"].isin(test_index)]\n",
        "\n",
        "    df_different_train_fold = df_different[\n",
        "        df_different[\"code1_group\"].isin(train_index)\n",
        "        & df_different[\"code2_group\"].isin(train_index)\n",
        "    ]\n",
        "    df_different_train_fold = df_different_train_fold.sample(len(df_similar_train_fold))\n",
        "    df_different_val_fold = df_different[\n",
        "        df_different[\"code1_group\"].isin(test_index) & df_different[\"code2_group\"].isin(test_index)\n",
        "    ]\n",
        "    df_different_val_fold = df_different_val_fold.sample(len(df_similar_val_fold))\n",
        "\n",
        "    df_train = pd.concat([df_similar_train_fold, df_different_train_fold])\n",
        "    # shuffle df_train\n",
        "    df_train = df_train.sample(frac=1)\n",
        "    # reset index for df_train\n",
        "    df_train = df_train.reset_index(drop=True)\n",
        "\n",
        "    df_val = pd.concat([df_similar_val_fold, df_different_val_fold])\n",
        "    # shuffle df_val\n",
        "    df_val = df_val.sample(frac=1)\n",
        "    # reset index for df_val\n",
        "    df_val = df_val.reset_index(drop=True)\n",
        "\n",
        "    print(df_train.shape, df_val.shape)\n",
        "    assert df_train.columns == df_val.columns\n",
        "\n",
        "    dataset_fold = DatasetDict()\n",
        "    dataset_train = Dataset.from_pandas(df_train)\n",
        "    dataset_val = Dataset.from_pandas(df_val)\n",
        "    dataset_fold[\"train\"] = dataset_train\n",
        "    dataset_fold[\"val\"] = dataset_val\n",
        "\n",
        "    name = f\"PoolC/{fold_index+1}-fold-clone-detection-600k-5fold\"\n",
        "    dataset_fold.push_to_hub(name, private=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLDaIRbOsYHn"
      },
      "source": [
        "미리 업로드한 데이터셋에서 90만개를 랜덤추출을 했기 때문에, Fold 별로 갈라진 Class는 동일하지만 추출된 90만 개의 데이터셋은 각 모델 별로 상이합니다. 이를 통해서 앙상블 성능을 높이려고 했습니다.\n",
        "\n",
        "> 작성자: 박상하, 안영진\n",
        "\n",
        "> This Notebook has been released under the Apache 2.0 open source license.\n",
        "\n",
        "댓글로 의견을 공유해주시면 저희에게도 공부할 기회가 될 것 같습니다 🙏\n",
        "감사합니다!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
