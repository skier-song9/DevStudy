{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybTuHh8ZryFt"
      },
      "source": [
        "## ìµœì¢… ì œì¶œë¬¼ì€ ì•½ 1000ë§Œ(9790K Pairs)ê°œì˜ ì½”ë“œìŒì„ í•™ìŠµì‹œí‚¨ ê²°ê³¼ì…ë‹ˆë‹¤.\n",
        "### ê° ëª¨ë¸ ë³„ ì„¸ë¶€ ì „ëµì— ì‚¬ìš©ëœ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpDZg1QsqvmJ"
      },
      "source": [
        "## Improved R-Drop Trainer for GraphCodeRoberta with RBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9cOeOKor9yC"
      },
      "source": [
        "### \"R-Drop: Regularized Dropout for Neural Networks\"(https://arxiv.org/abs/2106.14448)\n",
        "\n",
        "íŠ¸ëœìŠ¤í¬ë¨¸ ë‚´ë¶€ì˜ Dropoutì€ forward passing ë•Œë§ˆë‹¤ ëœë¤í•˜ê²Œ ë¶„í¬í•©ë‹ˆë‹¤. ì´ë•Œ ì–´í…ì…˜ ë¸”ë¡ë“¤ì„ ê°ì ë…ë¦½ì ìœ¼ë¡œ í†µê³¼í•œ ë‘ ê°œì˜ Logitì„ ë°”íƒ•ìœ¼ë¡œ KL Divergence Lossë¥¼ ìµœì†Œí™”ì‹œí‚¤ë©´ ì„±ëŠ¥í–¥ìƒì´ ìˆìŠµë‹ˆë‹¤. ì €í¬ íŒ€ì—ì„œëŠ” ì´ë•Œ ë…¼ë¬¸ëŒ€ë¡œ ë™ì¼í•œ ì½”ë“œìŒì„ ë‹¨ìˆœíˆ ë‘ ë²ˆ í†µê³¼ì‹œí‚¤ì§€ëŠ” ì•Šì•˜ìŠµë‹ˆë‹¤. ì•ë’¤ ìˆœì„œë¥¼ ë°”ê¾¼ ì½”ë“œìŒ: \"Code1[SEP][SEP]Code2\"ì™€ \"Code2[SEP][SEP]Code1\"ë¥¼ ê°ì í•œ ë²ˆì”© í†µê³¼ì‹œí‚¤ë©´ì„œ R-Drop Regularizationì„ ì§„í–‰í–ˆê³ , ì´ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ì˜¬ë ¸ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy0Rw8Paqzuv"
      },
      "outputs": [],
      "source": [
        "class RobertaRBERT(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Dropout(config.hidden_dropout_prob),\n",
        "            nn.Linear(config.hidden_size, config.hidden_size),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.classifier = nn.Linear(config.hidden_size * 4, config.num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        return_dict = (\n",
        "            return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        )\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        \n",
        "        hidden_states = outputs[0]\n",
        "        batch_size, _, hidden_size = hidden_states.shape    \n",
        "\n",
        "        # CLS code1 SEP SEP code2 SEP\n",
        "        cls_flag = input_ids == self.config.tokenizer_cls_token_id  # cls token\n",
        "        sep_flag = input_ids == self.config.tokenizer_sep_token_id  # sep token\n",
        "\n",
        "        special_token_states = hidden_states[cls_flag + sep_flag].view(batch_size, -1, hidden_size)  # (batch_size, 4, hidden_size)\n",
        "        special_hidden_states = self.net(special_token_states)  # (batch_size, 4, hidden_size)\n",
        "\n",
        "        pooled_output = special_hidden_states.view(batch_size, -1)  # (batch_size, hidden_size * 4)\n",
        "        logits = self.classifier(pooled_output)  # (batch_size, num_labels)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwI5u1fKqzsF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Any, List, Optional, Union, Tuple\n",
        "from transformers import Trainer\n",
        "from transformers.trainer_pt_utils import nested_detach\n",
        "from transformers.utils import logging\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "class ImprovedRDropTrainer(Trainer):\n",
        "    def get_kl_loss(self, loss_fn, logits_1, logits_2, alpha=1):\n",
        "        loss_kl_1 = loss_fn(F.log_softmax(logits_1, dim=-1), F.softmax(logits_2, dim=-1))\n",
        "        loss_kl_2 = loss_fn(F.log_softmax(logits_2, dim=-1), F.softmax(logits_1, dim=-1))\n",
        "        return alpha * (loss_kl_1 + loss_kl_2) / 2\n",
        "\n",
        "    def prediction_step(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "        prediction_loss_only: bool,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "\n",
        "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        if ignore_keys is None:\n",
        "            if hasattr(self.model, \"config\"):\n",
        "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
        "            else:\n",
        "                ignore_keys = []\n",
        "\n",
        "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
        "        if has_labels:\n",
        "            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
        "            if len(labels) == 1:\n",
        "                labels = labels[0]\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if has_labels:\n",
        "                with self.autocast_smart_context_manager():\n",
        "                    loss, logits = self.compute_eval_loss(model, inputs, return_outputs=True)\n",
        "                loss = loss.mean().detach()\n",
        "\n",
        "            else:\n",
        "                loss = None\n",
        "                with self.autocast_smart_context_manager():\n",
        "                    outputs1 = model(\n",
        "                        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
        "                    )\n",
        "                    outputs2 = model(\n",
        "                        input_ids=inputs[\"input_ids2\"], attention_mask=inputs[\"attention_mask2\"]\n",
        "                    )\n",
        "\n",
        "                    logits = (outputs1.logits + outputs2.logits) / 2\n",
        "\n",
        "        if prediction_loss_only:\n",
        "            return (loss, None, None)\n",
        "\n",
        "        if len(logits) == 1:\n",
        "            logits = logits[0]\n",
        "\n",
        "        return (loss, logits, labels)\n",
        "\n",
        "    def compute_loss(self, model, inputs):\n",
        "        num_labels = 2\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        # cls code1 sep sep code2 sep\n",
        "        outputs1 = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "        logits1 = outputs1.logits\n",
        "\n",
        "        # cls code2 sep sep code1 sep\n",
        "        outputs2 = model(input_ids=inputs[\"input_ids2\"], attention_mask=inputs[\"attention_mask2\"])\n",
        "        logits2 = outputs2.logits\n",
        "\n",
        "        # Crossentropy Loss\n",
        "        loss_fct_1 = nn.CrossEntropyLoss()\n",
        "        loss_nll = (\n",
        "            loss_fct_1(logits1.view(-1, num_labels), labels.view(-1))\n",
        "            + loss_fct_1(logits2.view(-1, num_labels), labels.view(-1))\n",
        "        ) / 2\n",
        "\n",
        "        # KL-Divergence Loss\n",
        "        loss_fct_2 = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        loss_kl = self.get_kl_loss(loss_fct_2, logits1, logits2)\n",
        "        return loss_nll + loss_kl\n",
        "\n",
        "    def compute_eval_loss(self, model, inputs, return_outputs=False):\n",
        "        num_labels = 2\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        outputs1 = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "        outputs2 = model(input_ids=inputs[\"input_ids2\"], attention_mask=inputs[\"attention_mask2\"])\n",
        "\n",
        "        logits = (outputs1.logits + outputs2.logits) / 2\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, logits) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-FhSBsWqrrg"
      },
      "source": [
        "## PLBART-Large Encoder\n",
        "\n",
        "### \"RBERT: Enriching Pre-Training Language Model with Entity Information for Relation Classification\" (https://arxiv.org/pdf/1905.08284.pdf)\n",
        "### \"Unified Pre-training for Program Understanding and Generation\"(https://arxiv.org/abs/2103.06333.pdf)\n",
        "\n",
        "\n",
        "GraphCodeRoberaì˜ ê²½ìš°ëŠ” ëª¨ë¸ì€ í›Œë¥­í•˜ê²Œ ê¸°í•™ìŠµë˜ì—ˆìœ¼ë‚˜ í† í¬ë‚˜ì´ì €ê°€ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤. ë°ì´í„°ì…‹ì˜ 75%ë§Œì´ 512 í† í° ì•ˆì— ë“¤ì–´ê°‘ë‹ˆë‹¤. ë°ì´í„°ì…‹ì´ ì§€ë‚˜ì¹˜ê²Œ ì˜ê²Œ ìª¼ê°œì§„ë‹¤ëŠ” ë‹¨ì ì„ ì €í¬ íŒ€ì—ì„œëŠ” ì–´ë–»ê²Œë“  ê·¹ë³µí•´ë³´ë ¤ê³  í–ˆìŠµë‹ˆë‹¤. ë˜í•œ Base ëª¨ë¸ ë°–ì— ì œê³µì´ ì•ˆ ëœë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "ì´ì— ë°˜í•´ PLBart ê°™ì€ ê²½ìš°ëŠ” Base ëª¨ë¸ì´ ì•„ë‹ˆë¼ Large ëª¨ë¸ê¹Œì§€ ì œê³µí•˜ë©°,  ì„ë² ë”©ì´ ê¹Šì´ê°€ 1024 ì°¨ì›ìœ¼ë¡œì¨ ë” ë§ì€ ë‚´ìš©ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ë¬´ì—‡ë³´ë‹¤ 92%ì˜ ë°ì´í„°ì…‹ì´ 512 í† í°ì•ˆì— ë“¤ì–´ê°‘ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, 12 Layer Encoder - 12 Layer Decoderì´ë¼ëŠ” ì ì—ì„œ ëª¨ë¸ì´ ì§€ë‚˜ì¹˜ê²Œ í¬ê¸° ë•Œë¬¸ì— í›ˆë ¨ì‹œí‚¤ê¸°ì—ëŠ” ë¹„íš¨ìœ¨ì ì´ë¼ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ Encoder - Decoder ëª¨ë¸ì„ ì „ë¶€ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ëŠ”, Encoder ë¶€ë¶„ë§Œ ë”°ì™€ì„œ ì‚¬ìš©í–ˆìœ¼ë©° ì´ ë°©ë²•ì´ Batch sizeë¥¼ ëŠ˜ë¦¬ê¸°ì—ë„ ì¢‹ê³  í›ˆë ¨ë„ ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "BARTëŠ” ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ëŠ” ë‹¤ë¥´ê²Œ, CLS í† í°ì´ ì—†ìŠµë‹ˆë‹¤. ë³´í†µ BARTë¡œ ë¶„ë¥˜ì‘ì—…ì„ í•  ë•Œ, SEPí† í°ê³¼ EOS í† í°ì´ ë™ì¼í•˜ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ í† í°ì„ ì¶”ì¶œí•˜ì—¬ì„œ ë¶„ë¥˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì´ë•Œ, PLBARTì˜ ë…¼ë¬¸ì—ì„œ íŠ¹ì´í•˜ê²Œ \"python\" í† í°ë„ ì§‘ì–´ë„£ì–´ì„œ ê¸°í•™ìŠµì„ ì§„í–‰í–ˆë‹¤ëŠ” ê±¸ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë˜í•œ 92%ì˜ ë°ì´í„°ê°€ ì „ë¶€ 512í† í° ì•ˆì— ë“¤ì–´ê°„ë‹¤ëŠ” ì ì„ ê°ì•ˆí•˜ì—¬ Python Tokenì„ í¬í•¨í•œ Code1 ë¬¸ì¥, Code2 ë¬¸ì¥ ë‹¨ìœ„ë¡œ Tokenë“¤ì˜ hidden stateì„ í‰ê· ì‹œì¼œì„œ RBERT ë°©ì‹ìœ¼ë¡œ ë¶„ë¥˜ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS8apgMVqtNW"
      },
      "outputs": [],
      "source": [
        "class VHBartEncoderForSequenceClassification(PLBartModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.entity_fc_layer = FCLayer(\n",
        "            self.config.hidden_size, self.config.hidden_size, self.config.dropout_rate\n",
        "        )\n",
        "        self.proj_fc_layer = FCLayer(\n",
        "            self.config.hidden_size * 4, self.config.hidden_size, self.config.dropout_rate\n",
        "        )\n",
        "        self.label_classifier = FCLayer(\n",
        "            self.config.hidden_size * 3,\n",
        "            self.config.num_labels,\n",
        "            self.config.dropout_rate,\n",
        "            use_activation=True,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, attention_mask, code1_mask, code2_mask, last_token_index, labels=None,\n",
        "    ):\n",
        "\n",
        "        outputs = self.encoder(\n",
        "            input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True,\n",
        "        )\n",
        "\n",
        "        # Global token's 4 hidden states concatenation and  projection result\n",
        "        idx_seq = torch.arange(input_ids.size(0)).to(input_ids.device)\n",
        "        # print(idx_seq)\n",
        "        cls_concat = torch.cat(\n",
        "            tuple(\n",
        "                [outputs[\"hidden_states\"][i][idx_seq, last_token_index] for i in [-4, -3, -2, -1]]\n",
        "            ),\n",
        "            dim=-1,\n",
        "        )\n",
        "        cls_output = self.proj_fc_layer(cls_concat)\n",
        "\n",
        "        # Global average on sentences\n",
        "        sequence_output = outputs[\"last_hidden_state\"]\n",
        "\n",
        "        code1_sentence_h = self.entity_average(\n",
        "            sequence_output, code1_mask\n",
        "        )  # token in between code1 entities ->\n",
        "        code1_sentence_h = self.entity_fc_layer(\n",
        "            code1_sentence_h\n",
        "        )  # code1 entity's fully connected layer | yellow on diagram\n",
        "\n",
        "        # Average on code2 sentence\n",
        "        code2_sentence_h = self.entity_average(\n",
        "            sequence_output, code2_mask\n",
        "        )  # token in between code2 entities\n",
        "        code2_sentence_h = self.entity_fc_layer(\n",
        "            code2_sentence_h\n",
        "        )  # code2 entity's fully connected layer | red on diagram\n",
        "\n",
        "        # Concat: global token,code1 average, code2 average\n",
        "        concat = torch.cat(\n",
        "            [\n",
        "                code1_sentence_h,\n",
        "                code2_sentence_h,\n",
        "                cls_output,\n",
        "            ],\n",
        "            dim=-1,\n",
        "        )\n",
        "\n",
        "        # yield logit from label classifier\n",
        "        logits = self.label_classifier(concat)\n",
        "        prob = nn.functional.softmax(logits)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            # print(logits)\n",
        "            # print(labels)\n",
        "            labels = labels.squeeze(-1)\n",
        "            loss = loss_fct(logits, labels)\n",
        "            return loss, prob\n",
        "        else:\n",
        "            return prob\n",
        "\n",
        "    def entity_average(self, hidden_output, e_mask):\n",
        "        \"\"\"\n",
        "        Average the entity hidden state vectors (H_i ~ H_j)\n",
        "        :param hidden_output: [batch_size, j-i+1, dim]\n",
        "        :param e_mask: [batch_size, max_seq_len]\n",
        "                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\n",
        "        :return: [batch_size, dim]\n",
        "        \"\"\"\n",
        "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
        "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
        "\n",
        "        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
        "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n",
        "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n",
        "        return avg_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aHBAqn-qtwU"
      },
      "source": [
        "## CodeT5 Encoder\n",
        "\n",
        "### \"An Improved Baseline for Sentence-level Relation Extraction\"(https://arxiv.org/abs/2102.01373)\n",
        "### \"EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks\"(https://arxiv.org/pdf/2110.08426.pdf)\n",
        "\n",
        "ë§ˆì°¬ê°€ì§€ë¡œ CodeT5ë„ ê²½ìš°ëŠ” í† í¬ë‚˜ì´ì €ê°€ ë¹„êµì  íš¨ìœ¨ì ì´ë¼ì„œ, ë°ì´í„°ì…‹ì˜ 82%ê°€ 512 í† í° ì•ˆì— ë“¤ì–´ê°‘ë‹ˆë‹¤. PLBARTì™€ ìœ ì‚¬í•˜ê²Œ Encoderë§Œ ë”°ì™€ì„œ í›ˆë ¨ì„ ì‹œì¼°ìŠµë‹ˆë‹¤.\n",
        "\n",
        "CodeT5ë¥¼ í•™ìŠµì‹œí‚¬ ë•Œ CLS í† í°ë¿ë§Œ ì•„ë‹ˆë¼ SEP í† í°ì˜ ì •ë³´ê¹Œì§€ í™œìš©í•´ì„œ, ë‘ ë¬¸ì¥ê°„ì˜ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì¶”ê°€ì ì¸ Punctuationì„ ì œê³µí•˜ì§€ë§Œ, ì´ë²ˆ ëŒ€íšŒëŠ” ì½”ë“œ ê¸¸ì´ê°€ ì¤‘ìš”í–ˆê¸° ë•Œë¬¸ì— CLS, SEPë§Œ ì¶”ì¶œí•˜ëŠ” ê±¸ë¡œ ì‚´ì§ ë³€í˜•í•´ì„œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë˜í•œ ì›ë˜ Decoderì—ê²Œ ì •ë³´ë¥¼ ë„˜ê²¨ì£¼ëŠ” Layerì—ì„œ ë¶„ë¥˜ ì‘ì—…ì„ í•˜ëŠ” ê²ƒì´ ë¶ˆì•ˆì •í•  ê²ƒì„ ìš°ë ¤í•˜ì—¬ Sentence Averageë„ ì¶”ì¶œí•˜ì˜€ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it0PS9_SqvN2"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from transformers.models.t5.modeling_t5 import T5EncoderModel, T5Config, T5PreTrainedModel, T5Stack\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n",
        "from utils.heads import (\n",
        "    AdaptivePooler,\n",
        "    MeanPooler,\n",
        "    FCLayer,\n",
        ")\n",
        "\n",
        "\n",
        "class VHT5EncoderForSequenceClassification(T5PreTrainedModel):\n",
        "    \"\"\" \n",
        "    Using [CLS token hidden states, Input Sequence Weighted Average, SEP token hidden states] for classification\n",
        "    \"\"\"\n",
        "\n",
        "    _keys_to_ignore_on_load_missing = [\n",
        "        r\"encoder\\.embed_tokens\\.weight\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, config: T5Config, dropout=0.1, pooler=\"adaptive\"):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
        "\n",
        "        encoder_config = copy.deepcopy(config)\n",
        "        encoder_config.use_cache = False\n",
        "        encoder_config.is_encoder_decoder = False\n",
        "        self.encoder = T5Stack(encoder_config, self.shared)\n",
        "        pooler_class = AdaptivePooler if pooler == \"adaptive\" else MeanPooler\n",
        "        self.pooler = pooler_class(input_size=config.hidden_size)\n",
        "\n",
        "        # FC Layers\n",
        "        self.cls_fc_layer = FCLayer(\n",
        "            self.config.hidden_size * 4,\n",
        "            self.config.hidden_size,\n",
        "            self.config.dropout_rate,\n",
        "            use_activation=True,\n",
        "        )\n",
        "\n",
        "        self.sep_fc_layer = FCLayer(\n",
        "            self.config.hidden_size * 4,\n",
        "            self.config.hidden_size,\n",
        "            self.config.dropout_rate,\n",
        "            use_activation=True,\n",
        "        )\n",
        "\n",
        "        self.label_classifier = FCLayer(\n",
        "            self.config.hidden_size * 3,\n",
        "            self.config.num_labels,\n",
        "            self.config.dropout_rate,\n",
        "            use_activation=False,\n",
        "        )\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "        # Model parallel\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "\n",
        "    def parallelize(self, device_map=None):\n",
        "        self.device_map = (\n",
        "            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n",
        "            if device_map is None\n",
        "            else device_map\n",
        "        )\n",
        "        assert_device_map(self.device_map, len(self.encoder.block))\n",
        "        self.encoder.parallelize(self.device_map)\n",
        "        self.classifier = self.classifier.to(self.encoder.first_device)\n",
        "        self.model_parallel = True\n",
        "\n",
        "    def deparallelize(self):\n",
        "        self.encoder.deparallelize()\n",
        "        self.encoder = self.encoder.to(\"cpu\")\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.shared = new_embeddings\n",
        "        self.encoder.set_input_embeddings(new_embeddings)\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        last_token_index=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # get adaptive pooler for the given sequence\n",
        "        weights, pooled_output = self.pooler(outputs[0], mask=attention_mask)\n",
        "\n",
        "        # Global token's 4 hidden states concatenation and  projection result\n",
        "        idx_seq = torch.arange(input_ids.size(0)).to(input_ids.device)\n",
        "\n",
        "        cls_concat = torch.cat(\n",
        "            tuple([outputs[\"hidden_states\"][i][idx_seq, 0, :] for i in [-4, -3, -2, -1]]), dim=-1\n",
        "        )\n",
        "        cls_output = self.cls_fc_layer(cls_concat)\n",
        "\n",
        "        sep_concat = torch.cat(\n",
        "            tuple(\n",
        "                [outputs[\"hidden_states\"][i][idx_seq, last_token_index] for i in [-4, -3, -2, -1]]\n",
        "            ),\n",
        "            dim=-1,\n",
        "        )\n",
        "        sep_output = self.sep_fc_layer(sep_concat)\n",
        "\n",
        "        concat = torch.cat([cls_output, pooled_output, sep_output], dim=-1)\n",
        "\n",
        "        logits = self.label_classifier(concat)\n",
        "\n",
        "        loss = None\n",
        "        outputs.hidden_states = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (\n",
        "                    labels.dtype == torch.long or labels.dtype == torch.int\n",
        "                ):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej8kd3E0rtej"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "ğŸ’¬ ì „ì²˜ë¦¬ ğŸ’¬\n",
        "ì „ì²˜ë¦¬ ë¶€ë¶„ì—ì„œ ê°€ì¥ ì£¼ìš”í–ˆë˜ ê²ƒì€ import ë¶€ë¶„ì„ ì•„ì˜ˆ ì œê±°í•˜ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤. ImportëŠ” ëŒ€ë¶€ë¶„ ì‚¬ìš©ì„ í•˜ì§€ ì•Šê³ , ì‚¬ìš©í•œë‹¤ê³  í•´ë„ ì–´ì°¨í”¼ ì½”ë“œì—ì„œ ë“œëŸ¬ë‚˜ë‹ˆ, import ë¶€ë¶„ì„ ì „ë¶€ ì œì™¸í•˜ìëŠ” ìƒí•˜ë‹˜ì˜ ì•„ì´ë””ì–´ëŠ” ê¸°ë°œí–ˆìŠµë‹ˆë‹¤ ğŸ‘ \n",
        "\n",
        "ê·¸ ì™¸ì— ì—¬ëŸ¬ ê°œì˜ space, tab ë“¤ì„ í•œ ê°œì˜ spaceë¡œ ì œê±°í•˜ëŠ” ì „ì²˜ë¦¬ë¥¼ ê±°ì³¤ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEmfgYU7ruls"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "class FunctionPreprocessor:\n",
        "    def __init__(self,):\n",
        "        pass\n",
        "\n",
        "    def get_function(self, code):\n",
        "        results = []\n",
        "        fn_list = re.findall(\"\\ndef [a-zA-Z0-9_]+\\(\", code)\n",
        "\n",
        "        for fn in fn_list:\n",
        "            results.append(fn[4:-1].strip())\n",
        "        return results\n",
        "\n",
        "    def determine_function(self, code, function_name):\n",
        "        num = len(re.findall(\"[^a-zA-Z]\" + function_name + \"[^a-zA-Z]\", code))\n",
        "        return False if num <= 1 else True\n",
        "\n",
        "    def delete_function(self, code, name):\n",
        "        start_id, _ = re.search(\"def \" + name, code).span()\n",
        "        ptr = start_id\n",
        "\n",
        "        while ptr < len(code) - 1:\n",
        "            if code[ptr] == \"\\n\" and re.search(\"[a-zA-Z]\", code[ptr + 1]) is not None:\n",
        "                break\n",
        "            ptr += 1\n",
        "\n",
        "        if ptr != len(code) - 1:\n",
        "            end_id = ptr\n",
        "            code = code[:start_id] + code[end_id:]\n",
        "\n",
        "        return code\n",
        "\n",
        "    def preprocess(self, code):\n",
        "        code = \"\\n\" + code\n",
        "        fn_list = self.get_function(code)\n",
        "        if len(fn_list) == 0:\n",
        "            return code\n",
        "\n",
        "        for fn in fn_list:\n",
        "            flag = self.determine_function(code, fn)\n",
        "\n",
        "            if flag == False:\n",
        "                code = self.delete_function(code, fn)\n",
        "\n",
        "        return code\n",
        "\n",
        "    def __call__(self, datasets):\n",
        "        code1_list = []\n",
        "        code2_list = []\n",
        "\n",
        "        size = len(datasets[\"code1\"])\n",
        "        for i in range(size):\n",
        "            code1 = self.preprocess(datasets[\"code1\"][i])\n",
        "            code2 = self.preprocess(datasets[\"code2\"][i])\n",
        "\n",
        "            code1_list.append(code1)\n",
        "            code2_list.append(code2)\n",
        "\n",
        "        datasets[\"code1\"] = code1_list\n",
        "        datasets[\"code2\"] = code2_list\n",
        "        return datasets\n",
        "\n",
        "\n",
        "class AnnotationPreprocessor:\n",
        "    def __init__(self,):\n",
        "        pass\n",
        "\n",
        "    def search(self, sen_list, string):\n",
        "        for i, sen in enumerate(sen_list):\n",
        "            if string in sen:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    def delete_annotation_block(self, code, string):\n",
        "        sens = [sen for sen in code.split(\"\\n\")]\n",
        "\n",
        "        start_id = self.search(sens, string)\n",
        "        end_id = self.search(sens[start_id + 1 :], string)\n",
        "        if end_id != -1:\n",
        "            end_id += start_id + 1\n",
        "            code = sens[:start_id] + sens[end_id + 1 :]\n",
        "        else:\n",
        "            code = sens[:start_id] + sens[start_id + 1 :]\n",
        "\n",
        "        code = \"\\n\".join(code)\n",
        "        return code\n",
        "\n",
        "    def delete_block(self, code, string):\n",
        "        while string in code:\n",
        "            code = self.delete_annotation_block(code, string)\n",
        "        return code\n",
        "\n",
        "    def delete_annotation(self, code):\n",
        "        sens = code.split(\"\\n\")\n",
        "\n",
        "        sens_processed = []\n",
        "        for sen in sens:\n",
        "            if \"#\" in sen:\n",
        "                index = sen.index(\"#\")\n",
        "                sen = sen[:index]\n",
        "            sens_processed.append(sen)\n",
        "\n",
        "        return \"\\n\".join(sens_processed)\n",
        "\n",
        "    def delete_import(self, code):\n",
        "        sens = code.split(\"\\n\")\n",
        "\n",
        "        sens_processed = []\n",
        "        for sen in sens:\n",
        "            if \"import\" not in sen:\n",
        "                sens_processed.append(sen)\n",
        "\n",
        "        return \"\\n\".join(sens_processed)\n",
        "\n",
        "    def preprocess(self, code):\n",
        "        code = self.delete_block(code, '\"\"\"')\n",
        "        code = self.delete_block(code, \"'''\")\n",
        "        code = self.delete_annotation(code)\n",
        "        code = self.delete_import(code)\n",
        "        code = re.sub(\"\\s+\", \" \", code).strip()\n",
        "        return code\n",
        "\n",
        "    def __call__(self, datasets):\n",
        "        code1_list = []\n",
        "        code2_list = []\n",
        "\n",
        "        size = len(datasets[\"code1\"])\n",
        "        for i in range(size):\n",
        "            code1 = self.preprocess(datasets[\"code1\"][i])\n",
        "            code2 = self.preprocess(datasets[\"code2\"][i])\n",
        "\n",
        "            code1_list.append(code1)\n",
        "            code2_list.append(code2)\n",
        "\n",
        "        datasets[\"code1\"] = code1_list\n",
        "        datasets[\"code2\"] = code2_list\n",
        "        return datasets\n",
        "\n",
        "\n",
        "class BasePreprocessor:\n",
        "    def __init__(self,):\n",
        "        pass\n",
        "\n",
        "    def search(self, sen_list, string):\n",
        "        for i, sen in enumerate(sen_list):\n",
        "            if string in sen:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    def delete_annotation_block(self, code, string):\n",
        "        sens = [sen for sen in code.split(\"\\n\")]\n",
        "\n",
        "        start_id = self.search(sens, string)\n",
        "        end_id = self.search(sens[start_id + 1 :], string)\n",
        "        if end_id != -1:\n",
        "            end_id += start_id + 1\n",
        "            code = sens[:start_id] + sens[end_id + 1 :]\n",
        "        else:\n",
        "            code = sens[:start_id] + sens[start_id + 1 :]\n",
        "\n",
        "        code = \"\\n\".join(code)\n",
        "        return code\n",
        "\n",
        "    def delete_block(self, code, string):\n",
        "        while string in code:\n",
        "            code = self.delete_annotation_block(code, string)\n",
        "        return code\n",
        "\n",
        "    def delete_annotation(self, code):\n",
        "        sens = code.split(\"\\n\")\n",
        "\n",
        "        sens_processed = []\n",
        "        for sen in sens:\n",
        "            if \"#\" in sen:\n",
        "                index = sen.index(\"#\")\n",
        "                sen = sen[:index]\n",
        "            sens_processed.append(sen)\n",
        "\n",
        "        return \"\\n\".join(sens_processed)\n",
        "\n",
        "    def delete_import(self, code):\n",
        "        sens = code.split(\"\\n\")\n",
        "\n",
        "        sens_processed = []\n",
        "        for sen in sens:\n",
        "            if \"import\" not in sen:\n",
        "                sens_processed.append(sen)\n",
        "\n",
        "        return \"\\n\".join(sens_processed)\n",
        "\n",
        "    def preprocess(self, code):\n",
        "        code = self.delete_block(code, '\"\"\"')\n",
        "        code = self.delete_block(code, \"'''\")\n",
        "        code = self.delete_annotation(code)\n",
        "        code = self.delete_import(code)\n",
        "        # code = re.sub('\\s+', ' ', code).strip()\n",
        "        return code\n",
        "\n",
        "    def __call__(self, datasets):\n",
        "        code1_list = []\n",
        "        code2_list = []\n",
        "        label_list = []\n",
        "\n",
        "        size = len(datasets[\"code1\"])\n",
        "        for i in range(size):\n",
        "            code1 = self.preprocess(datasets[\"code1\"][i])\n",
        "            code2 = self.preprocess(datasets[\"code2\"][i])\n",
        "            if \"similar\" in datasets:\n",
        "                label_list = [int(x) for x in datasets[\"similar\"]]\n",
        "\n",
        "            code1_list.append(code1)\n",
        "            code2_list.append(code2)\n",
        "\n",
        "        datasets[\"code1\"] = code1_list\n",
        "        datasets[\"code2\"] = code2_list\n",
        "        if \"similar\" in datasets:\n",
        "            datasets[\"similar\"] = label_list\n",
        "\n",
        "        return datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBTmSodGsVh3"
      },
      "source": [
        "### ë°ì´í„° ì œì‘ ì½”ë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMVLoR18sWMG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "\n",
        "# read .py documents\n",
        "df_train = pd.DataFrame({}, columns=[\"file_name\", \"question_label\", \"code\"])\n",
        "TRAIN_PATH = \"/home/ubuntu/plclassification/code\"\n",
        "list_questions = glob.glob(os.path.join(TRAIN_PATH, \"*\"))\n",
        "\n",
        "list_index = []\n",
        "list_problem_names = []\n",
        "list_code_text = []\n",
        "\n",
        "for index, question in enumerate(list_questions):\n",
        "    problem_name = os.path.basename(question)\n",
        "    # extract numerics only from problem_name\n",
        "    problem_name = \"\".join(i for i in problem_name if i.isdigit())\n",
        "\n",
        "    for file in glob.glob(os.path.join(question, \"*.py\")):\n",
        "        file_base_name = os.path.basename(file)\n",
        "        # read text from .py file\n",
        "        with open(file, \"r\") as f:\n",
        "            code_text = f.read()\n",
        "            # append row with index, problem_name and code text\n",
        "            list_index.append(file_base_name)\n",
        "            list_problem_names.append(problem_name)\n",
        "            list_code_text.append(code_text)\n",
        "\n",
        "df_train[\"file_name\"] = list_index\n",
        "df_train[\"question_label\"] = list_problem_names\n",
        "# remove 0 in front of the number in question_label\n",
        "df_train[\"question_label\"] = df_train[\"question_label\"].str.replace(\"^0+\", \"\").astype(int)\n",
        "df_train[\"code\"] = list_code_text\n",
        "\n",
        "# create similar pairs\n",
        "df_similar = pd.DataFrame(\n",
        "    {}, columns=[\"code1\", \"code2\", \"similar\", \"code1_group\", \"code2_group\", \"pair_id\"]\n",
        ")\n",
        "\n",
        "for i in tqdm(range(1, 301)):\n",
        "    list_code1 = []\n",
        "    list_code2 = []\n",
        "    list_question_pair_id = []\n",
        "    list_group_id = []\n",
        "    # get corresponding dataframe where question_label == i\n",
        "    group_index = df_train[df_train[\"question_label\"] == i].index\n",
        "    df_group = df_train.iloc[group_index]\n",
        "\n",
        "    # make unique pair combination from group_index list\n",
        "    combinated_indices = list(combinations(group_index, 2))\n",
        "    for index in combinated_indices:\n",
        "        code1 = df_train.iloc[index[0]][\"code\"]\n",
        "        code2 = df_train.iloc[index[1]][\"code\"]\n",
        "        list_code1.append(code1)\n",
        "        list_code2.append(code2)\n",
        "        question_pair_id = index[0] ** 3 + index[1] ** 3 + index[0] + index[1]\n",
        "        list_question_pair_id.append(question_pair_id)\n",
        "\n",
        "    df_similar = df_similar.append(\n",
        "        pd.DataFrame(\n",
        "            {\n",
        "                \"code1\": list_code1,\n",
        "                \"code2\": list_code2,\n",
        "                \"similar\": 1,\n",
        "                \"code1_group\": i,\n",
        "                \"code2_group\": i,\n",
        "                \"pair_id\": list_question_pair_id,\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "df_similar = df_similar.reset_index(drop=True)\n",
        "df_similar[\"code1_group\"] = df_similar[\"code1_group\"].astype(int)\n",
        "df_similar[\"code2_group\"] = df_similar[\"code2_group\"].astype(int)\n",
        "print(df_similar.shape)\n",
        "\n",
        "# Create different pairs\n",
        "NUM = 120000\n",
        "df_different = pd.DataFrame({}, columns=[\"code1\", \"code2\", \"similar\"])\n",
        "\n",
        "for i in tqdm(range(1, 301)):\n",
        "    # get corresponding dataframe where question_label == i\n",
        "    group_index = df_train[df_train[\"question_label\"] == i].index\n",
        "    other_index = df_train[df_train[\"question_label\"] != i].index\n",
        "    sample_index = np.random.choice(group_index, NUM, replace=True)\n",
        "    other_sample_index = np.random.choice(other_index, NUM, replace=True)\n",
        "\n",
        "    # make unique pair id: multiplication - sum\n",
        "    question_pair_id = (\n",
        "        sample_index ** 3 + other_sample_index ** 3 + (sample_index + other_sample_index)\n",
        "    )\n",
        "    # make group pair id\n",
        "    other_group_index = df_train.iloc[other_sample_index][\"question_label\"].values\n",
        "\n",
        "    # append df_train.iloc[sample_index][\"code\"] to df_different[\"code1\"]\n",
        "    code1 = df_train.iloc[sample_index][\"code\"].values\n",
        "    code2 = df_train.iloc[other_sample_index][\"code\"].values\n",
        "    df_different = df_different.append(\n",
        "        pd.DataFrame(\n",
        "            {\n",
        "                \"code1\": code1,\n",
        "                \"code2\": code2,\n",
        "                \"similar\": 0,\n",
        "                \"code1_group\": i,\n",
        "                \"code2_group\": other_group_index.astype(int),\n",
        "                \"question_pair_id\": question_pair_id,\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "df_different = df_different.drop_duplicates(subset=[\"question_pair_id\"])\n",
        "df_different[\"code1_group\"] = df_different[\"code1_group\"].astype(int)\n",
        "df_different[\"code2_group\"] = df_different[\"code2_group\"].astype(int)\n",
        "print(df_different.shape)\n",
        "\n",
        "\n",
        "# import KFold from sklearn\n",
        "list_groups = list([i for i in range(1, 301)])\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "folds_10 = kf.get_n_splits(list_groups)\n",
        "\n",
        "for fold_index, (train_index, test_index) in enumerate(kf.split(list_groups)):\n",
        "\n",
        "    df_similar_train_fold = df_similar[df_similar[\"code1_group\"].isin(train_index)]\n",
        "    df_similar_val_fold = df_similar[df_similar[\"code1_group\"].isin(test_index)]\n",
        "\n",
        "    df_different_train_fold = df_different[\n",
        "        df_different[\"code1_group\"].isin(train_index)\n",
        "        & df_different[\"code2_group\"].isin(train_index)\n",
        "    ]\n",
        "    df_different_train_fold = df_different_train_fold.sample(len(df_similar_train_fold))\n",
        "    df_different_val_fold = df_different[\n",
        "        df_different[\"code1_group\"].isin(test_index) & df_different[\"code2_group\"].isin(test_index)\n",
        "    ]\n",
        "    df_different_val_fold = df_different_val_fold.sample(len(df_similar_val_fold))\n",
        "\n",
        "    df_train = pd.concat([df_similar_train_fold, df_different_train_fold])\n",
        "    # shuffle df_train\n",
        "    df_train = df_train.sample(frac=1)\n",
        "    # reset index for df_train\n",
        "    df_train = df_train.reset_index(drop=True)\n",
        "\n",
        "    df_val = pd.concat([df_similar_val_fold, df_different_val_fold])\n",
        "    # shuffle df_val\n",
        "    df_val = df_val.sample(frac=1)\n",
        "    # reset index for df_val\n",
        "    df_val = df_val.reset_index(drop=True)\n",
        "\n",
        "    print(df_train.shape, df_val.shape)\n",
        "    assert df_train.columns == df_val.columns\n",
        "\n",
        "    dataset_fold = DatasetDict()\n",
        "    dataset_train = Dataset.from_pandas(df_train)\n",
        "    dataset_val = Dataset.from_pandas(df_val)\n",
        "    dataset_fold[\"train\"] = dataset_train\n",
        "    dataset_fold[\"val\"] = dataset_val\n",
        "\n",
        "    name = f\"PoolC/{fold_index+1}-fold-clone-detection-600k-5fold\"\n",
        "    dataset_fold.push_to_hub(name, private=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLDaIRbOsYHn"
      },
      "source": [
        "ë¯¸ë¦¬ ì—…ë¡œë“œí•œ ë°ì´í„°ì…‹ì—ì„œ 90ë§Œê°œë¥¼ ëœë¤ì¶”ì¶œì„ í–ˆê¸° ë•Œë¬¸ì—, Fold ë³„ë¡œ ê°ˆë¼ì§„ ClassëŠ” ë™ì¼í•˜ì§€ë§Œ ì¶”ì¶œëœ 90ë§Œ ê°œì˜ ë°ì´í„°ì…‹ì€ ê° ëª¨ë¸ ë³„ë¡œ ìƒì´í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ì„œ ì•™ìƒë¸” ì„±ëŠ¥ì„ ë†’ì´ë ¤ê³  í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ì‘ì„±ì: ë°•ìƒí•˜, ì•ˆì˜ì§„\n",
        "\n",
        "> This Notebook has been released under the Apache 2.0 open source license.\n",
        "\n",
        "ëŒ“ê¸€ë¡œ ì˜ê²¬ì„ ê³µìœ í•´ì£¼ì‹œë©´ ì €í¬ì—ê²Œë„ ê³µë¶€í•  ê¸°íšŒê°€ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤ ğŸ™\n",
        "ê°ì‚¬í•©ë‹ˆë‹¤!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
