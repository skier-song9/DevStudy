{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70KtyQmmdKdl",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T04:12:20.386421Z",
     "start_time": "2024-02-18T04:12:20.375245Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4813,
     "status": "ok",
     "timestamp": 1707967590344,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "70KtyQmmdKdl",
    "outputId": "7656ebba-c93f-4bf0-efe6-e151eafdc366"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# # Mount Google Drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa8a7f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:00:51.802795Z",
     "start_time": "2024-02-19T14:00:46.324017Z"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1707967599994,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "fa8a7f05"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import category_encoders as ce\n",
    "# import copy\n",
    "# import polars as pl\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "# import plotly.express as px\n",
    "# %matplotlib inline\n",
    "# matplotlib.rcParams['font.family'] = 'Malgun Gothic' # 한글 패치\n",
    "# Preprocessing & Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Modeling\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor, XGBRFRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
    "# from sklearn.base import ClassifierMixin\n",
    "\n",
    "# CatBoost\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss,mean_squared_error\n",
    "import sklearn\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import datetime # ⚠️2019년 12월30일과 31일의 week of year가 1인 오류가 있음\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "import holidays\n",
    "\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from num2words import num2words\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1fea9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:00:51.850115Z",
     "start_time": "2024-02-19T14:00:51.806793Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1707967811792,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "bc1fea9f",
    "outputId": "e060e065-27b4-4bd1-c8fc-db3c2b3904f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_row',None)\n",
    "pd.set_option('display.max_column',None)\n",
    "\n",
    "### Setting universal random_state\n",
    "np.random.seed(142)\n",
    "random.seed(142)\n",
    "sklearn.utils.check_random_state(142)\n",
    "torch.manual_seed(142)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark=False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072c99fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:00:51.882959Z",
     "start_time": "2024-02-19T14:00:51.853115Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1707967813304,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "072c99fa"
   },
   "outputs": [],
   "source": [
    "class PrepareData():\n",
    "    def __init__(self,stock_fp,news_fp):\n",
    "        self.stock_filepath = stock_fp\n",
    "        self.news_filepath = news_fp\n",
    "        self.topic_classes = ['CEO', 'CFO', 'Layoffs', 'Political', 'PressRelease', 'Undefined',\n",
    "       'cramer', 'earnings', 'gold', 'manda', 'paylimitwall', 'paywall',\n",
    "       'product', 'recession', 'tanalysis'] # undefined의 class가 5\n",
    "\n",
    "    def load_data(self):\n",
    "        stock = pd.read_csv(self.stock_filepath,index_col=0)\n",
    "        news = pd.read_csv(self.news_filepath,index_col=0)\n",
    "        ### parse date manually\n",
    "        stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "        news['date'] = pd.to_datetime(news['date'])\n",
    "        return stock, news\n",
    "\n",
    "    def merging(self, stock, news):\n",
    "        ### fill na value of PINS column\n",
    "        stock['PINS'] = stock['PINS'].fillna(stock['PINS'].iloc[75])\n",
    "\n",
    "        ### drop 'news_id' column\n",
    "#         news = news.drop(columns=['news_id'])\n",
    "\n",
    "        ### add date range from 18.01.02 to 18.12.31\n",
    "        temp_range = pd.DataFrame(dict(zip(stock.columns,[pd.date_range(start='2018-01-02',end='2018-12-31'),\n",
    "                                0,0,0,0, # 4\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 16\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 28\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 36\n",
    "                                0 # 37\n",
    "                            ])))\n",
    "        stock_inc = pd.concat([temp_range,stock],axis=0)\n",
    "\n",
    "        ### merge stock_inc and news\n",
    "        # left = stock_inc\n",
    "        # on = date\n",
    "        # how = left\n",
    "        # rename 'date' to 'Date' of news df\n",
    "        news = news.rename(columns={'date':'Date'})\n",
    "        merged = pd.merge(left=stock_inc,right=news,on='Date',how='left')\n",
    "\n",
    "        ### Cut before 2018-02-13\n",
    "        merged = merged[42:].reset_index(drop=True)\n",
    "\n",
    "        # fill na with latest non-null values\n",
    "        columns_to_fill = ['source_name', 'topics', 'rank_score',\n",
    "                        'sentiment_Negative','sentiment_Neutral',\n",
    "                        'sentiment_Positive', 'type_Article', 'type_Video']\n",
    "        merged_fillna = merged.copy()\n",
    "        for column in columns_to_fill:\n",
    "            merged_fillna[column].fillna(method='ffill',inplace=True)\n",
    "\n",
    "        ### add moving average to sentiments\n",
    "        ma_nums = [5,60,120]\n",
    "        def mode_window(window):\n",
    "            return window.mode().iloc[0] if not len(window.mode())==0 else None\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MA_sent_Neg']=merged_fillna['sentiment_Negative'].rolling(\n",
    "            window=num).mean()\n",
    "            merged_fillna[f'{num}MA_sent_Neu']=merged_fillna['sentiment_Neutral'].rolling(\n",
    "            window=num).mean()\n",
    "            merged_fillna[f'{num}MA_sent_Pos']=merged_fillna['sentiment_Positive'].rolling(\n",
    "            window=num).mean()\n",
    "        ### add moving mode to sentiments\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MM_sent_Neg']=merged_fillna['sentiment_Negative'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "            merged_fillna[f'{num}MM_sent_Neu']=merged_fillna['sentiment_Neutral'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "            merged_fillna[f'{num}MM_sent_Pos']=merged_fillna['sentiment_Positive'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "        ### adding moving mode to topics\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MM_topics']=merged_fillna['topics'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "\n",
    "        ### drop before 2019-01-02\n",
    "        total_df = merged_fillna.iloc[322:]\n",
    "        total_df = total_df.reset_index(drop=True)\n",
    "\n",
    "        ### drop unnecessaray columns\n",
    "        drop_cols = ['source_name','topics','rank_score',\n",
    "                    'sentiment_Negative','sentiment_Neutral',\n",
    "                    'sentiment_Positive','type_Article','type_Video']\n",
    "        total_df = total_df.drop(columns=drop_cols)\n",
    "\n",
    "        return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e695f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:00:51.897906Z",
     "start_time": "2024-02-19T14:00:51.887279Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1707967816443,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "96e695f0"
   },
   "outputs": [],
   "source": [
    "# 전체 오차율\n",
    "def error_ratio(pred, true):\n",
    "    return np.mean(np.abs(pred-true)/true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c21dd4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:00:55.189502Z",
     "start_time": "2024-02-19T14:00:51.902129Z"
    },
    "executionInfo": {
     "elapsed": 9234,
     "status": "ok",
     "timestamp": 1707967826109,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "6c21dd4c"
   },
   "outputs": [],
   "source": [
    "stock_filepath = '../../data/stock_price/netflix_60.csv' # 각자 파일 경로 설정\n",
    "news_filepath = '../../data/scraping/news_processed_filtered_2.csv'\n",
    "# stock_filepath = './drive/MyDrive/Colab Notebooks/data/bitamin_mini_project/netflix_60.csv'\n",
    "# news_filepath = './drive/MyDrive/Colab Notebooks/data/bitamin_mini_project/news_processed_filtered_2.csv'\n",
    "loader = PrepareData(stock_filepath,news_filepath)\n",
    "stock_df, news_df=loader.load_data() # >> 감성분석 미포함으로 모델 돌릴 땐 stock_df 바로 사용하면 됨\n",
    "total_df = loader.merging(stock=stock_df, news=news_df) # 주식데이터셋에 감성분석,토픽 포함시킨 전체 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26138410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:00:55.205493Z",
     "start_time": "2024-02-19T14:00:55.191506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24.99\n",
       "1    24.99\n",
       "2    24.99\n",
       "Name: PINS, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df['PINS'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8325c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:00:55.237078Z",
     "start_time": "2024-02-19T14:00:55.208035Z"
    }
   },
   "outputs": [],
   "source": [
    "### Set index as Date\n",
    "stock_df = stock_df.set_index('Date')\n",
    "total_df = total_df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5DzQJJ6MV9uw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:58:07.002014Z",
     "start_time": "2024-02-19T13:58:06.988322Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1707976805494,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "5DzQJJ6MV9uw",
    "outputId": "a3f73640-c5a3-4a93-aeb7-1bebb6e31c18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>468.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>470.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close Price\n",
       "Date                   \n",
       "2024-01-02       468.50\n",
       "2024-01-03       470.26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filepath = '../../data/test.csv'\n",
    "# test_filepath = './drive/MyDrive/Colab Notebooks/data/bitamin_mini_project/test.csv'\n",
    "y_test = pd.read_csv(test_filepath,index_col=0)\n",
    "y_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92f532c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:58:07.017312Z",
     "start_time": "2024-02-19T13:58:07.005011Z"
    }
   },
   "outputs": [],
   "source": [
    "### 각종 함수 및 클래스 정의용\n",
    "def split_xy(dataset, time_steps, y_column):\n",
    "        x, y = list(), list()\n",
    "        for i in range(len(dataset)):\n",
    "            x_end_number = i + time_steps\n",
    "            y_end_number = x_end_number + y_column\n",
    "\n",
    "            if y_end_number > len(dataset):\n",
    "                break\n",
    "            tmp_x = dataset.iloc[i:x_end_number, ]  # Adjusted for Pandas\n",
    "            tmp_y = dataset.iloc[x_end_number:y_end_number, 5]  # Adjusted for Pandas\n",
    "            x.append(tmp_x.values)  # Convert to numpy array\n",
    "            y.append(tmp_y.values)  # Convert to numpy array\n",
    "\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=120):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Model definition using Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead=4, num_layers=2, dropout=0.2, output_size=10,max_len=120):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Linear(input_dim, d_model,bias=True)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout,max_len)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead,bias=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, output_size,bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.decoder(x[:, -1, :])\n",
    "        return x\n",
    "    \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "import copy\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a542a829",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:58:07.220480Z",
     "start_time": "2024-02-19T13:58:07.200937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>error_ratio_total</th>\n",
       "      <th>r_squared_total</th>\n",
       "      <th>p_value_total</th>\n",
       "      <th>error_ratio_stock</th>\n",
       "      <th>r_squared_stock</th>\n",
       "      <th>p_value_stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_size  batch_size  output_size  hidden_size  error_ratio_total  \\\n",
       "0         0           0            0            0                  0   \n",
       "\n",
       "   r_squared_total  p_value_total  error_ratio_stock  r_squared_stock  \\\n",
       "0                0              0                  0                0   \n",
       "\n",
       "   p_value_stock  \n",
       "0              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.DataFrame({\n",
    "    'seq_size':[0],\n",
    "    'batch_size':[0],\n",
    "    'output_size':[0],\n",
    "    'hidden_size':[0],\n",
    "    'error_ratio_total':[0],\n",
    "    'r_squared_total':[0],\n",
    "    'p_value_total':[0],\n",
    "    'error_ratio_stock':[0],\n",
    "    'r_squared_stock':[0],\n",
    "    'p_value_stock':[0]\n",
    "})\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7813fda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:58:08.140355Z",
     "start_time": "2024-02-19T13:58:08.128713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>val_loss_total</th>\n",
       "      <th>val_loss_stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_size  batch_size  output_size  hidden_size  val_loss_total  \\\n",
       "0         0           0            0            0               0   \n",
       "\n",
       "   val_loss_stock  \n",
       "0               0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df = pd.DataFrame({\n",
    "        'seq_size':[0],\n",
    "        'batch_size':[0],\n",
    "        'output_size':[0],\n",
    "        'hidden_size':[0],\n",
    "        'val_loss_total':[0],\n",
    "        'val_loss_stock':[0]\n",
    "    })\n",
    "val_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2e8b51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:58:27.407546Z",
     "start_time": "2024-02-19T13:58:23.492674Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 125\u001b[0m\n\u001b[0;32m    123\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model_total(x_batch)\n\u001b[0;32m    124\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion_total(outputs, y_batch)\n\u001b[1;32m--> 125\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m optimizer_total\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    127\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SEQUENCE_SIZEs = [30,60,120]\n",
    "# BATCH_SIZEs = [1,5,32,64,128]\n",
    "BATCH_SIZEs = [1,4,8]\n",
    "OUTPUT_SIZEs = [10,20]\n",
    "HIDDEN_SIZEs = [64,128,256]\n",
    "import itertools\n",
    "\n",
    "last_val_losses_total, last_val_losses_stock = [],[]\n",
    "\n",
    "\n",
    "for SEQUENCE_SIZE,BATCH_SIZE,OUTPUT_SIZE,HIDDEN_SIZE in itertools.product(SEQUENCE_SIZEs,BATCH_SIZEs,OUTPUT_SIZEs,HIDDEN_SIZEs):\n",
    "\n",
    "    x_total, y_total = split_xy(total_df,SEQUENCE_SIZE,OUTPUT_SIZE)\n",
    "    x_stock, y_stock = split_xy(stock_df, SEQUENCE_SIZE,OUTPUT_SIZE)\n",
    "    \n",
    "    ### Set Test data\n",
    "    x_test_total = np.reshape(total_df.iloc[-SEQUENCE_SIZE:].values,(1,SEQUENCE_SIZE,-1))\n",
    "    x_test_stock = np.reshape(stock_df.iloc[-SEQUENCE_SIZE:].values,(1,SEQUENCE_SIZE,-1))\n",
    "\n",
    "    ### Scaling\n",
    "    # reshape to 2d and scaling\n",
    "    ss = StandardScaler()\n",
    "    x_total_scaled = ss.fit_transform(np.reshape(x_total,(\n",
    "            x_total.shape[0],x_total.shape[1]*x_total.shape[2]\n",
    "        )))\n",
    "    x_test_total = ss.transform(np.reshape(x_test_total,(\n",
    "            x_test_total.shape[0],x_test_total.shape[1]*x_test_total.shape[2]\n",
    "        )))\n",
    "    x_stock_scaled = ss.fit_transform(np.reshape(x_stock,(\n",
    "            x_stock.shape[0],x_stock.shape[1]*x_stock.shape[2]\n",
    "        )))\n",
    "    x_test_stock = ss.transform(np.reshape(x_test_stock,(\n",
    "            x_test_stock.shape[0],x_test_stock.shape[1]*x_test_stock.shape[2]\n",
    "        )))\n",
    "\n",
    "    ### Train, Validation Split\n",
    "    tv_ratio = int(x_total_scaled.shape[0]*0.2) \n",
    "    x_train_total = x_total_scaled[:-tv_ratio]\n",
    "    x_val_total = x_total_scaled[-tv_ratio:]\n",
    "    y_train_total = y_total[:-tv_ratio]\n",
    "    y_val_total = y_total[-tv_ratio:]\n",
    "\n",
    "    tv_ratio = int(x_stock_scaled.shape[0]*0.2) \n",
    "    x_train_stock = x_stock_scaled[:-tv_ratio]\n",
    "    x_val_stock = x_stock_scaled[-tv_ratio:]\n",
    "    y_train_stock = y_stock[:-tv_ratio]\n",
    "    y_val_stock = y_stock[-tv_ratio:]\n",
    "\n",
    "    ### reshape to 3d\n",
    "    x_train_total = np.reshape(x_train_total,(-1,SEQUENCE_SIZE,x_total.shape[2]))\n",
    "    x_val_total = np.reshape(x_val_total,(-1,SEQUENCE_SIZE,x_total.shape[2]))\n",
    "    x_test_total = np.reshape(x_test_total,(1,SEQUENCE_SIZE,-1))\n",
    "\n",
    "    x_train_stock = np.reshape(x_train_stock,(-1,SEQUENCE_SIZE,x_stock.shape[2]))\n",
    "    x_val_stock = np.reshape(x_val_stock,(-1,SEQUENCE_SIZE,x_stock.shape[2]))\n",
    "    x_test_stock = np.reshape(x_test_stock,(1,SEQUENCE_SIZE,-1))\n",
    "    \n",
    "    ### to DataLoader\n",
    "    train_loader_total = DataLoader(\n",
    "            TensorDataset(torch.tensor(x_train_total,dtype=torch.float32),\n",
    "                          torch.tensor(y_train_total,dtype=torch.float32)),\n",
    "            batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader_total = DataLoader(\n",
    "            TensorDataset(torch.tensor(x_val_total,dtype=torch.float32),\n",
    "                          torch.tensor(y_val_total,dtype=torch.float32)),\n",
    "            batch_size=BATCH_SIZE, shuffle=False\n",
    "        )\n",
    "\n",
    "    train_loader_stock = DataLoader(\n",
    "            TensorDataset(torch.tensor(x_train_stock,dtype=torch.float32),\n",
    "                          torch.tensor(y_train_stock,dtype=torch.float32)),\n",
    "            batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader_stock = DataLoader(\n",
    "            TensorDataset(torch.tensor(x_val_stock,dtype=torch.float32),\n",
    "                          torch.tensor(y_val_stock,dtype=torch.float32)),\n",
    "            batch_size=BATCH_SIZE, shuffle=False\n",
    "        )\n",
    "\n",
    "    ### test data to Tensor\n",
    "    x_test_total = torch.tensor(x_test_total,dtype=torch.float32)\n",
    "    x_test_stock = torch.tensor(x_test_stock,dtype=torch.float32)\n",
    "    \n",
    "    ### Model Declare\n",
    "    model_total = TransformerModel(input_dim=x_train_total.shape[2],\n",
    "                               max_len=240,\n",
    "                               d_model=HIDDEN_SIZE,\n",
    "                               output_size=OUTPUT_SIZE).to(device)\n",
    "    model_stock = TransformerModel(input_dim=x_train_stock.shape[2],\n",
    "                               max_len=240,\n",
    "                               d_model=HIDDEN_SIZE,\n",
    "                               output_size=OUTPUT_SIZE).to(device)\n",
    "    \n",
    "    # Model Compile\n",
    "    criterion_total = RMSELoss()\n",
    "    # criterion_total = nn.MSELoss()\n",
    "    optimizer_total = torch.optim.Adam(model_total.parameters(), lr=0.001)\n",
    "    scheduler_total = ReduceLROnPlateau(optimizer_total, 'min', factor=0.3, patience=10, verbose=False)\n",
    "\n",
    "    criterion_stock = RMSELoss()\n",
    "    # criterion_stock = nn.MSELoss()\n",
    "    optimizer_stock = torch.optim.Adam(model_stock.parameters(), lr=0.001)\n",
    "    scheduler_stock = ReduceLROnPlateau(optimizer_stock, 'min', factor=0.3, patience=10, verbose=False)\n",
    "    \n",
    "    ### Total dataset Training\n",
    "    epochs = 1000\n",
    "    epoch_counter = 0\n",
    "    min_val_loss = float('inf')\n",
    "    done = False\n",
    "    patience = 50\n",
    "    es = EarlyStopping(patience=patience)\n",
    "    tr_losses_fp, val_losses_fp = [],[]\n",
    "\n",
    "    while not done and epoch_counter<epochs:\n",
    "        epoch_counter+=1\n",
    "\n",
    "        ### Training\n",
    "        model_total.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader_total:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer_stock.zero_grad()\n",
    "            outputs = model_total(x_batch)\n",
    "            loss = criterion_total(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer_total.step()\n",
    "            train_losses.append(loss.item())\n",
    "        train_loss = np.mean(train_losses)\n",
    "        tr_losses_fp.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model_total.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_total:\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model_total(x_batch)\n",
    "                loss = criterion_total(outputs, y_batch)\n",
    "                val_losses.append(loss.item())\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_losses_fp.append(val_loss)\n",
    "        scheduler_total.step(val_loss)\n",
    "\n",
    "        if es(model_total, val_loss):\n",
    "            done = True\n",
    "\n",
    "        # print loss every 5 epochs\n",
    "    #     if epoch_counter%5 == 0 :\n",
    "    #         print(f\"Epoch {epoch_counter}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    last_val_losses_total.append(val_loss)\n",
    "    val_loss_total = val_loss\n",
    "    \n",
    "    ### Plot Train-Val loss\n",
    "#     path = f'../../plots/train_val_loss/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total.png'\n",
    "#     plt.plot(range(len(tr_losses_fp)),tr_losses_fp,color='blue',label='train_loss')\n",
    "#     plt.plot(range(len(val_losses_fp)),val_losses_fp,color='red',label='val_loss')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(path)\n",
    "#     plt.clf() # clear current figure\n",
    "    \n",
    "    ### evaluation\n",
    "    model_total.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_total = model_total(x_test_total.to(device))\n",
    "    pred_total = pred_total.to('cpu').detach().numpy()\n",
    "    pred_total_reshape = np.reshape(pred_total,(-1,1))\n",
    "    \n",
    "    y_test_SEQ = y_test[:OUTPUT_SIZE]\n",
    "    \n",
    "    ### Evaluate with error_ratio\n",
    "    print(f'🔸EVAL START🔸 \\n transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total')\n",
    "    error_ratio_total = error_ratio(pred_total_reshape, y_test_SEQ)\n",
    "    print('Error Ratio :',error_ratio_total)\n",
    "    ### R-squared, p-value\n",
    "    # r_squared는 1에 가까울수록 모델이 좋은 것\n",
    "    # p_value는 0.05 보다 작으면 유의미한 것\n",
    "    # 두가지를 만족하지 못한 것은 과적합된 것\n",
    "    r_squared_total = sm.OLS(y_test_SEQ,sm.add_constant(pred_total_reshape)).fit().rsquared\n",
    "    p_value_total = sm.OLS(y_test_SEQ,sm.add_constant(pred_total_reshape)).fit().f_pvalue\n",
    "    print(\"R-squared:\", r_squared_total)\n",
    "    print(\"p-value:\", p_value_total)\n",
    "    print('last validation loss:',val_loss)\n",
    "    print('🔸EVAL END🔸')\n",
    "    \n",
    "    ### Save result\n",
    "    path = f'../../data/results/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total_0219.csv'\n",
    "    result_total = pd.DataFrame(data={'true':np.reshape(y_test_SEQ,(-1)),\n",
    "                  'pred':np.reshape(pred_total_reshape,(-1)),\n",
    "                  'error_ratio':((pred_total_reshape-y_test_SEQ)/y_test_SEQ).iloc[:,0].values},\n",
    "                  index=y_test_SEQ.index)\n",
    "\n",
    "    result_total.to_csv(path)\n",
    "    \n",
    "    ### Plot Result\n",
    "    r = len(np.reshape(y_test_SEQ,(-1)))\n",
    "    plt.plot(list(range(r)),np.reshape(y_test_SEQ,(-1)), color='blue', label='true value')\n",
    "    plt.plot(list(range(r)),np.reshape(pred_total_reshape,(-1)), color='red',alpha=0.6, label='prediction')\n",
    "    plt.title(f'transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total')\n",
    "    plt.legend()\n",
    "    fig_path = f'../../plots/results/0219_transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total.png'\n",
    "    plt.savefig(fig_path,dpi=300)\n",
    "#     plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "#     actual_stock_price = stock_df[['Close','5MA','120MA']].reset_index().rename(columns={'Close':'true'})\n",
    "#     actual_stock_price[['pred','error_ratio']] = 0\n",
    "#     combined_df = pd.concat([actual_stock_price,\n",
    "#                             result_total.copy().reset_index()],\n",
    "#                             axis=0)\n",
    "#     combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "#     combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     plt.plot(combined_df['Date'], combined_df['true'],\n",
    "#              color='blue',label='Actual Stock Price')\n",
    "#     plt.plot(combined_df['Date'][:-OUTPUT_SIZE],combined_df['5MA'][:-OUTPUT_SIZE],\n",
    "#             color='purple',linestyle=':',label='5MA')\n",
    "#     plt.plot(combined_df['Date'][:-OUTPUT_SIZE],combined_df['5MA'][:-OUTPUT_SIZE],\n",
    "#             color='green',linestyle='--',label='120MA')\n",
    "#     plt.plot(combined_df['Date'][-OUTPUT_SIZE:],combined_df['pred'][-OUTPUT_SIZE:],color='red',\n",
    "#              label='Predicted Stock Price')\n",
    "#     for i,row in combined_df.iterrows():\n",
    "#         if not np.isnan(row['error_ratio']):\n",
    "#             plt.annotate(f'{row[\"error_ratio\"]*100:1f}%',(i,row['pred']),\n",
    "#                      textcoords='offset points',xytext=(0,10),ha='center')\n",
    "\n",
    "#     plt.title(f'transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total')\n",
    "#     plt.xlabel('Date')\n",
    "#     plt.ylabel('Price')\n",
    "#     plt.legend()\n",
    "\n",
    "#     fig_path = f'../../plots/results/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total.png'\n",
    "#     plt.savefig(fig_path,dpi=300)\n",
    "# #     plt.show()\n",
    "#     plt.clf()\n",
    "    \n",
    "    ### Stock-only dataset Training\n",
    "    epochs = 1000\n",
    "    epoch_counter = 0\n",
    "    min_val_loss = float('inf')\n",
    "    done = False\n",
    "    patience = 50\n",
    "    es = EarlyStopping(patience=patience)\n",
    "    tr_losses_fp, val_losses_fp = [],[]\n",
    "\n",
    "    while not done and epoch_counter<epochs:\n",
    "        epoch_counter+=1\n",
    "\n",
    "        ### Training\n",
    "        model_stock.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader_stock:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer_stock.zero_grad()\n",
    "            outputs = model_stock(x_batch)\n",
    "            loss = criterion_stock(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer_stock.step()\n",
    "            train_losses.append(loss.item())\n",
    "        train_loss = np.mean(train_losses)\n",
    "        tr_losses_fp.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model_stock.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_stock:\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model_stock(x_batch)\n",
    "                loss = criterion_stock(outputs, y_batch)\n",
    "                val_losses.append(loss.item())\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_losses_fp.append(val_loss)\n",
    "        scheduler_stock.step(val_loss)\n",
    "\n",
    "        if es(model_stock, val_loss):\n",
    "            done = True\n",
    "\n",
    "        # print loss every 5 epochs\n",
    "#         if epoch_counter%5 == 0 :\n",
    "#             print(f\"Epoch {epoch_counter}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    last_val_losses_stock.append(val_loss)\n",
    "    val_loss_stock = val_loss\n",
    "\n",
    "    ### Plot Train-Val loss\n",
    "#     path = f'../../plots/train_val_loss/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock.png'\n",
    "#     plt.plot(range(len(tr_losses_fp)),tr_losses_fp,color='blue',label='train_loss')\n",
    "#     plt.plot(range(len(val_losses_fp)),val_losses_fp,color='red',label='val_loss')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(path)\n",
    "#     plt.clf() # clear current figure\n",
    "    \n",
    "    # evaluation\n",
    "    model_stock.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_stock = model_stock(x_test_stock.to(device))\n",
    "    pred_stock = pred_stock.to('cpu').detach().numpy()\n",
    "    pred_stock_reshape = np.reshape(pred_stock,(-1,1))\n",
    "    \n",
    "    ### Evaluate with error_ratio\n",
    "    print(f'🔸EVAL START🔸 \\n transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock')\n",
    "    error_ratio_stock = error_ratio(pred_stock_reshape, y_test_SEQ)\n",
    "    print('Error Ratio :',error_ratio_stock)\n",
    "    ### R-squared, p-value\n",
    "    # r_squared는 1에 가까울수록 모델이 좋은 것\n",
    "    # p_value는 0.05 보다 작으면 유의미한 것\n",
    "    # 두가지를 만족하지 못한 것은 과적합된 것\n",
    "    r_squared_stock = sm.OLS(y_test_SEQ,sm.add_constant(pred_stock_reshape)).fit().rsquared\n",
    "    p_value_stock = sm.OLS(y_test_SEQ,sm.add_constant(pred_stock_reshape)).fit().f_pvalue\n",
    "    print(\"R-squared:\", r_squared_stock)\n",
    "    print(\"p-value:\", p_value_stock)\n",
    "    print('last validation loss:',val_loss)\n",
    "    print('🔸EVAL END🔸')\n",
    "    \n",
    "    ### Save result\n",
    "    path = f'../../data/results/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock_0219.csv'\n",
    "\n",
    "    result_stock = pd.DataFrame(data={'true':np.reshape(y_test_SEQ,(-1)),\n",
    "                  'pred':np.reshape(pred_stock_reshape,(-1)),\n",
    "                  'error_ratio':((pred_stock_reshape-y_test_SEQ)/y_test_SEQ).iloc[:,0].values},\n",
    "                  index=y_test_SEQ.index)\n",
    "\n",
    "    result_stock.to_csv(path)\n",
    "\n",
    "    ### Plot Result\n",
    "    r = len(np.reshape(y_test_SEQ,(-1)))\n",
    "    plt.plot(list(range(r)),np.reshape(y_test_SEQ,(-1)), color='blue', label='true value')\n",
    "    plt.plot(list(range(r)),np.reshape(pred_stock_reshape,(-1)), color='red',alpha=0.6, label='prediction')\n",
    "    plt.title(f'transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock')\n",
    "    plt.legend()\n",
    "    fig_path = f'../../plots/results/0219_transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock.png'\n",
    "    plt.savefig(fig_path,dpi=300)\n",
    "#     plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "#     actual_stock_price = stock_df[['Close','5MA','120MA']].reset_index().rename(columns={'Close':'true'})\n",
    "#     actual_stock_price[['pred','error_ratio']] = 0\n",
    "#     combined_df = pd.concat([actual_stock_price,\n",
    "#                             result_stock.copy().reset_index()],\n",
    "#                             axis=0)\n",
    "#     combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "#     combined_df = combined_df.reset_index(drop=True)\n",
    "#     combined_df\n",
    "\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     plt.plot(combined_df['Date'], combined_df['true'],\n",
    "#              color='blue',label='Actual Stock Price')\n",
    "#     plt.plot(combined_df['Date'][:-OUTPUT_SIZE],combined_df['5MA'][:-OUTPUT_SIZE],\n",
    "#             color='purple',linestyle=':',label='5MA')\n",
    "#     plt.plot(combined_df['Date'][:-OUTPUT_SIZE],combined_df['5MA'][:-OUTPUT_SIZE],\n",
    "#             color='green',linestyle='--',label='120MA')\n",
    "#     plt.plot(combined_df['Date'][-OUTPUT_SIZE:],combined_df['pred'][-OUTPUT_SIZE:],color='red',\n",
    "#              label='Predicted Stock Price')\n",
    "#     for i,row in combined_df.iterrows():\n",
    "#         if not np.isnan(row['error_ratio']):\n",
    "#             plt.annotate(f'{row[\"error_ratio\"]*100:1f}%',(i,row['pred']),\n",
    "#                      textcoords='offset points',xytext=(0,10),ha='center')\n",
    "\n",
    "#     plt.title(f'transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock')\n",
    "#     plt.xlabel('Date')\n",
    "#     plt.ylabel('Price')\n",
    "#     plt.legend()\n",
    "\n",
    "#     fig_path = f'../../plots/results/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock.png'\n",
    "#     plt.savefig(fig_path,dpi=300)\n",
    "# #     plt.show()\n",
    "#     plt.clf()\n",
    "    \n",
    "    ### add results to final dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        'seq_size':[SEQUENCE_SIZE],\n",
    "        'batch_size':[BATCH_SIZE],\n",
    "        'output_size':[OUTPUT_SIZE],\n",
    "        'hidden_size':[HIDDEN_SIZE],\n",
    "        'error_ratio_total':[error_ratio_total],\n",
    "        'r_squared_total':[r_squared_total],\n",
    "        'p_value_total':[p_value_total],\n",
    "        'error_ratio_stock':[error_ratio_stock],\n",
    "        'r_squared_stock':[r_squared_stock],\n",
    "        'p_value_stock':[p_value_stock]\n",
    "    })\n",
    "    final = pd.concat([final,results_df],axis=0)\n",
    "    \n",
    "    val_loss_df_temp = pd.DataFrame({\n",
    "        'seq_size':[SEQUENCE_SIZE],\n",
    "        'batch_size':[BATCH_SIZE],\n",
    "        'output_size':[OUTPUT_SIZE],\n",
    "        'hidden_size':[HIDDEN_SIZE],\n",
    "        'val_loss_total':[val_loss_total],\n",
    "        'val_loss_stock':[val_loss_stock]\n",
    "    })\n",
    "    val_loss_df = pd.concat([val_loss_df,val_loss_df_temp],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf1bad02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T12:09:04.629616Z",
     "start_time": "2024-02-18T12:09:04.600448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 10)\n"
     ]
    }
   ],
   "source": [
    "### save final \n",
    "# remove first row(dummy row)\n",
    "final = final.iloc[1:].reset_index(drop=True)\n",
    "print(final.shape)\n",
    "final.to_csv('../../data/results/final_all_results_0219.csv')\n",
    "val_loss_df = val_loss_df.iloc[1:].reset_index(drop=True)\n",
    "print(val_loss_df.shape)\n",
    "val_loss_df.to_csv('../../data/results/val_loss_results_0219.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "605b0cfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T04:21:55.802613Z",
     "start_time": "2024-02-19T04:21:55.773056Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('../../data/results/final_all_results.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e4fb75e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T04:21:58.800623Z",
     "start_time": "2024-02-19T04:21:58.768332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>error_ratio_total</th>\n",
       "      <th>r_squared_total</th>\n",
       "      <th>p_value_total</th>\n",
       "      <th>error_ratio_stock</th>\n",
       "      <th>r_squared_stock</th>\n",
       "      <th>p_value_stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.203406</td>\n",
       "      <td>2.420091e-02</td>\n",
       "      <td>0.667815</td>\n",
       "      <td>0.045170</td>\n",
       "      <td>0.407733</td>\n",
       "      <td>4.691567e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.157909</td>\n",
       "      <td>3.318244e-03</td>\n",
       "      <td>0.874408</td>\n",
       "      <td>0.021650</td>\n",
       "      <td>0.008583</td>\n",
       "      <td>7.990683e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.031342</td>\n",
       "      <td>3.007018e-02</td>\n",
       "      <td>0.631873</td>\n",
       "      <td>0.089128</td>\n",
       "      <td>0.183508</td>\n",
       "      <td>2.167777e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.121427</td>\n",
       "      <td>1.325274e-01</td>\n",
       "      <td>0.114582</td>\n",
       "      <td>0.052569</td>\n",
       "      <td>0.201426</td>\n",
       "      <td>4.715242e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.100733</td>\n",
       "      <td>6.809472e-01</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.056673</td>\n",
       "      <td>0.068630</td>\n",
       "      <td>2.645186e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.177341</td>\n",
       "      <td>1.919437e-01</td>\n",
       "      <td>0.053345</td>\n",
       "      <td>0.064862</td>\n",
       "      <td>0.808851</td>\n",
       "      <td>6.945284e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.184827</td>\n",
       "      <td>1.806441e-02</td>\n",
       "      <td>0.711245</td>\n",
       "      <td>0.033213</td>\n",
       "      <td>0.089823</td>\n",
       "      <td>4.001782e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.190569</td>\n",
       "      <td>1.371833e-03</td>\n",
       "      <td>0.919090</td>\n",
       "      <td>0.044442</td>\n",
       "      <td>0.028878</td>\n",
       "      <td>6.388165e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.188393</td>\n",
       "      <td>7.151231e-02</td>\n",
       "      <td>0.455092</td>\n",
       "      <td>0.106056</td>\n",
       "      <td>0.158785</td>\n",
       "      <td>2.540497e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.219501</td>\n",
       "      <td>2.530574e-02</td>\n",
       "      <td>0.502924</td>\n",
       "      <td>0.067613</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>9.471068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.220868</td>\n",
       "      <td>1.242497e-02</td>\n",
       "      <td>0.639883</td>\n",
       "      <td>0.069508</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>6.371259e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.221124</td>\n",
       "      <td>4.750927e-04</td>\n",
       "      <td>0.927325</td>\n",
       "      <td>0.185437</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>9.371721e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.147349</td>\n",
       "      <td>5.848509e-04</td>\n",
       "      <td>0.947129</td>\n",
       "      <td>0.060621</td>\n",
       "      <td>0.637509</td>\n",
       "      <td>5.616731e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.161310</td>\n",
       "      <td>3.684410e-01</td>\n",
       "      <td>0.062756</td>\n",
       "      <td>0.075874</td>\n",
       "      <td>0.366626</td>\n",
       "      <td>6.358459e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.158714</td>\n",
       "      <td>3.584422e-04</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>0.455458</td>\n",
       "      <td>0.420992</td>\n",
       "      <td>4.238951e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.183872</td>\n",
       "      <td>3.513952e-03</td>\n",
       "      <td>0.803941</td>\n",
       "      <td>0.186354</td>\n",
       "      <td>0.019549</td>\n",
       "      <td>5.565819e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.203552</td>\n",
       "      <td>3.297400e-03</td>\n",
       "      <td>0.809969</td>\n",
       "      <td>0.046145</td>\n",
       "      <td>0.654375</td>\n",
       "      <td>1.573812e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.216428</td>\n",
       "      <td>9.103256e-02</td>\n",
       "      <td>0.196076</td>\n",
       "      <td>0.459155</td>\n",
       "      <td>0.040532</td>\n",
       "      <td>3.946878e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.179748</td>\n",
       "      <td>2.746772e-01</td>\n",
       "      <td>0.119941</td>\n",
       "      <td>0.121927</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>9.913950e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.181072</td>\n",
       "      <td>5.520180e-03</td>\n",
       "      <td>0.838368</td>\n",
       "      <td>0.185237</td>\n",
       "      <td>0.007123</td>\n",
       "      <td>8.166948e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.197624</td>\n",
       "      <td>7.118391e-05</td>\n",
       "      <td>0.981545</td>\n",
       "      <td>0.400848</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>8.848441e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.213393</td>\n",
       "      <td>9.976054e-02</td>\n",
       "      <td>0.174909</td>\n",
       "      <td>0.134701</td>\n",
       "      <td>0.048199</td>\n",
       "      <td>3.523608e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.216372</td>\n",
       "      <td>1.076748e-01</td>\n",
       "      <td>0.157816</td>\n",
       "      <td>0.203592</td>\n",
       "      <td>0.015149</td>\n",
       "      <td>6.051818e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.231375</td>\n",
       "      <td>1.934072e-02</td>\n",
       "      <td>0.558715</td>\n",
       "      <td>0.170366</td>\n",
       "      <td>0.356801</td>\n",
       "      <td>5.419102e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.182485</td>\n",
       "      <td>1.113664e-02</td>\n",
       "      <td>0.771706</td>\n",
       "      <td>0.098001</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>8.041381e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.182138</td>\n",
       "      <td>1.907142e-01</td>\n",
       "      <td>0.206989</td>\n",
       "      <td>0.189974</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>9.455249e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.173252</td>\n",
       "      <td>7.734647e-03</td>\n",
       "      <td>0.809097</td>\n",
       "      <td>0.588540</td>\n",
       "      <td>0.405743</td>\n",
       "      <td>4.762839e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.213771</td>\n",
       "      <td>1.227034e-01</td>\n",
       "      <td>0.129994</td>\n",
       "      <td>0.213679</td>\n",
       "      <td>0.010812</td>\n",
       "      <td>6.626430e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.215863</td>\n",
       "      <td>9.877017e-03</td>\n",
       "      <td>0.676772</td>\n",
       "      <td>0.206351</td>\n",
       "      <td>0.304233</td>\n",
       "      <td>1.169853e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.206894</td>\n",
       "      <td>5.094396e-02</td>\n",
       "      <td>0.338655</td>\n",
       "      <td>0.217881</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>8.747811e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.230444</td>\n",
       "      <td>1.768056e-02</td>\n",
       "      <td>0.714220</td>\n",
       "      <td>0.060605</td>\n",
       "      <td>0.410400</td>\n",
       "      <td>4.597463e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.064752</td>\n",
       "      <td>4.389342e-01</td>\n",
       "      <td>0.036843</td>\n",
       "      <td>0.091416</td>\n",
       "      <td>0.536254</td>\n",
       "      <td>1.602624e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.103033</td>\n",
       "      <td>4.364562e-03</td>\n",
       "      <td>0.856112</td>\n",
       "      <td>0.105889</td>\n",
       "      <td>0.829468</td>\n",
       "      <td>2.489425e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.076007</td>\n",
       "      <td>1.857186e-02</td>\n",
       "      <td>0.566714</td>\n",
       "      <td>0.054516</td>\n",
       "      <td>0.439437</td>\n",
       "      <td>1.445172e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.047786</td>\n",
       "      <td>7.854084e-03</td>\n",
       "      <td>0.710229</td>\n",
       "      <td>0.071678</td>\n",
       "      <td>0.154553</td>\n",
       "      <td>8.638675e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.235907</td>\n",
       "      <td>2.756090e-01</td>\n",
       "      <td>0.017465</td>\n",
       "      <td>0.047670</td>\n",
       "      <td>0.456811</td>\n",
       "      <td>1.071365e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.181805</td>\n",
       "      <td>5.201293e-02</td>\n",
       "      <td>0.526260</td>\n",
       "      <td>0.024555</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>9.473686e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.188154</td>\n",
       "      <td>2.568953e-01</td>\n",
       "      <td>0.134880</td>\n",
       "      <td>0.051021</td>\n",
       "      <td>0.570125</td>\n",
       "      <td>1.157377e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.170882</td>\n",
       "      <td>6.340929e-02</td>\n",
       "      <td>0.482781</td>\n",
       "      <td>0.186712</td>\n",
       "      <td>0.447863</td>\n",
       "      <td>3.431405e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.214171</td>\n",
       "      <td>2.939819e-02</td>\n",
       "      <td>0.469806</td>\n",
       "      <td>0.058089</td>\n",
       "      <td>0.054188</td>\n",
       "      <td>3.233185e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.237079</td>\n",
       "      <td>8.647611e-02</td>\n",
       "      <td>0.208216</td>\n",
       "      <td>0.074794</td>\n",
       "      <td>0.168743</td>\n",
       "      <td>7.199127e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.252056</td>\n",
       "      <td>1.723913e-01</td>\n",
       "      <td>0.068688</td>\n",
       "      <td>0.167989</td>\n",
       "      <td>0.075696</td>\n",
       "      <td>2.403863e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>4.410478e-02</td>\n",
       "      <td>0.560331</td>\n",
       "      <td>0.163658</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>1.800722e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.142528</td>\n",
       "      <td>7.859865e-02</td>\n",
       "      <td>0.432696</td>\n",
       "      <td>0.016541</td>\n",
       "      <td>0.454005</td>\n",
       "      <td>3.265802e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.152822</td>\n",
       "      <td>1.343834e-01</td>\n",
       "      <td>0.297450</td>\n",
       "      <td>0.409726</td>\n",
       "      <td>0.034958</td>\n",
       "      <td>6.050023e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.179645</td>\n",
       "      <td>1.272022e-03</td>\n",
       "      <td>0.881335</td>\n",
       "      <td>0.109976</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>9.677280e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.190918</td>\n",
       "      <td>1.584668e-01</td>\n",
       "      <td>0.082155</td>\n",
       "      <td>0.126813</td>\n",
       "      <td>0.035996</td>\n",
       "      <td>4.230351e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.191895</td>\n",
       "      <td>4.657999e-02</td>\n",
       "      <td>0.360779</td>\n",
       "      <td>0.109110</td>\n",
       "      <td>0.439334</td>\n",
       "      <td>1.447696e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.170935</td>\n",
       "      <td>1.437127e-07</td>\n",
       "      <td>0.999171</td>\n",
       "      <td>0.019751</td>\n",
       "      <td>0.025045</td>\n",
       "      <td>6.623527e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.160479</td>\n",
       "      <td>2.950364e-03</td>\n",
       "      <td>0.881531</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.025551</td>\n",
       "      <td>6.591343e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.177961</td>\n",
       "      <td>6.073038e-02</td>\n",
       "      <td>0.492485</td>\n",
       "      <td>0.613562</td>\n",
       "      <td>0.424134</td>\n",
       "      <td>4.137157e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.203103</td>\n",
       "      <td>2.434456e-05</td>\n",
       "      <td>0.983529</td>\n",
       "      <td>0.214369</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>5.293321e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.196899</td>\n",
       "      <td>2.097707e-05</td>\n",
       "      <td>0.984710</td>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.148468</td>\n",
       "      <td>9.339940e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.214171</td>\n",
       "      <td>1.151529e-01</td>\n",
       "      <td>0.143273</td>\n",
       "      <td>0.250096</td>\n",
       "      <td>0.017583</td>\n",
       "      <td>5.773239e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>60</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.167071</td>\n",
       "      <td>4.823062e-01</td>\n",
       "      <td>0.025844</td>\n",
       "      <td>0.092289</td>\n",
       "      <td>0.063986</td>\n",
       "      <td>4.807293e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>60</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.161820</td>\n",
       "      <td>5.305120e-05</td>\n",
       "      <td>0.984068</td>\n",
       "      <td>0.084953</td>\n",
       "      <td>0.155187</td>\n",
       "      <td>2.600002e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>60</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.170181</td>\n",
       "      <td>2.886680e-01</td>\n",
       "      <td>0.109251</td>\n",
       "      <td>0.225900</td>\n",
       "      <td>0.059578</td>\n",
       "      <td>4.967530e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>60</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.202678</td>\n",
       "      <td>2.768040e-05</td>\n",
       "      <td>0.982437</td>\n",
       "      <td>0.264698</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>9.279965e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>60</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.216633</td>\n",
       "      <td>1.573670e-02</td>\n",
       "      <td>0.598209</td>\n",
       "      <td>0.367106</td>\n",
       "      <td>0.109293</td>\n",
       "      <td>1.545419e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.206558</td>\n",
       "      <td>1.688297e-01</td>\n",
       "      <td>0.071911</td>\n",
       "      <td>0.229094</td>\n",
       "      <td>0.052880</td>\n",
       "      <td>3.293969e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.025241</td>\n",
       "      <td>2.601587e-02</td>\n",
       "      <td>0.656205</td>\n",
       "      <td>0.116848</td>\n",
       "      <td>0.583227</td>\n",
       "      <td>1.014170e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.063330</td>\n",
       "      <td>1.075959e-01</td>\n",
       "      <td>0.354808</td>\n",
       "      <td>0.171576</td>\n",
       "      <td>0.103142</td>\n",
       "      <td>3.655558e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.310399</td>\n",
       "      <td>1.220599e-01</td>\n",
       "      <td>0.322402</td>\n",
       "      <td>0.066362</td>\n",
       "      <td>0.116244</td>\n",
       "      <td>3.349992e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.224202</td>\n",
       "      <td>3.953568e-01</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.102924</td>\n",
       "      <td>0.448171</td>\n",
       "      <td>1.244616e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.176095</td>\n",
       "      <td>3.516499e-01</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>0.041021</td>\n",
       "      <td>0.560828</td>\n",
       "      <td>1.451646e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.335817</td>\n",
       "      <td>5.713624e-02</td>\n",
       "      <td>0.310124</td>\n",
       "      <td>0.063011</td>\n",
       "      <td>0.541266</td>\n",
       "      <td>2.181254e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.164596</td>\n",
       "      <td>4.867315e-02</td>\n",
       "      <td>0.540206</td>\n",
       "      <td>0.060620</td>\n",
       "      <td>0.574776</td>\n",
       "      <td>1.104834e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.166846</td>\n",
       "      <td>3.319776e-02</td>\n",
       "      <td>0.614402</td>\n",
       "      <td>0.087315</td>\n",
       "      <td>0.206428</td>\n",
       "      <td>1.871261e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.163891</td>\n",
       "      <td>4.181387e-01</td>\n",
       "      <td>0.043332</td>\n",
       "      <td>0.089148</td>\n",
       "      <td>0.645786</td>\n",
       "      <td>5.096535e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.208523</td>\n",
       "      <td>1.467530e-02</td>\n",
       "      <td>0.610921</td>\n",
       "      <td>0.048408</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>3.612970e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.203072</td>\n",
       "      <td>1.525304e-01</td>\n",
       "      <td>0.088658</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.404990</td>\n",
       "      <td>2.555453e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.210886</td>\n",
       "      <td>4.830537e-02</td>\n",
       "      <td>0.351815</td>\n",
       "      <td>0.244776</td>\n",
       "      <td>0.259684</td>\n",
       "      <td>2.172383e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.143751</td>\n",
       "      <td>2.795160e-01</td>\n",
       "      <td>0.116143</td>\n",
       "      <td>0.064188</td>\n",
       "      <td>0.240149</td>\n",
       "      <td>1.504790e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.141333</td>\n",
       "      <td>4.905010e-02</td>\n",
       "      <td>0.538601</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>0.347854</td>\n",
       "      <td>7.271118e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.144016</td>\n",
       "      <td>1.795416e-01</td>\n",
       "      <td>0.222360</td>\n",
       "      <td>0.014454</td>\n",
       "      <td>0.236817</td>\n",
       "      <td>1.537740e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.225857</td>\n",
       "      <td>5.318177e-02</td>\n",
       "      <td>0.327980</td>\n",
       "      <td>0.093846</td>\n",
       "      <td>0.084357</td>\n",
       "      <td>2.141417e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.209885</td>\n",
       "      <td>1.693494e-03</td>\n",
       "      <td>0.863233</td>\n",
       "      <td>0.151652</td>\n",
       "      <td>0.196707</td>\n",
       "      <td>5.014269e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>120</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.219106</td>\n",
       "      <td>1.829640e-02</td>\n",
       "      <td>0.569632</td>\n",
       "      <td>0.084774</td>\n",
       "      <td>0.635071</td>\n",
       "      <td>2.600009e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>120</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.144555</td>\n",
       "      <td>1.866575e-01</td>\n",
       "      <td>0.212444</td>\n",
       "      <td>0.083869</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>9.898550e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>120</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.159972</td>\n",
       "      <td>5.046149e-02</td>\n",
       "      <td>0.532663</td>\n",
       "      <td>0.070194</td>\n",
       "      <td>0.379491</td>\n",
       "      <td>5.790640e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>120</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.176499</td>\n",
       "      <td>2.763857e-03</td>\n",
       "      <td>0.885315</td>\n",
       "      <td>0.350924</td>\n",
       "      <td>0.077353</td>\n",
       "      <td>4.365218e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>120</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.184359</td>\n",
       "      <td>4.652906e-02</td>\n",
       "      <td>0.361048</td>\n",
       "      <td>0.313957</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>7.620046e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>120</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.189125</td>\n",
       "      <td>5.880712e-02</td>\n",
       "      <td>0.302939</td>\n",
       "      <td>0.392692</td>\n",
       "      <td>0.104117</td>\n",
       "      <td>1.652702e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>120</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.206060</td>\n",
       "      <td>9.138287e-02</td>\n",
       "      <td>0.195175</td>\n",
       "      <td>0.420822</td>\n",
       "      <td>0.077874</td>\n",
       "      <td>2.334644e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.167234</td>\n",
       "      <td>1.069352e-01</td>\n",
       "      <td>0.356378</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>0.037287</td>\n",
       "      <td>5.929988e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.160905</td>\n",
       "      <td>1.397608e-02</td>\n",
       "      <td>0.744977</td>\n",
       "      <td>0.412412</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>8.140282e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.170970</td>\n",
       "      <td>1.277185e-02</td>\n",
       "      <td>0.755918</td>\n",
       "      <td>0.044363</td>\n",
       "      <td>0.078301</td>\n",
       "      <td>4.336048e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.203995</td>\n",
       "      <td>1.844267e-03</td>\n",
       "      <td>0.857332</td>\n",
       "      <td>0.251742</td>\n",
       "      <td>0.013327</td>\n",
       "      <td>6.279156e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.203286</td>\n",
       "      <td>2.113188e-01</td>\n",
       "      <td>0.041428</td>\n",
       "      <td>0.184006</td>\n",
       "      <td>0.055390</td>\n",
       "      <td>3.178584e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.205680</td>\n",
       "      <td>1.960500e-01</td>\n",
       "      <td>0.050573</td>\n",
       "      <td>0.147999</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>6.197718e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seq_size  batch_size  output_size  hidden_size  error_ratio_total  \\\n",
       "0         30           1           10           64           0.203406   \n",
       "1         30           1           10          128           0.157909   \n",
       "2         30           1           10          256           0.031342   \n",
       "3         30           1           20           64           0.121427   \n",
       "4         30           1           20          128           0.100733   \n",
       "5         30           1           20          256           0.177341   \n",
       "6         30           5           10           64           0.184827   \n",
       "7         30           5           10          128           0.190569   \n",
       "8         30           5           10          256           0.188393   \n",
       "9         30           5           20           64           0.219501   \n",
       "10        30           5           20          128           0.220868   \n",
       "11        30           5           20          256           0.221124   \n",
       "12        30          32           10           64           0.147349   \n",
       "13        30          32           10          128           0.161310   \n",
       "14        30          32           10          256           0.158714   \n",
       "15        30          32           20           64           0.183872   \n",
       "16        30          32           20          128           0.203552   \n",
       "17        30          32           20          256           0.216428   \n",
       "18        30          64           10           64           0.179748   \n",
       "19        30          64           10          128           0.181072   \n",
       "20        30          64           10          256           0.197624   \n",
       "21        30          64           20           64           0.213393   \n",
       "22        30          64           20          128           0.216372   \n",
       "23        30          64           20          256           0.231375   \n",
       "24        30         128           10           64           0.182485   \n",
       "25        30         128           10          128           0.182138   \n",
       "26        30         128           10          256           0.173252   \n",
       "27        30         128           20           64           0.213771   \n",
       "28        30         128           20          128           0.215863   \n",
       "29        30         128           20          256           0.206894   \n",
       "30        60           1           10           64           0.230444   \n",
       "31        60           1           10          128           0.064752   \n",
       "32        60           1           10          256           0.103033   \n",
       "33        60           1           20           64           0.076007   \n",
       "34        60           1           20          128           0.047786   \n",
       "35        60           1           20          256           0.235907   \n",
       "36        60           5           10           64           0.181805   \n",
       "37        60           5           10          128           0.188154   \n",
       "38        60           5           10          256           0.170882   \n",
       "39        60           5           20           64           0.214171   \n",
       "40        60           5           20          128           0.237079   \n",
       "41        60           5           20          256           0.252056   \n",
       "42        60          32           10           64           0.143564   \n",
       "43        60          32           10          128           0.142528   \n",
       "44        60          32           10          256           0.152822   \n",
       "45        60          32           20           64           0.179645   \n",
       "46        60          32           20          128           0.190918   \n",
       "47        60          32           20          256           0.191895   \n",
       "48        60          64           10           64           0.170935   \n",
       "49        60          64           10          128           0.160479   \n",
       "50        60          64           10          256           0.177961   \n",
       "51        60          64           20           64           0.203103   \n",
       "52        60          64           20          128           0.196899   \n",
       "53        60          64           20          256           0.214171   \n",
       "54        60         128           10           64           0.167071   \n",
       "55        60         128           10          128           0.161820   \n",
       "56        60         128           10          256           0.170181   \n",
       "57        60         128           20           64           0.202678   \n",
       "58        60         128           20          128           0.216633   \n",
       "59        60         128           20          256           0.206558   \n",
       "60       120           1           10           64           0.025241   \n",
       "61       120           1           10          128           0.063330   \n",
       "62       120           1           10          256           0.310399   \n",
       "63       120           1           20           64           0.224202   \n",
       "64       120           1           20          128           0.176095   \n",
       "65       120           1           20          256           0.335817   \n",
       "66       120           5           10           64           0.164596   \n",
       "67       120           5           10          128           0.166846   \n",
       "68       120           5           10          256           0.163891   \n",
       "69       120           5           20           64           0.208523   \n",
       "70       120           5           20          128           0.203072   \n",
       "71       120           5           20          256           0.210886   \n",
       "72       120          32           10           64           0.143751   \n",
       "73       120          32           10          128           0.141333   \n",
       "74       120          32           10          256           0.144016   \n",
       "75       120          32           20           64           0.225857   \n",
       "76       120          32           20          128           0.209885   \n",
       "77       120          32           20          256           0.219106   \n",
       "78       120          64           10           64           0.144555   \n",
       "79       120          64           10          128           0.159972   \n",
       "80       120          64           10          256           0.176499   \n",
       "81       120          64           20           64           0.184359   \n",
       "82       120          64           20          128           0.189125   \n",
       "83       120          64           20          256           0.206060   \n",
       "84       120         128           10           64           0.167234   \n",
       "85       120         128           10          128           0.160905   \n",
       "86       120         128           10          256           0.170970   \n",
       "87       120         128           20           64           0.203995   \n",
       "88       120         128           20          128           0.203286   \n",
       "89       120         128           20          256           0.205680   \n",
       "\n",
       "    r_squared_total  p_value_total  error_ratio_stock  r_squared_stock  \\\n",
       "0      2.420091e-02       0.667815           0.045170         0.407733   \n",
       "1      3.318244e-03       0.874408           0.021650         0.008583   \n",
       "2      3.007018e-02       0.631873           0.089128         0.183508   \n",
       "3      1.325274e-01       0.114582           0.052569         0.201426   \n",
       "4      6.809472e-01       0.000008           0.056673         0.068630   \n",
       "5      1.919437e-01       0.053345           0.064862         0.808851   \n",
       "6      1.806441e-02       0.711245           0.033213         0.089823   \n",
       "7      1.371833e-03       0.919090           0.044442         0.028878   \n",
       "8      7.151231e-02       0.455092           0.106056         0.158785   \n",
       "9      2.530574e-02       0.502924           0.067613         0.000251   \n",
       "10     1.242497e-02       0.639883           0.069508         0.012629   \n",
       "11     4.750927e-04       0.927325           0.185437         0.000355   \n",
       "12     5.848509e-04       0.947129           0.060621         0.637509   \n",
       "13     3.684410e-01       0.062756           0.075874         0.366626   \n",
       "14     3.584422e-04       0.958600           0.455458         0.420992   \n",
       "15     3.513952e-03       0.803941           0.186354         0.019549   \n",
       "16     3.297400e-03       0.809969           0.046145         0.654375   \n",
       "17     9.103256e-02       0.196076           0.459155         0.040532   \n",
       "18     2.746772e-01       0.119941           0.121927         0.000015   \n",
       "19     5.520180e-03       0.838368           0.185237         0.007123   \n",
       "20     7.118391e-05       0.981545           0.400848         0.002787   \n",
       "21     9.976054e-02       0.174909           0.134701         0.048199   \n",
       "22     1.076748e-01       0.157816           0.203592         0.015149   \n",
       "23     1.934072e-02       0.558715           0.170366         0.356801   \n",
       "24     1.113664e-02       0.771706           0.098001         0.008148   \n",
       "25     1.907142e-01       0.206989           0.189974         0.000621   \n",
       "26     7.734647e-03       0.809097           0.588540         0.405743   \n",
       "27     1.227034e-01       0.129994           0.213679         0.010812   \n",
       "28     9.877017e-03       0.676772           0.206351         0.304233   \n",
       "29     5.094396e-02       0.338655           0.217881         0.001418   \n",
       "30     1.768056e-02       0.714220           0.060605         0.410400   \n",
       "31     4.389342e-01       0.036843           0.091416         0.536254   \n",
       "32     4.364562e-03       0.856112           0.105889         0.829468   \n",
       "33     1.857186e-02       0.566714           0.054516         0.439437   \n",
       "34     7.854084e-03       0.710229           0.071678         0.154553   \n",
       "35     2.756090e-01       0.017465           0.047670         0.456811   \n",
       "36     5.201293e-02       0.526260           0.024555         0.000580   \n",
       "37     2.568953e-01       0.134880           0.051021         0.570125   \n",
       "38     6.340929e-02       0.482781           0.186712         0.447863   \n",
       "39     2.939819e-02       0.469806           0.058089         0.054188   \n",
       "40     8.647611e-02       0.208216           0.074794         0.168743   \n",
       "41     1.723913e-01       0.068688           0.167989         0.075696   \n",
       "42     4.410478e-02       0.560331           0.163658         0.212400   \n",
       "43     7.859865e-02       0.432696           0.016541         0.454005   \n",
       "44     1.343834e-01       0.297450           0.409726         0.034958   \n",
       "45     1.272022e-03       0.881335           0.109976         0.000093   \n",
       "46     1.584668e-01       0.082155           0.126813         0.035996   \n",
       "47     4.657999e-02       0.360779           0.109110         0.439334   \n",
       "48     1.437127e-07       0.999171           0.019751         0.025045   \n",
       "49     2.950364e-03       0.881531           0.146226         0.025551   \n",
       "50     6.073038e-02       0.492485           0.613562         0.424134   \n",
       "51     2.434456e-05       0.983529           0.214369         0.022346   \n",
       "52     2.097707e-05       0.984710           0.564642         0.148468   \n",
       "53     1.151529e-01       0.143273           0.250096         0.017583   \n",
       "54     4.823062e-01       0.025844           0.092289         0.063986   \n",
       "55     5.305120e-05       0.984068           0.084953         0.155187   \n",
       "56     2.886680e-01       0.109251           0.225900         0.059578   \n",
       "57     2.768040e-05       0.982437           0.264698         0.000466   \n",
       "58     1.573670e-02       0.598209           0.367106         0.109293   \n",
       "59     1.688297e-01       0.071911           0.229094         0.052880   \n",
       "60     2.601587e-02       0.656205           0.116848         0.583227   \n",
       "61     1.075959e-01       0.354808           0.171576         0.103142   \n",
       "62     1.220599e-01       0.322402           0.066362         0.116244   \n",
       "63     3.953568e-01       0.002982           0.102924         0.448171   \n",
       "64     3.516499e-01       0.005856           0.041021         0.560828   \n",
       "65     5.713624e-02       0.310124           0.063011         0.541266   \n",
       "66     4.867315e-02       0.540206           0.060620         0.574776   \n",
       "67     3.319776e-02       0.614402           0.087315         0.206428   \n",
       "68     4.181387e-01       0.043332           0.089148         0.645786   \n",
       "69     1.467530e-02       0.610921           0.048408         0.046482   \n",
       "70     1.525304e-01       0.088658           0.098184         0.404990   \n",
       "71     4.830537e-02       0.351815           0.244776         0.259684   \n",
       "72     2.795160e-01       0.116143           0.064188         0.240149   \n",
       "73     4.905010e-02       0.538601           0.016097         0.347854   \n",
       "74     1.795416e-01       0.222360           0.014454         0.236817   \n",
       "75     5.318177e-02       0.327980           0.093846         0.084357   \n",
       "76     1.693494e-03       0.863233           0.151652         0.196707   \n",
       "77     1.829640e-02       0.569632           0.084774         0.635071   \n",
       "78     1.866575e-01       0.212444           0.083869         0.000022   \n",
       "79     5.046149e-02       0.532663           0.070194         0.379491   \n",
       "80     2.763857e-03       0.885315           0.350924         0.077353   \n",
       "81     4.652906e-02       0.361048           0.313957         0.005225   \n",
       "82     5.880712e-02       0.302939           0.392692         0.104117   \n",
       "83     9.138287e-02       0.195175           0.420822         0.077874   \n",
       "84     1.069352e-01       0.356378           0.056165         0.037287   \n",
       "85     1.397608e-02       0.744977           0.412412         0.007334   \n",
       "86     1.277185e-02       0.755918           0.044363         0.078301   \n",
       "87     1.844267e-03       0.857332           0.251742         0.013327   \n",
       "88     2.113188e-01       0.041428           0.184006         0.055390   \n",
       "89     1.960500e-01       0.050573           0.147999         0.013963   \n",
       "\n",
       "    p_value_stock  \n",
       "0    4.691567e-02  \n",
       "1    7.990683e-01  \n",
       "2    2.167777e-01  \n",
       "3    4.715242e-02  \n",
       "4    2.645186e-01  \n",
       "5    6.945284e-08  \n",
       "6    4.001782e-01  \n",
       "7    6.388165e-01  \n",
       "8    2.540497e-01  \n",
       "9    9.471068e-01  \n",
       "10   6.371259e-01  \n",
       "11   9.371721e-01  \n",
       "12   5.616731e-03  \n",
       "13   6.358459e-02  \n",
       "14   4.238951e-02  \n",
       "15   5.565819e-01  \n",
       "16   1.573812e-05  \n",
       "17   3.946878e-01  \n",
       "18   9.913950e-01  \n",
       "19   8.166948e-01  \n",
       "20   8.848441e-01  \n",
       "21   3.523608e-01  \n",
       "22   6.051818e-01  \n",
       "23   5.419102e-03  \n",
       "24   8.041381e-01  \n",
       "25   9.455249e-01  \n",
       "26   4.762839e-02  \n",
       "27   6.626430e-01  \n",
       "28   1.169853e-02  \n",
       "29   8.747811e-01  \n",
       "30   4.597463e-02  \n",
       "31   1.602624e-02  \n",
       "32   2.489425e-04  \n",
       "33   1.445172e-03  \n",
       "34   8.638675e-02  \n",
       "35   1.071365e-03  \n",
       "36   9.473686e-01  \n",
       "37   1.157377e-02  \n",
       "38   3.431405e-02  \n",
       "39   3.233185e-01  \n",
       "40   7.199127e-02  \n",
       "41   2.403863e-01  \n",
       "42   1.800722e-01  \n",
       "43   3.265802e-02  \n",
       "44   6.050023e-01  \n",
       "45   9.677280e-01  \n",
       "46   4.230351e-01  \n",
       "47   1.447696e-03  \n",
       "48   6.623527e-01  \n",
       "49   6.591343e-01  \n",
       "50   4.137157e-02  \n",
       "51   5.293321e-01  \n",
       "52   9.339940e-02  \n",
       "53   5.773239e-01  \n",
       "54   4.807293e-01  \n",
       "55   2.600002e-01  \n",
       "56   4.967530e-01  \n",
       "57   9.279965e-01  \n",
       "58   1.545419e-01  \n",
       "59   3.293969e-01  \n",
       "60   1.014170e-02  \n",
       "61   3.655558e-01  \n",
       "62   3.349992e-01  \n",
       "63   1.244616e-03  \n",
       "64   1.451646e-04  \n",
       "65   2.181254e-04  \n",
       "66   1.104834e-02  \n",
       "67   1.871261e-01  \n",
       "68   5.096535e-03  \n",
       "69   3.612970e-01  \n",
       "70   2.555453e-03  \n",
       "71   2.172383e-02  \n",
       "72   1.504790e-01  \n",
       "73   7.271118e-02  \n",
       "74   1.537740e-01  \n",
       "75   2.141417e-01  \n",
       "76   5.014269e-02  \n",
       "77   2.600009e-05  \n",
       "78   9.898550e-01  \n",
       "79   5.790640e-02  \n",
       "80   4.365218e-01  \n",
       "81   7.620046e-01  \n",
       "82   1.652702e-01  \n",
       "83   2.334644e-01  \n",
       "84   5.929988e-01  \n",
       "85   8.140282e-01  \n",
       "86   4.336048e-01  \n",
       "87   6.279156e-01  \n",
       "88   3.178584e-01  \n",
       "89   6.197718e-01  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
