{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70KtyQmmdKdl",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:51.775073Z",
     "start_time": "2024-02-19T13:59:51.758057Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4813,
     "status": "ok",
     "timestamp": 1707967590344,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "70KtyQmmdKdl",
    "outputId": "7656ebba-c93f-4bf0-efe6-e151eafdc366"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# # Mount Google Drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8a7f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:56.064014Z",
     "start_time": "2024-02-19T13:59:51.777660Z"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1707967599994,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "fa8a7f05"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import category_encoders as ce\n",
    "# import copy\n",
    "# import polars as pl\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "# import plotly.express as px\n",
    "# %matplotlib inline\n",
    "# matplotlib.rcParams['font.family'] = 'Malgun Gothic' # ÌïúÍ∏Ä Ìå®Ïπò\n",
    "# Preprocessing & Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Modeling\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor, XGBRFRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
    "# from sklearn.base import ClassifierMixin\n",
    "\n",
    "# CatBoost\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss,mean_squared_error\n",
    "import sklearn\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import datetime # ‚ö†Ô∏è2019ÎÖÑ 12Ïõî30ÏùºÍ≥º 31ÏùºÏùò week of yearÍ∞Ä 1Ïù∏ Ïò§Î•òÍ∞Ä ÏûàÏùå\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "import holidays\n",
    "\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from num2words import num2words\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1fea9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:56.109952Z",
     "start_time": "2024-02-19T13:59:56.065526Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1707967811792,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "bc1fea9f",
    "outputId": "e060e065-27b4-4bd1-c8fc-db3c2b3904f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_row',None)\n",
    "pd.set_option('display.max_column',None)\n",
    "\n",
    "### Setting universal random_state\n",
    "np.random.seed(142)\n",
    "random.seed(142)\n",
    "sklearn.utils.check_random_state(142)\n",
    "torch.manual_seed(142)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark=False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "072c99fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:56.125570Z",
     "start_time": "2024-02-19T13:59:56.112127Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1707967813304,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "072c99fa"
   },
   "outputs": [],
   "source": [
    "class PrepareData():\n",
    "    def __init__(self,stock_fp,news_fp):\n",
    "        self.stock_filepath = stock_fp\n",
    "        self.news_filepath = news_fp\n",
    "        self.topic_classes = ['CEO', 'CFO', 'Layoffs', 'Political', 'PressRelease', 'Undefined',\n",
    "       'cramer', 'earnings', 'gold', 'manda', 'paylimitwall', 'paywall',\n",
    "       'product', 'recession', 'tanalysis'] # undefinedÏùò classÍ∞Ä 5\n",
    "\n",
    "    def load_data(self):\n",
    "        stock = pd.read_csv(self.stock_filepath,index_col=0)\n",
    "        news = pd.read_csv(self.news_filepath,index_col=0)\n",
    "        ### parse date manually\n",
    "        stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "        news['date'] = pd.to_datetime(news['date'])\n",
    "        return stock, news\n",
    "\n",
    "    def merging(self, stock, news):\n",
    "        ### fill na value of PINS column\n",
    "        stock['PINS'] = stock['PINS'].fillna(stock['PINS'].iloc[75])\n",
    "\n",
    "        ### drop 'news_id' column\n",
    "#         news = news.drop(columns=['news_id'])\n",
    "\n",
    "        ### add date range from 18.01.02 to 18.12.31\n",
    "        temp_range = pd.DataFrame(dict(zip(stock.columns,[pd.date_range(start='2018-01-02',end='2018-12-31'),\n",
    "                                0,0,0,0, # 4\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 16\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 28\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 36\n",
    "                                0 # 37\n",
    "                            ])))\n",
    "        stock_inc = pd.concat([temp_range,stock],axis=0)\n",
    "\n",
    "        ### merge stock_inc and news\n",
    "        # left = stock_inc\n",
    "        # on = date\n",
    "        # how = left\n",
    "        # rename 'date' to 'Date' of news df\n",
    "        news = news.rename(columns={'date':'Date'})\n",
    "        merged = pd.merge(left=stock_inc,right=news,on='Date',how='left')\n",
    "\n",
    "        ### Cut before 2018-02-13\n",
    "        merged = merged[42:].reset_index(drop=True)\n",
    "\n",
    "        # fill na with latest non-null values\n",
    "        columns_to_fill = ['source_name', 'topics', 'rank_score',\n",
    "                        'sentiment_Negative','sentiment_Neutral',\n",
    "                        'sentiment_Positive', 'type_Article', 'type_Video']\n",
    "        merged_fillna = merged.copy()\n",
    "        for column in columns_to_fill:\n",
    "            merged_fillna[column].fillna(method='ffill',inplace=True)\n",
    "\n",
    "        ### add moving average to sentiments\n",
    "        ma_nums = [5,60,120]\n",
    "        def mode_window(window):\n",
    "            return window.mode().iloc[0] if not len(window.mode())==0 else None\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MA_sent_Neg']=merged_fillna['sentiment_Negative'].rolling(\n",
    "            window=num).mean()\n",
    "            merged_fillna[f'{num}MA_sent_Neu']=merged_fillna['sentiment_Neutral'].rolling(\n",
    "            window=num).mean()\n",
    "            merged_fillna[f'{num}MA_sent_Pos']=merged_fillna['sentiment_Positive'].rolling(\n",
    "            window=num).mean()\n",
    "        ### add moving mode to sentiments\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MM_sent_Neg']=merged_fillna['sentiment_Negative'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "            merged_fillna[f'{num}MM_sent_Neu']=merged_fillna['sentiment_Neutral'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "            merged_fillna[f'{num}MM_sent_Pos']=merged_fillna['sentiment_Positive'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "        ### adding moving mode to topics\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MM_topics']=merged_fillna['topics'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "\n",
    "        ### drop before 2019-01-02\n",
    "        total_df = merged_fillna.iloc[322:]\n",
    "        total_df = total_df.reset_index(drop=True)\n",
    "\n",
    "        ### drop unnecessaray columns\n",
    "        drop_cols = ['source_name','topics','rank_score',\n",
    "                    'sentiment_Negative','sentiment_Neutral',\n",
    "                    'sentiment_Positive','type_Article','type_Video']\n",
    "        total_df = total_df.drop(columns=drop_cols)\n",
    "\n",
    "        return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e695f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:56.140814Z",
     "start_time": "2024-02-19T13:59:56.126572Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1707967816443,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "96e695f0"
   },
   "outputs": [],
   "source": [
    "# Ï†ÑÏ≤¥ Ïò§Ï∞®Ïú®\n",
    "def error_ratio(pred, true):\n",
    "    return np.mean(np.abs(pred-true)/true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c21dd4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:58.356524Z",
     "start_time": "2024-02-19T13:59:56.143375Z"
    },
    "executionInfo": {
     "elapsed": 9234,
     "status": "ok",
     "timestamp": 1707967826109,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "6c21dd4c"
   },
   "outputs": [],
   "source": [
    "stock_filepath = '../../data/stock_price/netflix_60.csv' # Í∞ÅÏûê ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "news_filepath = '../../data/scraping/news_processed_filtered_2.csv'\n",
    "# stock_filepath = './drive/MyDrive/Colab Notebooks/data/bitamin_mini_project/netflix_60.csv'\n",
    "# news_filepath = './drive/MyDrive/Colab Notebooks/data/bitamin_mini_project/news_processed_filtered_2.csv'\n",
    "loader = PrepareData(stock_filepath,news_filepath)\n",
    "stock_df, news_df=loader.load_data() # >> Í∞êÏÑ±Î∂ÑÏÑù ÎØ∏Ìè¨Ìï®ÏúºÎ°ú Î™®Îç∏ ÎèåÎ¶¥ Îïê stock_df Î∞îÎ°ú ÏÇ¨Ïö©ÌïòÎ©¥ Îê®\n",
    "total_df = loader.merging(stock=stock_df, news=news_df) # Ï£ºÏãùÎç∞Ïù¥ÌÑ∞ÏÖãÏóê Í∞êÏÑ±Î∂ÑÏÑù,ÌÜ†ÌîΩ Ìè¨Ìï®ÏãúÌÇ® Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26138410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:58.372168Z",
     "start_time": "2024-02-19T13:59:58.358589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24.99\n",
       "1    24.99\n",
       "2    24.99\n",
       "Name: PINS, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df['PINS'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8325c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:58.387883Z",
     "start_time": "2024-02-19T13:59:58.375169Z"
    }
   },
   "outputs": [],
   "source": [
    "### Set index as Date\n",
    "stock_df = stock_df.set_index('Date')\n",
    "total_df = total_df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5DzQJJ6MV9uw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:58.403412Z",
     "start_time": "2024-02-19T13:59:58.389880Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1707976805494,
     "user": {
      "displayName": "skiersong9",
      "userId": "11108221763786546299"
     },
     "user_tz": -540
    },
    "id": "5DzQJJ6MV9uw",
    "outputId": "a3f73640-c5a3-4a93-aeb7-1bebb6e31c18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>468.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>470.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close Price\n",
       "Date                   \n",
       "2024-01-02       468.50\n",
       "2024-01-03       470.26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filepath = '../../data/test.csv'\n",
    "# test_filepath = './drive/MyDrive/Colab Notebooks/data/bitamin_mini_project/test.csv'\n",
    "y_test = pd.read_csv(test_filepath,index_col=0)\n",
    "y_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92f532c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:58.433714Z",
     "start_time": "2024-02-19T13:59:58.406502Z"
    }
   },
   "outputs": [],
   "source": [
    "### Í∞ÅÏ¢Ö Ìï®Ïàò Î∞è ÌÅ¥ÎûòÏä§ Ï†ïÏùòÏö©\n",
    "def split_xy(dataset, time_steps, y_column):\n",
    "        x, y = list(), list()\n",
    "        for i in range(len(dataset)):\n",
    "            x_end_number = i + time_steps\n",
    "            y_end_number = x_end_number + y_column\n",
    "\n",
    "            if y_end_number > len(dataset):\n",
    "                break\n",
    "            tmp_x = dataset.iloc[i:x_end_number, ]  # Adjusted for Pandas\n",
    "            tmp_y = dataset.iloc[x_end_number:y_end_number, 1]  # Adjusted for Pandas\n",
    "            x.append(tmp_x.values)  # Convert to numpy array\n",
    "            y.append(tmp_y.values)  # Convert to numpy array\n",
    "\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=120):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Model definition using Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead=4, num_layers=2, dropout=0.2, output_size=10,max_len=120):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Linear(input_dim, d_model,bias=True)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout,max_len)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead,bias=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, output_size,bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.decoder(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super(LSTMModel,self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_size = 64, batch_first = True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.seq = nn.Sequential(nn.Linear(64, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, PRED_SIZE)\n",
    "                                )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x,_ = self.lstm(x)\n",
    "#         print('output of lstm :',x.size()) # torch.Size([1, 120, 64])\n",
    "        x = self.dropout(x[:,-1,:])\n",
    "#         print('from lstm to linear :',x.size()) # torch.Size([1, 64])\n",
    "        x = self.seq(x)\n",
    "        return x  \n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "import copy\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a542a829",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:58.449315Z",
     "start_time": "2024-02-19T13:59:58.436723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>error_ratio_total</th>\n",
       "      <th>r_squared_total</th>\n",
       "      <th>p_value_total</th>\n",
       "      <th>error_ratio_stock</th>\n",
       "      <th>r_squared_stock</th>\n",
       "      <th>p_value_stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_size  batch_size  output_size  hidden_size  error_ratio_total  \\\n",
       "0         0           0            0            0                  0   \n",
       "\n",
       "   r_squared_total  p_value_total  error_ratio_stock  r_squared_stock  \\\n",
       "0                0              0                  0                0   \n",
       "\n",
       "   p_value_stock  \n",
       "0              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.DataFrame({\n",
    "    'seq_size':[0],\n",
    "    'batch_size':[0],\n",
    "    'output_size':[0],\n",
    "    'hidden_size':[0],\n",
    "    'error_ratio_total':[0],\n",
    "    'r_squared_total':[0],\n",
    "    'p_value_total':[0],\n",
    "    'error_ratio_stock':[0],\n",
    "    'r_squared_stock':[0],\n",
    "    'p_value_stock':[0]\n",
    "})\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b51d2c31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:59:58.465062Z",
     "start_time": "2024-02-19T13:59:58.451315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>output_size</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>val_loss_total</th>\n",
       "      <th>val_loss_stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_size  batch_size  output_size  hidden_size  val_loss_total  \\\n",
       "0         0           0            0            0               0   \n",
       "\n",
       "   val_loss_stock  \n",
       "0               0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df = pd.DataFrame({\n",
    "        'seq_size':[0],\n",
    "        'batch_size':[0],\n",
    "        'output_size':[0],\n",
    "        'hidden_size':[0],\n",
    "        'val_loss_total':[0],\n",
    "        'val_loss_stock':[0]\n",
    "    })\n",
    "val_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c34ae2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:31:59.282086Z",
     "start_time": "2024-02-19T14:31:59.277718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d2e8b51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:18:09.978289Z",
     "start_time": "2024-02-19T14:14:19.603641Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000, Train Loss: nan, Validation Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [03:50, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 125\u001b[0m\n\u001b[0;32m    123\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model_total(x_batch)\n\u001b[0;32m    124\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion_total(outputs, y_batch)\n\u001b[1;32m--> 125\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m optimizer_total\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    127\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SEQUENCE_SIZEs = [30,60,120]\n",
    "# BATCH_SIZEs = [1,5,32,64,128]\n",
    "BATCH_SIZEs = [1,4,8]\n",
    "OUTPUT_SIZEs = [10,20]\n",
    "HIDDEN_SIZEs = [64,128,256]\n",
    "import itertools\n",
    "\n",
    "last_val_losses_total, last_val_losses_stock = [],[]\n",
    "\n",
    "\n",
    "for SEQUENCE_SIZE,BATCH_SIZE,OUTPUT_SIZE,HIDDEN_SIZE in tqdm(itertools.product(SEQUENCE_SIZEs,BATCH_SIZEs,OUTPUT_SIZEs,HIDDEN_SIZEs)):\n",
    "\n",
    "    x_total, y_total = split_xy(total_df,SEQUENCE_SIZE,OUTPUT_SIZE)\n",
    "    x_stock, y_stock = split_xy(stock_df, SEQUENCE_SIZE,OUTPUT_SIZE)\n",
    "    \n",
    "    ### Set Test data\n",
    "    x_test_total = np.reshape(total_df.iloc[-SEQUENCE_SIZE:].values,(1,SEQUENCE_SIZE,-1))\n",
    "    x_test_stock = np.reshape(stock_df.iloc[-SEQUENCE_SIZE:].values,(1,SEQUENCE_SIZE,-1))\n",
    "\n",
    "    ### Scaling\n",
    "    # reshape to 2d and scaling\n",
    "    ss = StandardScaler()\n",
    "    x_total_scaled = ss.fit_transform(np.reshape(x_total,(\n",
    "            x_total.shape[0],x_total.shape[1]*x_total.shape[2]\n",
    "        )))\n",
    "    x_test_total = ss.transform(np.reshape(x_test_total,(\n",
    "            x_test_total.shape[0],x_test_total.shape[1]*x_test_total.shape[2]\n",
    "        )))\n",
    "    x_stock_scaled = ss.fit_transform(np.reshape(x_stock,(\n",
    "            x_stock.shape[0],x_stock.shape[1]*x_stock.shape[2]\n",
    "        )))\n",
    "    x_test_stock = ss.transform(np.reshape(x_test_stock,(\n",
    "            x_test_stock.shape[0],x_test_stock.shape[1]*x_test_stock.shape[2]\n",
    "        )))\n",
    "\n",
    "    ### Train, Validation Split\n",
    "    tv_ratio = int(x_total_scaled.shape[0]*0.2) \n",
    "    x_train_total = x_total_scaled[:-tv_ratio]\n",
    "    x_val_total = x_total_scaled[-tv_ratio:]\n",
    "    y_train_total = y_total[:-tv_ratio]\n",
    "    y_val_total = y_total[-tv_ratio:]\n",
    "\n",
    "    tv_ratio = int(x_stock_scaled.shape[0]*0.2) \n",
    "    x_train_stock = x_stock_scaled[:-tv_ratio]\n",
    "    x_val_stock = x_stock_scaled[-tv_ratio:]\n",
    "    y_train_stock = y_stock[:-tv_ratio]\n",
    "    y_val_stock = y_stock[-tv_ratio:]\n",
    "\n",
    "    ### reshape to 3d\n",
    "    x_train_total = np.reshape(x_train_total,(-1,SEQUENCE_SIZE,x_total.shape[2]))\n",
    "    x_val_total = np.reshape(x_val_total,(-1,SEQUENCE_SIZE,x_total.shape[2]))\n",
    "    x_test_total = np.reshape(x_test_total,(1,SEQUENCE_SIZE,-1))\n",
    "\n",
    "    x_train_stock = np.reshape(x_train_stock,(-1,SEQUENCE_SIZE,x_stock.shape[2]))\n",
    "    x_val_stock = np.reshape(x_val_stock,(-1,SEQUENCE_SIZE,x_stock.shape[2]))\n",
    "    x_test_stock = np.reshape(x_test_stock,(1,SEQUENCE_SIZE,-1))\n",
    "    \n",
    "    ### to DataLoader\n",
    "    train_loader_total = DataLoader(\n",
    "            TensorDataset(torch.tensor(x_train_total,dtype=torch.float32),\n",
    "                          torch.tensor(y_train_total,dtype=torch.float32)),\n",
    "            batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader_total = DataLoader(\n",
    "            TensorDataset(torch.tensor(x_val_total,dtype=torch.float32),\n",
    "                          torch.tensor(y_val_total,dtype=torch.float32)),\n",
    "            batch_size=BATCH_SIZE, shuffle=False\n",
    "        )\n",
    "\n",
    "    train_loader_stock = DataLoader(\n",
    "            TensorDataset(torch.tensor(x_train_stock,dtype=torch.float32),\n",
    "                          torch.tensor(y_train_stock,dtype=torch.float32)),\n",
    "            batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader_stock = DataLoader(\n",
    "            TensorDataset(torch.tensor(x_val_stock,dtype=torch.float32),\n",
    "                          torch.tensor(y_val_stock,dtype=torch.float32)),\n",
    "            batch_size=BATCH_SIZE, shuffle=False\n",
    "        )\n",
    "\n",
    "    ### test data to Tensor\n",
    "    x_test_total = torch.tensor(x_test_total,dtype=torch.float32)\n",
    "    x_test_stock = torch.tensor(x_test_stock,dtype=torch.float32)\n",
    "    \n",
    "    ### Model Declare\n",
    "#     model_total = TransformerModel(input_dim=x_train_total.shape[2],\n",
    "#                                max_len=240,\n",
    "#                                d_model=HIDDEN_SIZE,\n",
    "#                                output_size=OUTPUT_SIZE).to(device)\n",
    "#     model_stock = TransformerModel(input_dim=x_train_stock.shape[2],\n",
    "#                                max_len=240,\n",
    "#                                d_model=HIDDEN_SIZE,\n",
    "#                                output_size=OUTPUT_SIZE).to(device)\n",
    "    model_total = LSTMModel(x_train_total.shape[2])\n",
    "    model_stock = LSTMModel(x_train_total.shape[2])\n",
    "    \n",
    "    # Model Compile\n",
    "    criterion_total = RMSELoss()\n",
    "    # criterion_total = nn.MSELoss()\n",
    "    optimizer_total = torch.optim.Adam(model_total.parameters(), lr=0.001)\n",
    "    scheduler_total = ReduceLROnPlateau(optimizer_total, 'min', factor=0.3, patience=10, verbose=False)\n",
    "\n",
    "    criterion_stock = RMSELoss()\n",
    "    # criterion_stock = nn.MSELoss()\n",
    "    optimizer_stock = torch.optim.Adam(model_stock.parameters(), lr=0.001)\n",
    "    scheduler_stock = ReduceLROnPlateau(optimizer_stock, 'min', factor=0.3, patience=10, verbose=False)\n",
    "    \n",
    "    ### Total dataset Training\n",
    "    epochs = 1000\n",
    "    epoch_counter = 0\n",
    "    min_val_loss = float('inf')\n",
    "    done = False\n",
    "    patience = 50\n",
    "    es = EarlyStopping(patience=patience)\n",
    "    tr_losses_fp, val_losses_fp = [],[]\n",
    "\n",
    "    while not done and epoch_counter<epochs:\n",
    "        epoch_counter+=1\n",
    "\n",
    "        ### Training\n",
    "        model_total.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader_total:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer_stock.zero_grad()\n",
    "            outputs = model_total(x_batch)\n",
    "            loss = criterion_total(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer_total.step()\n",
    "            train_losses.append(loss.item())\n",
    "        train_loss = np.mean(train_losses)\n",
    "        tr_losses_fp.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model_total.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_total:\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model_total(x_batch)\n",
    "                loss = criterion_total(outputs, y_batch)\n",
    "                val_losses.append(loss.item())\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_losses_fp.append(val_loss)\n",
    "        scheduler_total.step(val_loss)\n",
    "\n",
    "        if es(model_total, val_loss):\n",
    "            done = True\n",
    "\n",
    "        # print loss every 20 epochs\n",
    "        if epoch_counter%20 == 0 :\n",
    "            print(f\"Epoch {epoch_counter}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    last_val_losses_total.append(val_loss.to('cpu').detach().numpy())\n",
    "    val_loss_total = val_loss.to('cpu').detach().numpy()\n",
    "    \n",
    "    ### Plot Train-Val loss\n",
    "#     path = f'../../plots/train_val_loss/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total.png'\n",
    "#     plt.plot(range(len(tr_losses_fp)),tr_losses_fp,color='blue',label='train_loss')\n",
    "#     plt.plot(range(len(val_losses_fp)),val_losses_fp,color='red',label='val_loss')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(path)\n",
    "#     plt.clf() # clear current figure\n",
    "    \n",
    "    ### evaluation\n",
    "    model_total.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_total = model_total(x_test_total.to(device))\n",
    "    pred_total = pred_total.to('cpu').detach().numpy()\n",
    "    pred_total_reshape = np.reshape(pred_total,(-1,1))\n",
    "    \n",
    "    y_test_SEQ = y_test[:OUTPUT_SIZE]\n",
    "    \n",
    "    ### Evaluate with error_ratio\n",
    "    print(f'üî∏EVAL STARTüî∏ \\n transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total')\n",
    "    error_ratio_total = error_ratio(pred_total_reshape, y_test_SEQ)\n",
    "    print('Error Ratio :',error_ratio_total)\n",
    "    ### R-squared, p-value\n",
    "    # r_squaredÎäî 1Ïóê Í∞ÄÍπåÏö∏ÏàòÎ°ù Î™®Îç∏Ïù¥ Ï¢ãÏùÄ Í≤É\n",
    "    # p_valueÎäî 0.05 Î≥¥Îã§ ÏûëÏúºÎ©¥ Ïú†ÏùòÎØ∏Ìïú Í≤É\n",
    "    # ÎëêÍ∞ÄÏßÄÎ•º ÎßåÏ°±ÌïòÏßÄ Î™ªÌïú Í≤ÉÏùÄ Í≥ºÏ†ÅÌï©Îêú Í≤É\n",
    "    r_squared_total = sm.OLS(y_test_SEQ,sm.add_constant(pred_total_reshape)).fit().rsquared\n",
    "    p_value_total = sm.OLS(y_test_SEQ,sm.add_constant(pred_total_reshape)).fit().f_pvalue\n",
    "    print(\"R-squared:\", r_squared_total)\n",
    "    print(\"p-value:\", p_value_total)\n",
    "    print('last validation loss:',val_loss)\n",
    "    print('üî∏EVAL ENDüî∏')\n",
    "    \n",
    "    ### Save result\n",
    "    path = f'../../data/results/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total_0219.csv'\n",
    "    result_total = pd.DataFrame(data={'true':np.reshape(y_test_SEQ,(-1)),\n",
    "                  'pred':np.reshape(pred_total_reshape,(-1)),\n",
    "                  'error_ratio':((pred_total_reshape-y_test_SEQ)/y_test_SEQ).iloc[:,0].values},\n",
    "                  index=y_test_SEQ.index)\n",
    "\n",
    "    result_total.to_csv(path)\n",
    "    \n",
    "    ### Plot Result\n",
    "    r = len(np.reshape(y_test_SEQ,(-1)))\n",
    "    plt.plot(list(range(r)),np.reshape(y_test_SEQ,(-1)), color='blue', label='true value')\n",
    "    plt.plot(list(range(r)),np.reshape(pred_total_reshape,(-1)), color='red',alpha=0.6, label='prediction')\n",
    "    plt.title(f'transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total')\n",
    "    plt.legend()\n",
    "    fig_path = f'../../plots/results/0219_transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total.png'\n",
    "    plt.savefig(fig_path,dpi=300)\n",
    "#     plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "#     actual_stock_price = stock_df[['Close','5MA','120MA']].reset_index().rename(columns={'Close':'true'})\n",
    "#     actual_stock_price[['pred','error_ratio']] = 0\n",
    "#     combined_df = pd.concat([actual_stock_price,\n",
    "#                             result_total.copy().reset_index()],\n",
    "#                             axis=0)\n",
    "#     combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "#     combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     plt.plot(combined_df['Date'], combined_df['true'],\n",
    "#              color='blue',label='Actual Stock Price')\n",
    "#     plt.plot(combined_df['Date'][:-OUTPUT_SIZE],combined_df['5MA'][:-OUTPUT_SIZE],\n",
    "#             color='purple',linestyle=':',label='5MA')\n",
    "#     plt.plot(combined_df['Date'][:-OUTPUT_SIZE],combined_df['5MA'][:-OUTPUT_SIZE],\n",
    "#             color='green',linestyle='--',label='120MA')\n",
    "#     plt.plot(combined_df['Date'][-OUTPUT_SIZE:],combined_df['pred'][-OUTPUT_SIZE:],color='red',\n",
    "#              label='Predicted Stock Price')\n",
    "#     for i,row in combined_df.iterrows():\n",
    "#         if not np.isnan(row['error_ratio']):\n",
    "#             plt.annotate(f'{row[\"error_ratio\"]*100:1f}%',(i,row['pred']),\n",
    "#                      textcoords='offset points',xytext=(0,10),ha='center')\n",
    "\n",
    "#     plt.title(f'transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total')\n",
    "#     plt.xlabel('Date')\n",
    "#     plt.ylabel('Price')\n",
    "#     plt.legend()\n",
    "\n",
    "#     fig_path = f'../../plots/results/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_total.png'\n",
    "#     plt.savefig(fig_path,dpi=300)\n",
    "# #     plt.show()\n",
    "#     plt.clf()\n",
    "    \n",
    "    ### Stock-only dataset Training\n",
    "    epochs = 1000\n",
    "    epoch_counter = 0\n",
    "    min_val_loss = float('inf')\n",
    "    done = False\n",
    "    patience = 50\n",
    "    es = EarlyStopping(patience=patience)\n",
    "    tr_losses_fp, val_losses_fp = [],[]\n",
    "\n",
    "    while not done and epoch_counter<epochs:\n",
    "        epoch_counter+=1\n",
    "\n",
    "        ### Training\n",
    "        model_stock.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader_stock:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer_stock.zero_grad()\n",
    "            outputs = model_stock(x_batch)\n",
    "            loss = criterion_stock(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer_stock.step()\n",
    "            train_losses.append(loss.item())\n",
    "        train_loss = np.mean(train_losses)\n",
    "        tr_losses_fp.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model_stock.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_stock:\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model_stock(x_batch)\n",
    "                loss = criterion_stock(outputs, y_batch)\n",
    "                val_losses.append(loss.item())\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_losses_fp.append(val_loss)\n",
    "        scheduler_stock.step(val_loss)\n",
    "\n",
    "        if es(model_stock, val_loss):\n",
    "            done = True\n",
    "\n",
    "        # print loss every 20 epochs\n",
    "        if epoch_counter%20 == 0 :\n",
    "            print(f\"Epoch {epoch_counter}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    last_val_losses_stock.append(val_loss.to('cpu').detach().numpy())\n",
    "    val_loss_stock = val_loss.to('cpu').detach().numpy()\n",
    "\n",
    "    ### Plot Train-Val loss\n",
    "#     path = f'../../plots/train_val_loss/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock.png'\n",
    "#     plt.plot(range(len(tr_losses_fp)),tr_losses_fp,color='blue',label='train_loss')\n",
    "#     plt.plot(range(len(val_losses_fp)),val_losses_fp,color='red',label='val_loss')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(path)\n",
    "#     plt.clf() # clear current figure\n",
    "    \n",
    "    # evaluation\n",
    "    model_stock.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_stock = model_stock(x_test_stock.to(device))\n",
    "    pred_stock = pred_stock.to('cpu').detach().numpy()\n",
    "    pred_stock_reshape = np.reshape(pred_stock,(-1,1))\n",
    "    \n",
    "    ### Evaluate with error_ratio\n",
    "    print(f'üî∏EVAL STARTüî∏ \\n transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock')\n",
    "    error_ratio_stock = error_ratio(pred_stock_reshape, y_test_SEQ)\n",
    "    print('Error Ratio :',error_ratio_stock)\n",
    "    ### R-squared, p-value\n",
    "    # r_squaredÎäî 1Ïóê Í∞ÄÍπåÏö∏ÏàòÎ°ù Î™®Îç∏Ïù¥ Ï¢ãÏùÄ Í≤É\n",
    "    # p_valueÎäî 0.05 Î≥¥Îã§ ÏûëÏúºÎ©¥ Ïú†ÏùòÎØ∏Ìïú Í≤É\n",
    "    # ÎëêÍ∞ÄÏßÄÎ•º ÎßåÏ°±ÌïòÏßÄ Î™ªÌïú Í≤ÉÏùÄ Í≥ºÏ†ÅÌï©Îêú Í≤É\n",
    "    r_squared_stock = sm.OLS(y_test_SEQ,sm.add_constant(pred_stock_reshape)).fit().rsquared\n",
    "    p_value_stock = sm.OLS(y_test_SEQ,sm.add_constant(pred_stock_reshape)).fit().f_pvalue\n",
    "    print(\"R-squared:\", r_squared_stock)\n",
    "    print(\"p-value:\", p_value_stock)\n",
    "    print('last validation loss:',val_loss)\n",
    "    print('üî∏EVAL ENDüî∏')\n",
    "    \n",
    "    ### Save result\n",
    "    path = f'../../data/results/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock_0219.csv'\n",
    "\n",
    "    result_stock = pd.DataFrame(data={'true':np.reshape(y_test_SEQ,(-1)),\n",
    "                  'pred':np.reshape(pred_stock_reshape,(-1)),\n",
    "                  'error_ratio':((pred_stock_reshape-y_test_SEQ)/y_test_SEQ).iloc[:,0].values},\n",
    "                  index=y_test_SEQ.index)\n",
    "\n",
    "    result_stock.to_csv(path)\n",
    "\n",
    "    ### Plot Result\n",
    "    r = len(np.reshape(y_test_SEQ,(-1)))\n",
    "    plt.plot(list(range(r)),np.reshape(y_test_SEQ,(-1)), color='blue', label='true value')\n",
    "    plt.plot(list(range(r)),np.reshape(pred_stock_reshape,(-1)), color='red',alpha=0.6, label='prediction')\n",
    "    plt.title(f'transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock')\n",
    "    plt.legend()\n",
    "    fig_path = f'../../plots/results/0219_transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock.png'\n",
    "    plt.savefig(fig_path,dpi=300)\n",
    "#     plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "#     actual_stock_price = stock_df[['Close','5MA','120MA']].reset_index().rename(columns={'Close':'true'})\n",
    "#     actual_stock_price[['pred','error_ratio']] = 0\n",
    "#     combined_df = pd.concat([actual_stock_price,\n",
    "#                             result_stock.copy().reset_index()],\n",
    "#                             axis=0)\n",
    "#     combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "#     combined_df = combined_df.reset_index(drop=True)\n",
    "#     combined_df\n",
    "\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     plt.plot(combined_df['Date'], combined_df['true'],\n",
    "#              color='blue',label='Actual Stock Price')\n",
    "#     plt.plot(combined_df['Date'][:-OUTPUT_SIZE],combined_df['5MA'][:-OUTPUT_SIZE],\n",
    "#             color='purple',linestyle=':',label='5MA')\n",
    "#     plt.plot(combined_df['Date'][:-OUTPUT_SIZE],combined_df['5MA'][:-OUTPUT_SIZE],\n",
    "#             color='green',linestyle='--',label='120MA')\n",
    "#     plt.plot(combined_df['Date'][-OUTPUT_SIZE:],combined_df['pred'][-OUTPUT_SIZE:],color='red',\n",
    "#              label='Predicted Stock Price')\n",
    "#     for i,row in combined_df.iterrows():\n",
    "#         if not np.isnan(row['error_ratio']):\n",
    "#             plt.annotate(f'{row[\"error_ratio\"]*100:1f}%',(i,row['pred']),\n",
    "#                      textcoords='offset points',xytext=(0,10),ha='center')\n",
    "\n",
    "#     plt.title(f'transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock')\n",
    "#     plt.xlabel('Date')\n",
    "#     plt.ylabel('Price')\n",
    "#     plt.legend()\n",
    "\n",
    "#     fig_path = f'../../plots/results/transformer_seq{SEQUENCE_SIZE}_batch{BATCH_SIZE}_out{OUTPUT_SIZE}_hidden{HIDDEN_SIZE}_stock.png'\n",
    "#     plt.savefig(fig_path,dpi=300)\n",
    "# #     plt.show()\n",
    "#     plt.clf()\n",
    "    \n",
    "    ### add results to final dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        'seq_size':[SEQUENCE_SIZE],\n",
    "        'batch_size':[BATCH_SIZE],\n",
    "        'output_size':[OUTPUT_SIZE],\n",
    "        'hidden_size':[HIDDEN_SIZE],\n",
    "        'error_ratio_total':[error_ratio_total],\n",
    "        'r_squared_total':[r_squared_total],\n",
    "        'p_value_total':[p_value_total],\n",
    "        'error_ratio_stock':[error_ratio_stock],\n",
    "        'r_squared_stock':[r_squared_stock],\n",
    "        'p_value_stock':[p_value_stock]\n",
    "    })\n",
    "    final = pd.concat([final,results_df],axis=0)\n",
    "    \n",
    "    val_loss_df_temp = pd.DataFrame({\n",
    "        'seq_size':[SEQUENCE_SIZE],\n",
    "        'batch_size':[BATCH_SIZE],\n",
    "        'output_size':[OUTPUT_SIZE],\n",
    "        'hidden_size':[HIDDEN_SIZE],\n",
    "        'val_loss_total':[val_loss_total],\n",
    "        'val_loss_stock':[val_loss_stock]\n",
    "    })\n",
    "    val_loss_df = pd.concat([val_loss_df,val_loss_df_temp],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bad02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:11:59.614279Z",
     "start_time": "2024-02-19T14:11:59.614279Z"
    }
   },
   "outputs": [],
   "source": [
    "### save final \n",
    "# remove first row(dummy row)\n",
    "final = final.iloc[1:].reset_index(drop=True)\n",
    "print(final.shape)\n",
    "final.to_csv('../../data/results/final_all_results_0219.csv')\n",
    "val_loss_df = val_loss_df.iloc[1:].reset_index(drop=True)\n",
    "print(val_loss_df.shape)\n",
    "val_loss_df.to_csv('../../data/results/val_loss_results_0219.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b0cfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:11:59.616295Z",
     "start_time": "2024-02-19T14:11:59.616295Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('../../data/results/final_all_results.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fb75e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:11:59.617527Z",
     "start_time": "2024-02-19T14:11:59.617527Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
