{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc80feb9",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2307e7f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:18.066284Z",
     "start_time": "2024-02-23T03:42:18.054771Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# # Mount Google Drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2059c61b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.937740Z",
     "start_time": "2024-02-23T03:42:18.068283Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "2059c61b",
    "outputId": "576afb16-4157-4337-f4db-d04264361874"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 49\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackingClassifier, StackingRegressor\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# from sklearn.base import ClassifierMixin\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# CatBoost\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# from catboost import CatBoostRegressor\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01minit\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader, TensorDataset\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\torch\\__init__.py:123\u001b[0m\n\u001b[0;32m    121\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 123\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import category_encoders as ce\n",
    "# import copy\n",
    "# import polars as pl\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "# import plotly.express as px\n",
    "# %matplotlib inline\n",
    "# matplotlib.rcParams['font.family'] = 'Malgun Gothic' # 한글 패치\n",
    "# Preprocessing & Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Modeling\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor, XGBRFRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
    "# from sklearn.base import ClassifierMixin\n",
    "\n",
    "# CatBoost\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss,mean_squared_error\n",
    "import sklearn\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import datetime # ⚠️2019년 12월30일과 31일의 week of year가 1인 오류가 있음\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations,product\n",
    "from scipy.stats.mstats import gmean\n",
    "import holidays\n",
    "\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from num2words import num2words\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "\n",
    "pd.set_option('display.max_row',None)\n",
    "pd.set_option('display.max_column',None)\n",
    "\n",
    "### Setting universal random_state\n",
    "np.random.seed(142)\n",
    "random.seed(142)\n",
    "sklearn.utils.check_random_state(142)\n",
    "torch.manual_seed(142)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark=False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zVBHDA4xub-S",
   "metadata": {
    "id": "zVBHDA4xub-S"
   },
   "source": [
    "### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jPRldaW5c653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.940948Z",
     "start_time": "2024-02-23T03:42:30.940948Z"
    },
    "id": "jPRldaW5c653"
   },
   "outputs": [],
   "source": [
    "class PrepareData():\n",
    "    def __init__(self,stock_fp,news_fp):\n",
    "        self.stock_filepath = stock_fp\n",
    "        self.news_filepath = news_fp\n",
    "        self.topic_classes = ['CEO', 'CFO', 'Layoffs', 'Political', 'PressRelease', 'Undefined',\n",
    "       'cramer', 'earnings', 'gold', 'manda', 'paylimitwall', 'paywall',\n",
    "       'product', 'recession', 'tanalysis'] # undefined의 class가 5\n",
    "\n",
    "    def load_data(self):\n",
    "        stock = pd.read_csv(self.stock_filepath,index_col=0)\n",
    "        news = pd.read_csv(self.news_filepath,index_col=0)\n",
    "        ### parse date manually\n",
    "        stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "        news['date'] = pd.to_datetime(news['date'])\n",
    "        return stock, news\n",
    "\n",
    "    def merging(self, stock, news):\n",
    "        ### fill na value of PINS column\n",
    "#         stock['PINS'] = stock['PINS'].fillna(stock['PINS'].iloc[75])\n",
    "\n",
    "        ### drop 'news_id' column\n",
    "#         news = news.drop(columns=['news_id'])\n",
    "\n",
    "        ### add date range from 18.01.02 to 18.12.31\n",
    "        temp_range = pd.DataFrame(dict(zip(stock.columns,[pd.date_range(start='2018-01-02',end='2018-12-31'),\n",
    "                                0,0,0,0, # 4\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 16\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 28\n",
    "                                0,0,0,0,\n",
    "                                0,0,0,0, # 36\n",
    "                                0 # 37\n",
    "                            ])))\n",
    "        stock_inc = pd.concat([temp_range,stock],axis=0)\n",
    "\n",
    "        ### merge stock_inc and news\n",
    "        # left = stock_inc\n",
    "        # on = date\n",
    "        # how = left\n",
    "        # rename 'date' to 'Date' of news df\n",
    "        news = news.rename(columns={'date':'Date'})\n",
    "        merged = pd.merge(left=stock_inc,right=news,on='Date',how='left')\n",
    "\n",
    "        ### Cut before 2018-02-13\n",
    "        merged = merged[42:].reset_index(drop=True)\n",
    "\n",
    "        # fill na with latest non-null values\n",
    "        columns_to_fill = ['source_name', 'topics', 'rank_score',\n",
    "                        'sentiment_Negative','sentiment_Neutral',\n",
    "                        'sentiment_Positive', 'type_Article', 'type_Video']\n",
    "        merged_fillna = merged.copy()\n",
    "        for column in columns_to_fill:\n",
    "            merged_fillna[column].fillna(method='ffill',inplace=True)\n",
    "\n",
    "        ### add moving average to sentiments\n",
    "        ma_nums = [5,60,120]\n",
    "        def mode_window(window):\n",
    "            return window.mode().iloc[0] if not len(window.mode())==0 else None\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MA_sent_Neg']=merged_fillna['sentiment_Negative'].rolling(\n",
    "            window=num).mean()\n",
    "            merged_fillna[f'{num}MA_sent_Neu']=merged_fillna['sentiment_Neutral'].rolling(\n",
    "            window=num).mean()\n",
    "            merged_fillna[f'{num}MA_sent_Pos']=merged_fillna['sentiment_Positive'].rolling(\n",
    "            window=num).mean()\n",
    "        ### add moving mode to sentiments\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MM_sent_Neg']=merged_fillna['sentiment_Negative'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "            merged_fillna[f'{num}MM_sent_Neu']=merged_fillna['sentiment_Neutral'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "            merged_fillna[f'{num}MM_sent_Pos']=merged_fillna['sentiment_Positive'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "        ### adding moving mode to topics\n",
    "        for num in ma_nums:\n",
    "            merged_fillna[f'{num}MM_topics']=merged_fillna['topics'].rolling(\n",
    "            window=num).apply(mode_window)\n",
    "\n",
    "        ### drop before 2019-01-02\n",
    "        total_df = merged_fillna.iloc[322:]\n",
    "        total_df = total_df.reset_index(drop=True)\n",
    "\n",
    "        ### drop unnecessaray columns\n",
    "        drop_cols = ['source_name','topics','rank_score',\n",
    "                    'sentiment_Negative','sentiment_Neutral',\n",
    "                    'sentiment_Positive','type_Article','type_Video']\n",
    "        total_df = total_df.drop(columns=drop_cols)\n",
    "\n",
    "        return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cXT6Ts-1dPIr",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.943955Z",
     "start_time": "2024-02-23T03:42:30.943955Z"
    },
    "id": "cXT6Ts-1dPIr"
   },
   "outputs": [],
   "source": [
    "stock_filepath = '../../data/stock_price/netflix_60.csv' # 각자 파일 경로 설정\n",
    "news_filepath = '../../data/scraping/news_processed_filtered_2.csv'\n",
    "# stock_filepath = './drive/MyDrive/Colab Notebooks/data/bitamin_mini_project/netflix_60.csv'\n",
    "# news_filepath = './drive/MyDrive/Colab Notebooks/data/bitamin_mini_project/news_processed_filtered_2.csv'\n",
    "loader = PrepareData(stock_filepath, news_filepath)\n",
    "stock_df, news_df=loader.load_data() # >> 감성분석 미포함으로 모델 돌릴 땐 stock_df 바로 사용하면 됨\n",
    "total_df = loader.merging(stock=stock_df, news=news_df) # 주식데이터셋에 감성분석,토픽 포함시킨 전체 데이터셋\n",
    "testfile = '../../data/test.csv'\n",
    "test_df = pd.read_csv(testfile)\n",
    "\n",
    "stock_df.index = stock_df[\"Date\"]\n",
    "stock_df.drop(columns = \"Date\", inplace = True)\n",
    "stock_df[\"PINS\"].fillna(24.99, inplace = True)\n",
    "total_df.index = total_df[\"Date\"]\n",
    "total_df.drop(columns = \"Date\", inplace = True)\n",
    "total_df[\"PINS\"].fillna(24.99, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc59463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.945951Z",
     "start_time": "2024-02-23T03:42:30.945951Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_row',20)\n",
    "# pd.set_option('display.max_column',8)\n",
    "# total_df[['5MA_sent_Neg','5MA_sent_Neu','5MA_sent_Pos','60MA_sent_Neg',\n",
    "#           '60MA_sent_Neu','60MA_sent_Pos','120MA_sent_Neg','120MA_sent_Neu',\n",
    "#           '120MA_sent_Pos','5MM_sent_Neg','5MM_sent_Neu','5MM_sent_Pos',\n",
    "#           '60MM_sent_Neg','60MM_sent_Neu','60MM_sent_Pos','120MM_sent_Neg',\n",
    "#           '120MM_sent_Neu','120MM_sent_Pos','5MM_topics','60MM_topics',\n",
    "#           '120MM_topics']].iloc[0:115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956bfbf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.946952Z",
     "start_time": "2024-02-23T03:42:30.946952Z"
    }
   },
   "outputs": [],
   "source": [
    " result_df = pd.DataFrame({\n",
    "    'data':[0],\n",
    "    'target':[0],\n",
    "    'seq_size':[0],\n",
    "    'pred_size':[0],\n",
    "    'batch_size':[0],\n",
    "    'hidden_size':[0],\n",
    "    'best_val_loss':[0],\n",
    "    'mean_error_ratio':[0]\n",
    "})\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e804a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a74ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.949959Z",
     "start_time": "2024-02-23T03:42:30.949959Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_losses = []\n",
    "mean_error_ratios = []\n",
    "\n",
    "datanames = ['total_df']#,'stock_df']\n",
    "\n",
    "for dataname in datanames:\n",
    "    ### Choose dataset & Hyperparameter setting\n",
    "    if dataname=='total_df':\n",
    "        data = total_df\n",
    "    else:\n",
    "        data = stock_df\n",
    "    MODEL = 'LSTM'                 # 'LSTM' / 'GRU' / 'Transformer'\n",
    "    TARGET = \"1d_ROC\"                      # \"Close\" / \"1d_ROC\"\n",
    "    SEQ_SIZE = 30                         # 30 / 60 / 120\n",
    "    PRED_SIZE = 10\n",
    "    BATCH_SIZE = 1                        # 1 / 4 / 8\n",
    "    HIDDEN_SIZE = 64                      # 64 / 128\n",
    "    EPOCHS = 1000\n",
    "    \n",
    "    filename = f'{MODEL}_{dataname}_{TARGET}_seq{SEQ_SIZE}_batch{BATCH_SIZE}_hidden{HIDDEN_SIZE}'\n",
    "\n",
    "    print(f'🔸START - {filename}🔸')\n",
    "    ### Make train datset\n",
    "\n",
    "    def split_xy(dataset, time_steps, y_column):\n",
    "        x, y = list(), list()\n",
    "        for i in range(len(dataset)):\n",
    "            x_end_number = i + time_steps\n",
    "            y_end_number = x_end_number + y_column\n",
    "\n",
    "            if y_end_number > len(dataset):\n",
    "                break\n",
    "            tmp_x = dataset.iloc[i:x_end_number, :]  # Adjusted for Pandas\n",
    "            tmp_y = dataset.iloc[x_end_number:y_end_number, :].loc[:, TARGET]\n",
    "            x.append(tmp_x.values)  # Convert to numpy array\n",
    "            y.append(tmp_y.values)  # Convert to numpy array\n",
    "\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    X, y = split_xy(data, SEQ_SIZE, PRED_SIZE)\n",
    "#     print(X[0,:],\"\\n\", y[0])\n",
    "#     print(\"X size : \", X.shape)\n",
    "#     print(\"y size : \", y.shape)\n",
    "\n",
    "    ### Define X_test\n",
    "\n",
    "    X_test = data.tail(SEQ_SIZE).values.reshape(1, SEQ_SIZE, data.shape[1])\n",
    "#     print(X_test)\n",
    "#     print(\"X_test size : \", X_test.shape)\n",
    "\n",
    "    ### Standardization\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X = X.reshape(X.shape[0], SEQ_SIZE, data.shape[1])\n",
    "\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test = X_test.reshape(X_test.shape[0], SEQ_SIZE, data.shape[1])\n",
    "\n",
    "#     print(\"X size : \", X.shape)\n",
    "#     print(\"X_test size : \", X_test.shape)\n",
    "\n",
    "    ### Split train-validation dataset\n",
    "\n",
    "    # to DataLoader\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    for trial in tqdm(list(range(1,51))):\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state = 1, test_size = 0.2)\n",
    "\n",
    "        # to tensor\n",
    "        X_train = torch.tensor(X_train.astype(np.float32), dtype = torch.float32)\n",
    "        X_valid = torch.tensor(X_valid.astype(np.float32), dtype = torch.float32)\n",
    "        y_train = torch.tensor(y_train.astype(np.float32), dtype = torch.float32)\n",
    "        y_valid = torch.tensor(y_valid.astype(np.float32), dtype = torch.float32)\n",
    "\n",
    "        # to DataLoader\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size = BATCH_SIZE, shuffle = True)\n",
    "        val_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "        ### Initialize Model\n",
    "#         try:\n",
    "#             for param in model.parameters():\n",
    "#                 if param.requires_grad:\n",
    "#                     if len(param.shape) > 1:\n",
    "#                         init.xavier_uniform_(param)\n",
    "#                     else:\n",
    "#                         init.zeros_(param)\n",
    "#         except:\n",
    "#             try:\n",
    "#                 model.reset_parameters()\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "        ### Modeling\n",
    "\n",
    "        import copy\n",
    "        class EarlyStopping:\n",
    "            def __init__(self, patience = 5, min_delta = 0, restore_best_weights = True):\n",
    "                self.patience = patience\n",
    "                self.min_delta = min_delta\n",
    "                self.restore_best_weights = restore_best_weights\n",
    "                self.best_model = None\n",
    "                self.best_loss = None\n",
    "                self.counter = 0\n",
    "                self.status = \"\"\n",
    "\n",
    "            def __call__(self, model, val_loss):\n",
    "                if self.best_loss is None:\n",
    "                    self.best_loss = val_loss\n",
    "                    self.best_model = copy.deepcopy(model.state_dict())\n",
    "                elif self.best_loss - val_loss >= self.min_delta:\n",
    "                    self.best_model = copy.deepcopy(model.state_dict())\n",
    "                    self.best_loss = val_loss\n",
    "                    self.counter = 0\n",
    "                    self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "                else:\n",
    "                    self.counter += 1\n",
    "                    self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "                    if self.counter >= self.patience:\n",
    "                        self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                        if self.restore_best_weights:\n",
    "                            model.load_state_dict(self.best_model)\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "        class LSTMModel(nn.Module):\n",
    "            def __init__(self,input_dim,HIDDEN_SIZE):\n",
    "                super(LSTMModel,self).__init__()\n",
    "                self.lstm = nn.LSTM(input_dim, hidden_size = HIDDEN_SIZE, batch_first = True)\n",
    "                self.dropout = nn.Dropout(0.2)\n",
    "                self.seq = nn.Sequential(nn.Linear(HIDDEN_SIZE, 32),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(32, 32),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(32, 32),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(32, 32),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(32, 32),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(32, PRED_SIZE)\n",
    "                                        )\n",
    "\n",
    "            def forward(self,x):\n",
    "                x,_ = self.lstm(x)\n",
    "        #         print('output of lstm :',x.size()) # torch.Size([1, 120, 64])\n",
    "                x = self.dropout(x[:,-1,:])\n",
    "        #         print('from lstm to linear :',x.size()) # torch.Size([1, 64])\n",
    "                x = self.seq(x)\n",
    "                return x\n",
    "\n",
    "        class RMSELoss(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(RMSELoss,self).__init__()\n",
    "                self.mse = nn.MSELoss()\n",
    "\n",
    "            def forward(self,yhat,y):\n",
    "                return torch.sqrt(self.mse(yhat,y))\n",
    "\n",
    "        globals()[f'model{trial}'] = LSTMModel(X_train.shape[2],HIDDEN_SIZE).to(device)\n",
    "        criterion = RMSELoss()\n",
    "        optimizer = optim.Adam(globals()[f'model{trial}'].parameters(), lr = 0.001)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor = 0.1, patience = 20, verbose = True)\n",
    "\n",
    "        ### RUN!!\n",
    "\n",
    "        epoch_counter = 0\n",
    "        patience = 30\n",
    "        best_loss = float('inf')\n",
    "        done = False\n",
    "        es = EarlyStopping(patience=patience)\n",
    "        tr_losses_fp, val_losses_fp = [],[]\n",
    "\n",
    "        while not done and epoch_counter<EPOCHS:\n",
    "            epoch_counter+=1\n",
    "\n",
    "            # train\n",
    "            globals()[f'model{trial}'].train()\n",
    "            train_losses = []\n",
    "            for batch in train_loader:\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = globals()[f'model{trial}'](x_batch)\n",
    "                loss = criterion(output,y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "            train_loss = np.mean(train_losses)\n",
    "            tr_losses_fp.append(train_loss)\n",
    "\n",
    "            # validation\n",
    "            globals()[f'model{trial}'].eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    x_batch, y_batch = batch\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    output = globals()[f'model{trial}'](x_batch)\n",
    "                    loss = criterion(output, y_batch)\n",
    "                    val_losses.append(loss.item())\n",
    "            val_loss = np.mean(val_losses)\n",
    "            val_losses_fp.append(val_loss)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            if es(globals()[f'model{trial}'], val_loss):\n",
    "                done = True\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "\n",
    "            if epoch_counter%20 == 0:\n",
    "                print(f\"Epoch {epoch_counter}/{EPOCHS}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        print(f\"Best validation loss : {best_loss}\")\n",
    "        best_losses.append(best_loss)\n",
    "        \n",
    "        ### Visualize train-validation loss\n",
    "\n",
    "#         plt.plot(range(len(tr_losses_fp)),tr_losses_fp,color='blue',label='train_loss')\n",
    "#         plt.plot(range(len(val_losses_fp)),val_losses_fp,color='red',label='val_loss')\n",
    "#         plt.legend()\n",
    "#         # plt.show()\n",
    "#         plt.savefig(f'../../plots/train_val_loss_2/{filename}.png')\n",
    "#         plt.clf()\n",
    "        globals()[f'train_losses_{trial}']=tr_losses_fp\n",
    "        globals()[f'val_losses_{trial}']=val_losses_fp\n",
    "\n",
    "        ### Prediction\n",
    "\n",
    "        # evaluation\n",
    "        globals()[f'model{trial}'].eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.tensor(X_test.astype(np.float32), dtype = torch.float32,device=device)\n",
    "            pred = globals()[f'model{trial}'](X_test_tensor)\n",
    "\n",
    "        pred = pred.to('cpu').detach().numpy()\n",
    "        # print(pred)\n",
    "\n",
    "        if TARGET == \"1d_ROC\" :\n",
    "            endPrice = data['Close'].iloc[-1]\n",
    "            pred_close = []\n",
    "\n",
    "            for i in pred[0] :\n",
    "                endPrice = endPrice + endPrice*0.01*i\n",
    "                pred_close.append(endPrice)\n",
    "\n",
    "            pred = np.array(pred_close).reshape(1, PRED_SIZE)\n",
    "        #     pred\n",
    "        else :\n",
    "            pass\n",
    "\n",
    "        globals()[f'pred_{trial}'] = pred\n",
    "        \n",
    "        # pred_length = len(np.reshape(pred, (-1)))\n",
    "        # pred_indices = list(range(pred_length))\n",
    "        # plt.plot(pred_indices, np.reshape(pred, (-1)), color='red', alpha=0.6, label='Prediction')\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "        ### Evaluation\n",
    "\n",
    "        # Label\n",
    "#         testfile = '../../data/test.csv'\n",
    "#         test_df = pd.read_csv(testfile)\n",
    "#         label = pd.read_csv(testfile) # 각자 test.csv 파일 경로\n",
    "        label = test_df.copy()\n",
    "        label = np.array(label.head(PRED_SIZE)[\"Close Price\"])\n",
    "\n",
    "        # Prediction\n",
    "        pred = np.array(pred).reshape(PRED_SIZE)\n",
    "\n",
    "        # 날짜 데이터\n",
    "        period = test_df[\"Date\"].copy()\n",
    "        period = [d for d in period.head(PRED_SIZE)]\n",
    "\n",
    "        # 오차율 계산\n",
    "        error_rate = np.abs((label - pred) / label) * 100\n",
    "\n",
    "        # 시각화\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         plt.plot(period, label, marker='o', color='blue', label='Actual Close Price')\n",
    "#         plt.plot(period, pred, marker='x', color='red', linestyle='--', label='Predicted Close Price')\n",
    "\n",
    "#         # 오차율을 각 포인트에 텍스트로 표시\n",
    "#         for date, lbl, prd, err in zip(period, label, pred, error_rate):\n",
    "#             plt.text(date, prd, f'{err:.2f}%', color='black', ha='right', va='bottom')\n",
    "\n",
    "#         plt.xticks(rotation = 45)  # 날짜 레이블 회전\n",
    "#         plt.xlabel('Date')\n",
    "#         plt.ylabel('Close Price')\n",
    "#         plt.title(f'model = LSTM, data = {dataname}, target = {TARGET}, seq_size = {SEQ_SIZE}, pred_size = {PRED_SIZE}, batch_size = {BATCH_SIZE}, model_size = {HIDDEN_SIZE}')\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()  # 레이아웃 조정\n",
    "#         # plt.show()\n",
    "#         plt.savefig(f'../../plots/results_2/{filename}.png')\n",
    "#         plt.clf()\n",
    "\n",
    "        # 오차율을 출력\n",
    "        error_rate_dict = dict(zip(period, error_rate))\n",
    "        # error_rate_dict\n",
    "\n",
    "        # 평균 오차율 계산\n",
    "        average_error_rate = np.mean(error_rate)\n",
    "        \n",
    "        print(f\"Average Error Rate: {average_error_rate:.2f}%\")\n",
    "        mean_error_ratios.append(average_error_rate)\n",
    "        \n",
    "        ### Save results to a DataFrame\n",
    "#         temp_df = pd.DataFrame({\n",
    "#             'data':[dataname],\n",
    "#             'target':[TARGET],\n",
    "#             'seq_size':[SEQ_SIZE],\n",
    "#             'pred_size':[PRED_SIZE],\n",
    "#             'batch_size':[BATCH_SIZE],\n",
    "#             'hidden_size':[HIDDEN_SIZE],\n",
    "#             'best_val_loss':[best_loss],\n",
    "#             'mean_error_ratio':[average_error_rate]\n",
    "#         })\n",
    "\n",
    "#         result_df = pd.concat([result_df,temp_df],axis=0)\n",
    "print('🔸END🔸')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd3c99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.951955Z",
     "start_time": "2024-02-23T03:42:30.951955Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for t in list(range(1,51)):\n",
    "    preds.append(globals()[f'pred_{t}'].tolist())\n",
    "preds = np.reshape(np.array(preds),(10,50))\n",
    "# preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43970f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.953950Z",
     "start_time": "2024-02-23T03:42:30.953950Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394da38d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.955950Z",
     "start_time": "2024-02-23T03:42:30.955950Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds,columns=['model_1','model_2',\n",
    "                           'model_3','model_4',\n",
    "                           'model_5','model_6',\n",
    "                           'model_7','model_8',\n",
    "                           'model_9','model_10',\n",
    "                           'model_11','model_12',\n",
    "                           'model_13','model_14',\n",
    "                           'model_15','model_16',\n",
    "                           'model_17','model_18',\n",
    "                           'model_19','model_20',\n",
    "                           'model_21','model_22',\n",
    "                           'model_23','model_24',\n",
    "                           'model_25','model_26',\n",
    "                           'model_27','model_28',\n",
    "                           'model_29','model_30',\n",
    "                           'model_31','model_32',\n",
    "                           'model_33','model_34',\n",
    "                           'model_35','model_36',\n",
    "                           'model_37','model_38',\n",
    "                           'model_39','model_40',\n",
    "                           'model_41','model_42',\n",
    "                           'model_43','model_44',\n",
    "                           'model_45','model_46',\n",
    "                           'model_47','model_48',\n",
    "                           'model_49','model_50'])\n",
    "preds_df['Mean']=preds_df.mean(axis=1)\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a78d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.957050Z",
     "start_time": "2024-02-23T03:42:30.957050Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([best_losses,mean_error_ratios], columns=[\n",
    "                            'model_1','model_2',\n",
    "                           'model_3','model_4',\n",
    "                           'model_5','model_6',\n",
    "                           'model_7','model_8',\n",
    "                           'model_9','model_10',\n",
    "                           'model_11','model_12',\n",
    "                           'model_13','model_14',\n",
    "                           'model_15','model_16',\n",
    "                           'model_17','model_18',\n",
    "                           'model_19','model_20',\n",
    "                           'model_21','model_22',\n",
    "                           'model_23','model_24',\n",
    "                           'model_25','model_26',\n",
    "                           'model_27','model_28',\n",
    "                           'model_29','model_30',\n",
    "                           'model_31','model_32',\n",
    "                           'model_33','model_34',\n",
    "                           'model_35','model_36',\n",
    "                           'model_37','model_38',\n",
    "                           'model_39','model_40',\n",
    "                           'model_41','model_42',\n",
    "                           'model_43','model_44',\n",
    "                           'model_45','model_46',\n",
    "                           'model_47','model_48',\n",
    "                           'model_49','model_50'])\n",
    "print('Mean Val_Loss :',results_df.iloc[0].mean())\n",
    "print('Mean Error Ratio :',results_df.iloc[1].mean())\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131af681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.959562Z",
     "start_time": "2024-02-23T03:42:30.959562Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 시각화\n",
    "label = test_df.copy()\n",
    "label = np.array(label.head(PRED_SIZE)[\"Close Price\"])\n",
    "\n",
    "# Prediction\n",
    "pred_ = preds_df['Mean'].to_numpy().reshape(PRED_SIZE)\n",
    "\n",
    "# 날짜 데이터\n",
    "period = test_df[\"Date\"].copy()\n",
    "period = [d for d in period.head(PRED_SIZE)]\n",
    "d5_period = ['2023-12-22','2023-12-26','2023-12-27','2023-12-28','2023-12-29']\n",
    "d5_stock = stock_df.loc[d5_period,'Close'].to_list()\n",
    "\n",
    "# 오차율 계산\n",
    "error_rate = np.abs((label - pred_) / label) * 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(d5_period+period, d5_stock+label.tolist(), marker='o', color='blue', label='Actual Close Price')\n",
    "plt.plot(period, pred_, marker='x', color='red', linestyle='--', label='Predicted Close Price')\n",
    "plt.axvline(x='2024-01-02', color='red',linestyle='-',linewidth=.5)\n",
    "# 오차율을 각 포인트에 텍스트로 표시\n",
    "for date, lbl, prd, err in zip(period, label, pred_, error_rate):\n",
    "    plt.text(date, prd, f'{err:.2f}%', color='black', ha='right', va='bottom')\n",
    "\n",
    "    \n",
    "plt.xticks(rotation = 45)  # 날짜 레이블 회전\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title(f'model = {MODEL}, data = {dataname}, target = {TARGET}, seq_size = {SEQ_SIZE}, pred_size = {PRED_SIZE}, batch_size = {BATCH_SIZE}, model_size = {HIDDEN_SIZE}')\n",
    "plt.legend()\n",
    "plt.tight_layout()  # 레이아웃 조정\n",
    "plt.savefig(f'../../plots/best_plots/50trials_{filename}.png') #### 본인 파일 경로\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# 오차율을 출력\n",
    "error_rate_dict = dict(zip(period, error_rate))\n",
    "# error_rate_dict\n",
    "\n",
    "# 평균 오차율 계산\n",
    "average_error_rate = np.mean(error_rate)\n",
    "\n",
    "print(f\"Average Error Rate: {average_error_rate:.2f}%\")\n",
    "mean_error_ratios.append(average_error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f7f35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T03:42:30.962637Z",
     "start_time": "2024-02-23T03:42:30.962637Z"
    }
   },
   "outputs": [],
   "source": [
    "### Visualize train-validation loss\n",
    "\n",
    "for t in list(range(1,11)):\n",
    "    plt.plot(range(len(globals()[f'train_losses_{t}'])),\n",
    "             globals()[f'train_losses_{t}'],\n",
    "             color='blue',alpha=0.6)\n",
    "    plt.plot(range(len(globals()[f'val_losses_{t}'])),\n",
    "             globals()[f'val_losses_{t}'],\n",
    "             color='red',alpha=0.6)\n",
    "plt.legend(['train_loss','val_loss'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../../plots/best_plots/50trials_train_val_loss_{filename}_2.png')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
