{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T14:31:02.868684Z",
     "start_time": "2024-01-18T14:31:02.851345Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-13T01:04:46.841212Z",
     "iopub.status.busy": "2024-01-13T01:04:46.840698Z",
     "iopub.status.idle": "2024-01-13T01:04:52.708924Z",
     "shell.execute_reply": "2024-01-13T01:04:52.707999Z",
     "shell.execute_reply.started": "2024-01-13T01:04:46.841183Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import lightgbm as lgb\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h4>The idea of using Class Objects for feature generation as an efficient method has been presented in this really <a href=\"https://www.kaggle.com/code/vitalykudelya/enefit-object-oriented-gbdt\">insightful notebook</a> by <a href=\"https://www.kaggle.com/vitalykudelya\">Vitaly Kudelya</a>. The notebook presents various new feature engineering methods and the author's efforts in putting them together are commendable. Recently, they have also proposed a method to predict <i>the difference</i> between the last available 'target_48h' and the 'target'  rather than the 'target' itself, which I really liked. It can be seen <a href='https://www.kaggle.com/code/vitalykudelya/enefit-target-diff'>here</a></h4> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h4>In this notebook, I have tried to build upon this work by:<ul> <li>adding some more features</li><li> tuning the parameters a bit and</li> <li>performed some data analysis</li></ul></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T14:31:02.916040Z",
     "start_time": "2024-01-18T14:31:02.870694Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"../data/\"\n",
    "client_org = pd.read_csv(path+'client.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T14:31:03.073753Z",
     "start_time": "2024-01-18T14:31:03.052679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_type</th>\n",
       "      <th>county</th>\n",
       "      <th>eic_count</th>\n",
       "      <th>installed_capacity</th>\n",
       "      <th>is_business</th>\n",
       "      <th>date</th>\n",
       "      <th>data_block_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>952.89</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>166.40</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_type  county  eic_count  installed_capacity  is_business  \\\n",
       "0             1       0        108              952.89            0   \n",
       "1             2       0         17              166.40            0   \n",
       "\n",
       "         date  data_block_id  \n",
       "0  2021-09-01              2  \n",
       "1  2021-09-01              2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_org.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T14:31:04.086921Z",
     "start_time": "2024-01-18T14:31:04.058308Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-13T01:04:52.71131Z",
     "iopub.status.busy": "2024-01-13T01:04:52.710985Z",
     "iopub.status.idle": "2024-01-13T01:04:52.734369Z",
     "shell.execute_reply": "2024-01-13T01:04:52.733367Z",
     "shell.execute_reply.started": "2024-01-13T01:04:52.711272Z"
    }
   },
   "outputs": [],
   "source": [
    "class Warehouse:\n",
    "#     root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "    root = '../data' # my filepath root\n",
    "\n",
    "    data_columns = [\"target\",\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",\"row_id\",]\n",
    "    client_columns = [\"product_type\",\"county\",\"eic_count\",\"installed_capacity\",\"is_business\",\"date\",]\n",
    "    gas_prices_columns = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n",
    "    electricity_prices_columns = [\"forecast_date\", \"euros_per_mwh\"]\n",
    "    forecast_weather_columns = [\"latitude\",\"longitude\",\"hours_ahead\",\"temperature\",\"dewpoint\",\"cloudcover_high\",\n",
    "                             \"cloudcover_low\",\"cloudcover_mid\",\"cloudcover_total\",\"10_metre_u_wind_component\",\n",
    "                             \"10_metre_v_wind_component\",\"forecast_datetime\",\"direct_solar_radiation\",\n",
    "                             \"surface_solar_radiation_downwards\",\"snowfall\",\"total_precipitation\",]\n",
    "    historical_weather_columns = [\"datetime\",\"temperature\",\"dewpoint\",\"rain\",\"snowfall\",\"surface_pressure\",\n",
    "                               \"cloudcover_total\",\"cloudcover_low\",\"cloudcover_mid\",\"cloudcover_high\",\n",
    "                               \"windspeed_10m\",\"winddirection_10m\",\"shortwave_radiation\",\"direct_solar_radiation\",\"diffuse_radiation\",\"latitude\",\"longitude\",\n",
    "                              ]\n",
    "    location_columns = [\"longitude\", \"latitude\", \"county\"]\n",
    "    target_columns = [\"target\",\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_data = pl.read_csv(\n",
    "            os.path.join(self.root, \"train.csv\"),\n",
    "            columns=self.data_columns,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_client = pl.read_csv(\n",
    "            os.path.join(self.root, \"client.csv\"),\n",
    "            columns=self.client_columns,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_gas_prices = pl.read_csv(\n",
    "            os.path.join(self.root, \"gas_prices.csv\"),\n",
    "            columns=self.gas_prices_columns,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_electricity_prices = pl.read_csv(\n",
    "            os.path.join(self.root, \"electricity_prices.csv\"),\n",
    "            columns=self.electricity_prices_columns,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_forecast_weather = pl.read_csv(\n",
    "            os.path.join(self.root, \"forecast_weather.csv\"),\n",
    "            columns=self.forecast_weather_columns,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_historical_weather = pl.read_csv(\n",
    "            os.path.join(self.root, \"historical_weather.csv\"),\n",
    "            columns=self.historical_weather_columns,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_weather_station_to_county_mapping = pl.read_csv(\n",
    "            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n",
    "            columns=self.location_columns,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_data = self.df_data.filter(\n",
    "            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n",
    "        )\n",
    "        self.df_target = self.df_data.select(self.target_columns)\n",
    "        self.schema_data = self.df_data.schema\n",
    "        self.schema_client = self.df_client.schema\n",
    "        self.schema_gas_prices = self.df_gas_prices.schema\n",
    "        self.schema_electricity_prices = self.df_electricity_prices.schema\n",
    "        self.schema_forecast_weather = self.df_forecast_weather.schema\n",
    "        self.schema_historical_weather = self.df_historical_weather.schema\n",
    "        self.schema_target = self.df_target.schema\n",
    "\n",
    "        self.df_weather_station_to_county_mapping = (\n",
    "            self.df_weather_station_to_county_mapping.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def update_data(self,df_client_new,df_gas_price_new,df_elec_price_new,df_forecast_new,df_hist_weather_new,df_target_new,):\n",
    "        df_client_new = pl.from_pandas(df_client_new[self.client_columns], schema_overrides=self.schema_client)\n",
    "        \n",
    "        df_gas_price_new = pl.from_pandas(df_gas_price_new[self.gas_prices_columns],schema_overrides=self.schema_gas_prices,)\n",
    "        \n",
    "        df_elec_price_new = pl.from_pandas(df_elec_price_new[self.electricity_prices_columns],schema_overrides=self.schema_electricity_prices,)\n",
    "        \n",
    "        df_forecast_new = pl.from_pandas(df_forecast_new[self.forecast_weather_columns],schema_overrides=self.schema_forecast_weather,)\n",
    "        \n",
    "        df_hist_weather_new = pl.from_pandas(df_hist_weather_new[self.historical_weather_columns],schema_overrides=self.schema_historical_weather,)\n",
    "        \n",
    "        df_target_new = pl.from_pandas(df_target_new[self.target_columns], schema_overrides=self.schema_target)\n",
    "\n",
    "        self.df_client = pl.concat([self.df_client, df_client_new]).unique([\"date\", \"county\", \"is_business\", \"product_type\"])\n",
    "        \n",
    "        self.df_gas_prices = pl.concat([self.df_gas_prices, df_gas_price_new]).unique([\"forecast_date\"])\n",
    "        \n",
    "        self.df_electricity_prices = pl.concat([self.df_electricity_prices, df_elec_price_new]).unique([\"forecast_date\"])\n",
    "        \n",
    "        self.df_forecast_weather = pl.concat([self.df_forecast_weather, df_forecast_new]).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n",
    "        \n",
    "        self.df_historical_weather = pl.concat([self.df_historical_weather, df_hist_weather_new]).unique([\"datetime\", \"latitude\", \"longitude\"])\n",
    "        \n",
    "        self.df_target = pl.concat([self.df_target, df_target_new]).unique([\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"])\n",
    "\n",
    "    def preprocess_test(self, df_test):\n",
    "        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "        df_test = pl.from_pandas(\n",
    "            df_test[self.data_columns[1:]], schema_overrides=self.schema_data\n",
    "        )\n",
    "        return df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T14:31:04.976786Z",
     "start_time": "2024-01-18T14:31:04.930029Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-13T01:04:52.737533Z",
     "iopub.status.busy": "2024-01-13T01:04:52.737167Z",
     "iopub.status.idle": "2024-01-13T01:04:52.783395Z",
     "shell.execute_reply": "2024-01-13T01:04:52.782437Z",
     "shell.execute_reply.started": "2024-01-13T01:04:52.7375Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.estonian_holidays = list(\n",
    "            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n",
    "        )\n",
    "\n",
    "    def _general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(pl.concat_str(\"county\",\"is_business\",\"product_type\",\"is_consumption\",separator=\"_\",).alias(\"segment\"),)\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),(np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),(np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _client_features(self, df_features):\n",
    "        df_client = self.data.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns((pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),on=[\"county\", \"is_business\", \"product_type\", \"date\"],how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def is_country_holiday(self, row):\n",
    "        return (\n",
    "            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n",
    "            in self.estonian_holidays\n",
    "        )\n",
    "\n",
    "    def _holidays_features(self, df_features):\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.struct([\"year\", \"month\", \"day\"])\n",
    "            .apply(self.is_country_holiday)\n",
    "            .alias(\"is_country_holiday\")\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            .drop(\"hours_ahead\")\n",
    "            .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),)\n",
    "            .join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\"))\n",
    "\n",
    "        df_forecast_weather_date = (df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\"))\n",
    "\n",
    "        df_forecast_weather_local = (df_forecast_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean())\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=\"datetime\",how=\"left\",suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (self.data.df_weather_station_to_county_mapping)\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            ).join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (df_historical_weather.group_by(\"datetime\").mean().drop(\"county\"))\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=\"datetime\",how=\"left\",suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),).filter(pl.col(\"hour\") <= 10).drop(\"hour\"),on=\"datetime\",how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _target_features(self, df_features):\n",
    "        df_target = self.data.df_target\n",
    "\n",
    "        df_target_all_type_sum = (df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\"))\n",
    "\n",
    "        df_target_all_county_type_sum = (df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\", \"county\"))\n",
    "        \n",
    "        hours_list=[i*24 for i in range(2,15)]\n",
    "\n",
    "        for hours_lag in hours_list:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [f\"target_{hours_lag}h\" for hours_lag in hours_list[:4]]\n",
    "        \n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats).transpose().std().transpose().to_series().alias(f\"target_std\"),\n",
    "            )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),(\"target\", 24 * 2, 24 * 9),(\"target\", 24 * 3, 24 * 10),(\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),(\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),(\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (pl.col(f\"{target_prefix}_{lag_nominator}h\")/ (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\"datetime\", \"hour\", \"dayofyear\")\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"segment\",]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "            \n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    # added some new features here\n",
    "    def _additional_features(self,df):\n",
    "        for col in [\n",
    "                    'temperature', \n",
    "                    'dewpoint', \n",
    "                    '10_metre_u_wind_component', \n",
    "                    '10_metre_v_wind_component', \n",
    "            ]:\n",
    "            for window in [1]:\n",
    "                df[f\"{col}_diff_{window}\"] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])[col].diff(window)\n",
    "        return df\n",
    "    \n",
    "    def _log_outliers(self,df):\n",
    "        l1=['installed_capacity', 'target_mean', 'target_std']\n",
    "        for i in l1:\n",
    "            df = df.with_columns([(f\"log_{i}\", pl.when(df[i] != 0).then(np.log(pl.col(i))).otherwise(0))])\n",
    "        return df\n",
    "        \n",
    "\n",
    "    def generate_features(self, df_prediction_items,isTrain):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._general_features,self._client_features,self._forecast_weather_features,\n",
    "            self._historical_weather_features,self._target_features,self._holidays_features,\n",
    "            self._log_outliers,self._reduce_memory_usage,self._drop_columns,]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "        df_features = self._additional_features(df_features)\n",
    "       \n",
    "        return df_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T14:31:05.662721Z",
     "start_time": "2024-01-18T14:31:05.642753Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-13T01:04:52.78609Z",
     "iopub.status.busy": "2024-01-13T01:04:52.785744Z",
     "iopub.status.idle": "2024-01-13T01:04:52.800768Z",
     "shell.execute_reply": "2024-01-13T01:04:52.799741Z",
     "shell.execute_reply.started": "2024-01-13T01:04:52.786065Z"
    }
   },
   "outputs": [],
   "source": [
    "model_parameters = {\n",
    "                             \"objective\": \"regression_l1\",\"device\": \"gpu\",\n",
    "            \"n_estimators\": 3000,\"learning_rate\": 0.05,\"colsample_bytree\": 0.8,\"colsample_bynode\": 0.5,\n",
    "            \"lambda_l1\": 3.4,\"lambda_l2\": 1.4,\"max_depth\": 15,\"num_leaves\": 490,\"min_data_in_leaf\": 48,\n",
    "        }\n",
    "\n",
    "\n",
    "m1 = VotingRegressor([\n",
    "                (f\"clgb_{i}\",lgb.LGBMRegressor(**model_parameters, random_state=i),)\n",
    "                for i in range(12)\n",
    "            ])\n",
    "\n",
    "m2 = VotingRegressor([\n",
    "                (f\"plgb_{i}\",lgb.LGBMRegressor(**model_parameters, random_state=i),)\n",
    "                for i in range(12)\n",
    "            ])\n",
    "\n",
    "def fit_model(train_feats,hours_lag,model_consumption=m1,model_production=m2):\n",
    "    mask = train_feats[\"is_consumption\"] == 1\n",
    "    model_consumption.fit(\n",
    "        X=train_feats[mask].drop(columns=[\"target\"]),\n",
    "        y=train_feats[mask][\"target\"]- train_feats[mask][f\"target_{hours_lag}h\"].fillna(0),\n",
    "        )\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    mask = train_feats[\"is_consumption\"] == 0\n",
    "    model_production.fit(\n",
    "            X=train_feats[mask].drop(columns=[\"target\"]),\n",
    "            y=train_feats[mask][\"target\"]\n",
    "            - train_feats[mask][f\"target_{hours_lag}h\"].fillna(0),\n",
    "        )\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "def predict_model(df_features,hours_lag,model_consumption=m1,model_production=m2):\n",
    "    predictions = np.zeros(len(df_features))\n",
    "\n",
    "    mask = df_features[\"is_consumption\"] == 1\n",
    "    predictions[mask.values] = np.clip(\n",
    "        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n",
    "        model_consumption.predict(df_features[mask]),0,np.inf,\n",
    "        )\n",
    "\n",
    "    mask = df_features[\"is_consumption\"] == 0\n",
    "    predictions[mask.values] = np.clip(\n",
    "        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n",
    "        model_production.predict(df_features[mask]),0,np.inf,\n",
    "        )\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T14:31:08.453346Z",
     "start_time": "2024-01-18T14:31:07.169583Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-13T01:04:52.802517Z",
     "iopub.status.busy": "2024-01-13T01:04:52.802139Z",
     "iopub.status.idle": "2024-01-13T01:04:58.454486Z",
     "shell.execute_reply": "2024-01-13T01:04:58.453609Z",
     "shell.execute_reply.started": "2024-01-13T01:04:52.802481Z"
    }
   },
   "outputs": [],
   "source": [
    "store = Warehouse()\n",
    "feat_gen = FeatureEngineer(data=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T14:31:35.720698Z",
     "start_time": "2024-01-18T14:31:10.739087Z"
    },
    "execution": {
     "iopub.execute_input": "2024-01-13T01:04:58.455908Z",
     "iopub.status.busy": "2024-01-13T01:04:58.455606Z",
     "iopub.status.idle": "2024-01-13T01:05:23.169697Z",
     "shell.execute_reply": "2024-01-13T01:05:23.168864Z",
     "shell.execute_reply.started": "2024-01-13T01:04:58.455883Z"
    }
   },
   "outputs": [
    {
     "ename": "PanicException",
     "evalue": "not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mfeat_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_data\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df_train \u001b[38;5;241m=\u001b[39m df_train[df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()]\n",
      "Cell \u001b[1;32mIn[6], line 223\u001b[0m, in \u001b[0;36mFeatureEngineer.generate_features\u001b[1;34m(self, df_prediction_items, isTrain)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m add_features \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_general_features,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_features,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forecast_weather_features,\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_historical_weather_features,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_features,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_holidays_features,\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_outliers,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_memory_usage,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_columns,]:\n\u001b[0;32m    221\u001b[0m     df_features \u001b[38;5;241m=\u001b[39m add_features(df_features)\n\u001b[1;32m--> 223\u001b[0m df_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m df_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_additional_features(df_features)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_features\n",
      "Cell \u001b[1;32mIn[6], line 175\u001b[0m, in \u001b[0;36mFeatureEngineer._to_pandas\u001b[1;34m(self, df_features, y)\u001b[0m\n\u001b[0;32m    172\u001b[0m cat_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounty\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_business\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_consumption\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment\u001b[39m\u001b[38;5;124m\"\u001b[39m,]\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     df_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mdf_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto_pandas()], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     df_features \u001b[38;5;241m=\u001b[39m df_features\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\polars\\dataframe\\frame.py:2289\u001b[0m, in \u001b[0;36mDataFrame.to_pandas\u001b[1;34m(self, use_pyarrow_extension_array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2286\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2287\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(msg)\n\u001b[1;32m-> 2289\u001b[0m record_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2290\u001b[0m tbl \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(record_batches)\n\u001b[0;32m   2291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pyarrow_extension_array:\n",
      "\u001b[1;31mPanicException\u001b[0m: not implemented"
     ]
    }
   ],
   "source": [
    "df_train = feat_gen.generate_features(store.df_data,True)\n",
    "df_train = df_train[df_train['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:05:23.171078Z",
     "iopub.status.busy": "2024-01-13T01:05:23.170786Z",
     "iopub.status.idle": "2024-01-13T01:05:23.178072Z",
     "shell.execute_reply": "2024-01-13T01:05:23.177195Z",
     "shell.execute_reply.started": "2024-01-13T01:05:23.171054Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will be using Plotly for Data Analysis. \n",
    "Plotly has **hover tool capabilities** that allow us to detect any outliers or anomalies in a large number of data points.\n",
    "The resultant plots are highly interactive and it allows to zoom in and focus on certain regions of the plot for a deeper analysis.\n",
    "It allows for endless customization of graphs that makes the plot more meaningful and understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## segment-wise energy consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:05:23.17973Z",
     "iopub.status.busy": "2024-01-13T01:05:23.179389Z",
     "iopub.status.idle": "2024-01-13T01:05:25.766487Z",
     "shell.execute_reply": "2024-01-13T01:05:25.765562Z",
     "shell.execute_reply.started": "2024-01-13T01:05:23.179698Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "segment_list = df_train.segment.unique()[:3]\n",
    "\n",
    "# Filter the dataset for the specific segment\n",
    "for seg in segment_list:\n",
    "    consumption_segment = df_train[df_train.segment == seg]\n",
    "\n",
    "    # Create a line plot using Plotly Express\n",
    "    fig = px.line(consumption_segment, x='date', y='target', \n",
    "              title=f'Target Over Time for Segment {seg}',\n",
    "              labels={'date': 'Date', 'target': 'Target'},\n",
    "              template='plotly_dark',line_shape='linear')\n",
    "    fig.update_traces(line=dict(color='blue', width=1.5))\n",
    "\n",
    "    # Customize the x-axis date format and tick interval\n",
    "    fig.update_xaxes(type='date', tickformat='%Y-%m-%d', tickmode='linear', dtick=15)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFT Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <h4>Initially studied it in this <a href=\"https://www.kaggle.com/code/chaozhuang/enefit-eda-w-fft-ssa-lgbm-voting-regressor\">amazing notebook</a> by <a href=\"https://www.kaggle.com/chaozhuang\">CHAO ZHUANG</a>. He has done a very deep analysis and explained everything on the go</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:05:25.768258Z",
     "iopub.status.busy": "2024-01-13T01:05:25.767908Z",
     "iopub.status.idle": "2024-01-13T01:05:26.990527Z",
     "shell.execute_reply": "2024-01-13T01:05:26.989035Z",
     "shell.execute_reply.started": "2024-01-13T01:05:25.768228Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "segment_list = df_train.segment.unique()[:10]\n",
    "example_df = df_train[np.isin(df_train.segment, segment_list)]\n",
    "segments = example_df['segment'].unique()\n",
    "\n",
    "# Define periods in days and calculate corresponding frequencies\n",
    "periods = {'Annual': 365,'Semiannual': 365 / 2,'Quarterly': 365 / 4,'Monthly': 30,'Biweekly': 14,'Weekly': 7,'Semiweekly': 3.5}\n",
    "frequencies_for_periods = {k: 1 / v for k, v in periods.items()}\n",
    "\n",
    "# Initialize the figure for the spectra using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Convert the x-axis to a log scale\n",
    "fig.update_xaxes(type='log')\n",
    "\n",
    "# Plot the spectrum for each segment with offset\n",
    "for i, segment in enumerate(segments):\n",
    "    segment_data = example_df[example_df['segment'] == segment]['target']\n",
    "    fft_values = np.fft.fft(segment_data)\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n",
    "    magnitudes = np.abs(fft_values)[frequencies > 0]\n",
    "    normalized_magnitudes = magnitudes / np.max(magnitudes)\n",
    "    positive_freqs = frequencies[frequencies > 0]\n",
    "\n",
    "    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n",
    "    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Semiannual']]\n",
    "    valid_magnitudes = normalized_magnitudes[positive_freqs > frequencies_for_periods['Semiannual']]\n",
    "\n",
    "    # Offset each segment's spectrum for clarity\n",
    "    offset_magnitudes = valid_magnitudes + i\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=valid_freqs, y=offset_magnitudes, mode='lines', name=f'Segment {segment}'))\n",
    "\n",
    "# Customize the plot layout\n",
    "fig.update_layout(\n",
    "    title='Frequency Spectra of hourly target for Each Segment',\n",
    "    xaxis_title='Frequency',\n",
    "    yaxis_title='Normalized Magnitude + Offset',\n",
    "    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:05:26.994135Z",
     "iopub.status.busy": "2024-01-13T01:05:26.993811Z",
     "iopub.status.idle": "2024-01-13T01:05:27.002345Z",
     "shell.execute_reply": "2024-01-13T01:05:27.001408Z",
     "shell.execute_reply.started": "2024-01-13T01:05:26.994107Z"
    }
   },
   "outputs": [],
   "source": [
    "def fft_plots_enefit(name):\n",
    "    # Initialize the figure for the spectrum using Plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Convert the x-axis to a log scale\n",
    "    fig.update_xaxes(type='log')\n",
    "\n",
    "    # Plot the spectrum for the specified segment\n",
    "    segment_data = example_df[example_df['segment'] == '0_0_1_1'][name]\n",
    "    fft_values = np.fft.fft(segment_data)\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n",
    "    magnitudes = np.abs(fft_values)[frequencies > 0]\n",
    "    positive_freqs = frequencies[frequencies > 0]\n",
    "\n",
    "    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n",
    "    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Semiannual']]\n",
    "    valid_magnitudes = magnitudes[positive_freqs > frequencies_for_periods['Semiannual']]\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=valid_freqs, y=valid_magnitudes, mode='lines', name='0_0_1_1'))\n",
    "\n",
    "    #  Customize the plot layout\n",
    "    fig.update_layout(\n",
    "    title=f'{name} frequency spectrum',\n",
    "    xaxis_title='Frequency',\n",
    "    yaxis_title='Magnitude',\n",
    "    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n",
    "    showlegend=True,\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:05:27.00387Z",
     "iopub.status.busy": "2024-01-13T01:05:27.003558Z",
     "iopub.status.idle": "2024-01-13T01:05:27.11641Z",
     "shell.execute_reply": "2024-01-13T01:05:27.115487Z",
     "shell.execute_reply.started": "2024-01-13T01:05:27.003843Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_list=['temperature','direct_solar_radiation']\n",
    "for i in plot_list:\n",
    "    fft_plots_enefit(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:05:27.117875Z",
     "iopub.status.busy": "2024-01-13T01:05:27.117585Z",
     "iopub.status.idle": "2024-01-13T01:05:27.536426Z",
     "shell.execute_reply": "2024-01-13T01:05:27.535423Z",
     "shell.execute_reply.started": "2024-01-13T01:05:27.11785Z"
    }
   },
   "outputs": [],
   "source": [
    "#dropping date column now\n",
    "df_train.drop(columns=['date'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:05:27.537822Z",
     "iopub.status.busy": "2024-01-13T01:05:27.537539Z",
     "iopub.status.idle": "2024-01-13T01:05:27.894912Z",
     "shell.execute_reply": "2024-01-13T01:05:27.893969Z",
     "shell.execute_reply.started": "2024-01-13T01:05:27.537798Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'literal' in df_train.columns:\n",
    "    df_train.drop(columns=['literal'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:05:27.896558Z",
     "iopub.status.busy": "2024-01-13T01:05:27.896211Z",
     "iopub.status.idle": "2024-01-13T01:05:27.90504Z",
     "shell.execute_reply": "2024-01-13T01:05:27.904018Z",
     "shell.execute_reply.started": "2024-01-13T01:05:27.896526Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:05:27.906634Z",
     "iopub.status.busy": "2024-01-13T01:05:27.906306Z",
     "iopub.status.idle": "2024-01-13T01:06:15.40372Z",
     "shell.execute_reply": "2024-01-13T01:06:15.402107Z",
     "shell.execute_reply.started": "2024-01-13T01:05:27.906603Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_model(df_train,48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:06:43.833903Z",
     "iopub.status.busy": "2024-01-13T01:06:43.833532Z",
     "iopub.status.idle": "2024-01-13T01:06:43.857326Z",
     "shell.execute_reply": "2024-01-13T01:06:43.856374Z",
     "shell.execute_reply.started": "2024-01-13T01:06:43.833873Z"
    }
   },
   "outputs": [],
   "source": [
    "import enefit\n",
    "\n",
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T01:06:44.820992Z",
     "iopub.status.busy": "2024-01-13T01:06:44.820047Z",
     "iopub.status.idle": "2024-01-13T01:06:51.028281Z",
     "shell.execute_reply": "2024-01-13T01:06:51.026925Z",
     "shell.execute_reply.started": "2024-01-13T01:06:44.820949Z"
    }
   },
   "outputs": [],
   "source": [
    "for (\n",
    "    df_test, \n",
    "    df_new_target, \n",
    "    df_new_client, \n",
    "    df_new_historical_weather,\n",
    "    df_new_forecast_weather, \n",
    "    df_new_electricity_prices, \n",
    "    df_new_gas_prices, \n",
    "    df_sample_prediction\n",
    ") in iter_test:\n",
    "\n",
    "    store.update_data(\n",
    "        df_client_new=df_new_client,\n",
    "        df_gas_price_new=df_new_gas_prices,\n",
    "        df_elec_price_new=df_new_electricity_prices,\n",
    "        df_forecast_new=df_new_forecast_weather,\n",
    "        df_hist_weather_new=df_new_historical_weather,\n",
    "        df_target_new=df_new_target\n",
    "    )\n",
    "    df_test = store.preprocess_test(df_test)\n",
    "    \n",
    "    df_test_feats = feat_gen.generate_features(df_test,False)\n",
    "    \n",
    "    df_test_feats.drop(columns=['date'],inplace=True)\n",
    "    if 'literal' in df_test_feats.columns:\n",
    "        df_test_feats.drop(columns=['literal'],inplace=True)\n",
    "        \n",
    "    df_sample_prediction[\"target\"] = predict_model(df_test_feats,48)\n",
    "    \n",
    "    env.predict(df_sample_prediction)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
